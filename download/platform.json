[
	{
		"content": "#!/usr/bin/env python\n\n\"\"\" This module tries to retrieve as much platform-identifying data as\n    possible. It makes this information available via function APIs.\n\n    If called from the command line, it prints the platform\n    information concatenated as single string to stdout. The output\n    format is useable as part of a filename.\n\n\"\"\"\n#    This module is maintained by Marc-Andre Lemburg <mal@egenix.com>.\n#    If you find problems, please submit bug reports/patches via the\n#    Python bug tracker (http://bugs.python.org) and assign them to \"lemburg\".\n#\n#    Note: Please keep this module compatible to Python 1.5.2.\n#\n#    Still needed:\n#    * more support for WinCE\n#    * support for MS-DOS (PythonDX ?)\n#    * support for Amiga and other still unsupported platforms running Python\n#    * support for additional Linux distributions\n#\n#    Many thanks to all those who helped adding platform-specific\n#    checks (in no particular order):\n#\n#      Charles G Waldman, David Arnold, Gordon McMillan, Ben Darnell,\n#      Jeff Bauer, Cliff Crawford, Ivan Van Laningham, Josef\n#      Betancourt, Randall Hopper, Karl Putland, John Farrell, Greg\n#      Andruk, Just van Rossum, Thomas Heller, Mark R. Levinson, Mark\n#      Hammond, Bill Tutt, Hans Nowak, Uwe Zessin (OpenVMS support),\n#      Colin Kong, Trent Mick, Guido van Rossum, Anthony Baxter\n#\n#    History:\n#\n#    <see CVS and SVN checkin messages for history>\n#\n#    1.0.7 - added DEV_NULL\n#    1.0.6 - added linux_distribution()\n#    1.0.5 - fixed Java support to allow running the module on Jython\n#    1.0.4 - added IronPython support\n#    1.0.3 - added normalization of Windows system name\n#    1.0.2 - added more Windows support\n#    1.0.1 - reformatted to make doc.py happy\n#    1.0.0 - reformatted a bit and checked into Python CVS\n#    0.8.0 - added sys.version parser and various new access\n#            APIs (python_version(), python_compiler(), etc.)\n#    0.7.2 - fixed architecture() to use sizeof(pointer) where available\n#    0.7.1 - added support for Caldera OpenLinux\n#    0.7.0 - some fixes for WinCE; untabified the source file\n#    0.6.2 - support for OpenVMS - requires version 1.5.2-V006 or higher and\n#            vms_lib.getsyi() configured\n#    0.6.1 - added code to prevent 'uname -p' on platforms which are\n#            known not to support it\n#    0.6.0 - fixed win32_ver() to hopefully work on Win95,98,NT and Win2k;\n#            did some cleanup of the interfaces - some APIs have changed\n#    0.5.5 - fixed another type in the MacOS code... should have\n#            used more coffee today ;-)\n#    0.5.4 - fixed a few typos in the MacOS code\n#    0.5.3 - added experimental MacOS support; added better popen()\n#            workarounds in _syscmd_ver() -- still not 100% elegant\n#            though\n#    0.5.2 - fixed uname() to return '' instead of 'unknown' in all\n#            return values (the system uname command tends to return\n#            'unknown' instead of just leaving the field emtpy)\n#    0.5.1 - included code for slackware dist; added exception handlers\n#            to cover up situations where platforms don't have os.popen\n#            (e.g. Mac) or fail on socket.gethostname(); fixed libc\n#            detection RE\n#    0.5.0 - changed the API names referring to system commands to *syscmd*;\n#            added java_ver(); made syscmd_ver() a private\n#            API (was system_ver() in previous versions) -- use uname()\n#            instead; extended the win32_ver() to also return processor\n#            type information\n#    0.4.0 - added win32_ver() and modified the platform() output for WinXX\n#    0.3.4 - fixed a bug in _follow_symlinks()\n#    0.3.3 - fixed popen() and \"file\" command invokation bugs\n#    0.3.2 - added architecture() API and support for it in platform()\n#    0.3.1 - fixed syscmd_ver() RE to support Windows NT\n#    0.3.0 - added system alias support\n#    0.2.3 - removed 'wince' again... oh well.\n#    0.2.2 - added 'wince' to syscmd_ver() supported platforms\n#    0.2.1 - added cache logic and changed the platform string format\n#    0.2.0 - changed the API to use functions instead of module globals\n#            since some action take too long to be run on module import\n#    0.1.0 - first release\n#\n#    You can always get the latest version of this module at:\n#\n#             http://www.egenix.com/files/python/platform.py\n#\n#    If that URL should fail, try contacting the author.\n\n__copyright__ = \"\"\"\n    Copyright (c) 1999-2000, Marc-Andre Lemburg; mailto:mal@lemburg.com\n    Copyright (c) 2000-2010, eGenix.com Software GmbH; mailto:info@egenix.com\n\n    Permission to use, copy, modify, and distribute this software and its\n    documentation for any purpose and without fee or royalty is hereby granted,\n    provided that the above copyright notice appear in all copies and that\n    both that copyright notice and this permission notice appear in\n    supporting documentation or portions thereof, including modifications,\n    that you make.\n\n    EGENIX.COM SOFTWARE GMBH DISCLAIMS ALL WARRANTIES WITH REGARD TO\n    THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND\n    FITNESS, IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL,\n    INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING\n    FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,\n    NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION\n    WITH THE USE OR PERFORMANCE OF THIS SOFTWARE !\n\n\"\"\"\n\n__version__ = '1.0.7'\n\nimport sys,string,os,re\n\n### Globals & Constants\n\n# Determine the platform's /dev/null device\ntry:\n    DEV_NULL = os.devnull\nexcept AttributeError:\n    # os.devnull was added in Python 2.4, so emulate it for earlier\n    # Python versions\n    if sys.platform in ('dos','win32','win16','os2'):\n        # Use the old CP/M NUL as device name\n        DEV_NULL = 'NUL'\n    else:\n        # Standard Unix uses /dev/null\n        DEV_NULL = '/dev/null'\n\n### Platform specific APIs\n\n_libc_search = re.compile(r'(__libc_init)'\n                          '|'\n                          '(GLIBC_([0-9.]+))'\n                          '|'\n                          '(libc(_\\w+)?\\.so(?:\\.(\\d[0-9.]*))?)')\n\ndef libc_ver(executable=sys.executable,lib='',version='',\n\n             chunksize=2048):\n\n    \"\"\" Tries to determine the libc version that the file executable\n        (which defaults to the Python interpreter) is linked against.\n\n        Returns a tuple of strings (lib,version) which default to the\n        given parameters in case the lookup fails.\n\n        Note that the function has intimate knowledge of how different\n        libc versions add symbols to the executable and thus is probably\n        only useable for executables compiled using gcc.\n\n        The file is read and scanned in chunks of chunksize bytes.\n\n    \"\"\"\n    if hasattr(os.path, 'realpath'):\n        # Python 2.2 introduced os.path.realpath(); it is used\n        # here to work around problems with Cygwin not being\n        # able to open symlinks for reading\n        executable = os.path.realpath(executable)\n    f = open(executable,'rb')\n    binary = f.read(chunksize)\n    pos = 0\n    while 1:\n        m = _libc_search.search(binary,pos)\n        if not m:\n            binary = f.read(chunksize)\n            if not binary:\n                break\n            pos = 0\n            continue\n        libcinit,glibc,glibcversion,so,threads,soversion = m.groups()\n        if libcinit and not lib:\n            lib = 'libc'\n        elif glibc:\n            if lib != 'glibc':\n                lib = 'glibc'\n                version = glibcversion\n            elif glibcversion > version:\n                version = glibcversion\n        elif so:\n            if lib != 'glibc':\n                lib = 'libc'\n                if soversion and soversion > version:\n                    version = soversion\n                if threads and version[-len(threads):] != threads:\n                    version = version + threads\n        pos = m.end()\n    f.close()\n    return lib,version\n\ndef _dist_try_harder(distname,version,id):\n\n    \"\"\" Tries some special tricks to get the distribution\n        information in case the default method fails.\n\n        Currently supports older SuSE Linux, Caldera OpenLinux and\n        Slackware Linux distributions.\n\n    \"\"\"\n    if os.path.exists('/var/adm/inst-log/info'):\n        # SuSE Linux stores distribution information in that file\n        info = open('/var/adm/inst-log/info').readlines()\n        distname = 'SuSE'\n        for line in info:\n            tv = string.split(line)\n            if len(tv) == 2:\n                tag,value = tv\n            else:\n                continue\n            if tag == 'MIN_DIST_VERSION':\n                version = string.strip(value)\n            elif tag == 'DIST_IDENT':\n                values = string.split(value,'-')\n                id = values[2]\n        return distname,version,id\n\n    if os.path.exists('/etc/.installed'):\n        # Caldera OpenLinux has some infos in that file (thanks to Colin Kong)\n        info = open('/etc/.installed').readlines()\n        for line in info:\n            pkg = string.split(line,'-')\n            if len(pkg) >= 2 and pkg[0] == 'OpenLinux':\n                # XXX does Caldera support non Intel platforms ? If yes,\n                #     where can we find the needed id ?\n                return 'OpenLinux',pkg[1],id\n\n    if os.path.isdir('/usr/lib/setup'):\n        # Check for slackware version tag file (thanks to Greg Andruk)\n        verfiles = os.listdir('/usr/lib/setup')\n        for n in range(len(verfiles)-1, -1, -1):\n            if verfiles[n][:14] != 'slack-version-':\n                del verfiles[n]\n        if verfiles:\n            verfiles.sort()\n            distname = 'slackware'\n            version = verfiles[-1][14:]\n            return distname,version,id\n\n    return distname,version,id\n\n_release_filename = re.compile(r'(\\w+)[-_](release|version)')\n_lsb_release_version = re.compile(r'(.+)'\n                                   ' release '\n                                   '([\\d.]+)'\n                                   '[^(]*(?:\\((.+)\\))?')\n_release_version = re.compile(r'([^0-9]+)'\n                               '(?: release )?'\n                               '([\\d.]+)'\n                               '[^(]*(?:\\((.+)\\))?')\n\n# See also http://www.novell.com/coolsolutions/feature/11251.html\n# and http://linuxmafia.com/faq/Admin/release-files.html\n# and http://data.linux-ntfs.org/rpm/whichrpm\n# and http://www.die.net/doc/linux/man/man1/lsb_release.1.html\n\n_supported_dists = (\n    'SuSE', 'debian', 'fedora', 'redhat', 'centos',\n    'mandrake', 'mandriva', 'rocks', 'slackware', 'yellowdog', 'gentoo',\n    'UnitedLinux', 'turbolinux')\n\ndef _parse_release_file(firstline):\n\n    # Default to empty 'version' and 'id' strings.  Both defaults are used\n    # when 'firstline' is empty.  'id' defaults to empty when an id can not\n    # be deduced.\n    version = ''\n    id = ''\n\n    # Parse the first line\n    m = _lsb_release_version.match(firstline)\n    if m is not None:\n        # LSB format: \"distro release x.x (codename)\"\n        return tuple(m.groups())\n\n    # Pre-LSB format: \"distro x.x (codename)\"\n    m = _release_version.match(firstline)\n    if m is not None:\n        return tuple(m.groups())\n\n    # Unknown format... take the first two words\n    l = string.split(string.strip(firstline))\n    if l:\n        version = l[0]\n        if len(l) > 1:\n            id = l[1]\n    return '', version, id\n\ndef linux_distribution(distname='', version='', id='',\n\n                       supported_dists=_supported_dists,\n                       full_distribution_name=1):\n\n    \"\"\" Tries to determine the name of the Linux OS distribution name.\n\n        The function first looks for a distribution release file in\n        /etc and then reverts to _dist_try_harder() in case no\n        suitable files are found.\n\n        supported_dists may be given to define the set of Linux\n        distributions to look for. It defaults to a list of currently\n        supported Linux distributions identified by their release file\n        name.\n\n        If full_distribution_name is true (default), the full\n        distribution read from the OS is returned. Otherwise the short\n        name taken from supported_dists is used.\n\n        Returns a tuple (distname,version,id) which default to the\n        args given as parameters.\n\n    \"\"\"\n    try:\n        etc = os.listdir('/etc')\n    except os.error:\n        # Probably not a Unix system\n        return distname,version,id\n    etc.sort()\n    for file in etc:\n        m = _release_filename.match(file)\n        if m is not None:\n            _distname,dummy = m.groups()\n            if _distname in supported_dists:\n                distname = _distname\n                break\n    else:\n        return _dist_try_harder(distname,version,id)\n\n    # Read the first line\n    f = open('/etc/'+file, 'r')\n    firstline = f.readline()\n    f.close()\n    _distname, _version, _id = _parse_release_file(firstline)\n\n    if _distname and full_distribution_name:\n        distname = _distname\n    if _version:\n        version = _version\n    if _id:\n        id = _id\n    return distname, version, id\n\n# To maintain backwards compatibility:\n\ndef dist(distname='',version='',id='',\n\n         supported_dists=_supported_dists):\n\n    \"\"\" Tries to determine the name of the Linux OS distribution name.\n\n        The function first looks for a distribution release file in\n        /etc and then reverts to _dist_try_harder() in case no\n        suitable files are found.\n\n        Returns a tuple (distname,version,id) which default to the\n        args given as parameters.\n\n    \"\"\"\n    return linux_distribution(distname, version, id,\n                              supported_dists=supported_dists,\n                              full_distribution_name=0)\n\nclass _popen:\n\n    \"\"\" Fairly portable (alternative) popen implementation.\n\n        This is mostly needed in case os.popen() is not available, or\n        doesn't work as advertised, e.g. in Win9X GUI programs like\n        PythonWin or IDLE.\n\n        Writing to the pipe is currently not supported.\n\n    \"\"\"\n    tmpfile = ''\n    pipe = None\n    bufsize = None\n    mode = 'r'\n\n    def __init__(self,cmd,mode='r',bufsize=None):\n\n        if mode != 'r':\n            raise ValueError,'popen()-emulation only supports read mode'\n        import tempfile\n        self.tmpfile = tmpfile = tempfile.mktemp()\n        os.system(cmd + ' > %s' % tmpfile)\n        self.pipe = open(tmpfile,'rb')\n        self.bufsize = bufsize\n        self.mode = mode\n\n    def read(self):\n\n        return self.pipe.read()\n\n    def readlines(self):\n\n        if self.bufsize is not None:\n            return self.pipe.readlines()\n\n    def close(self,\n\n              remove=os.unlink,error=os.error):\n\n        if self.pipe:\n            rc = self.pipe.close()\n        else:\n            rc = 255\n        if self.tmpfile:\n            try:\n                remove(self.tmpfile)\n            except error:\n                pass\n        return rc\n\n    # Alias\n    __del__ = close\n\ndef popen(cmd, mode='r', bufsize=None):\n\n    \"\"\" Portable popen() interface.\n    \"\"\"\n    # Find a working popen implementation preferring win32pipe.popen\n    # over os.popen over _popen\n    popen = None\n    if os.environ.get('OS','') == 'Windows_NT':\n        # On NT win32pipe should work; on Win9x it hangs due to bugs\n        # in the MS C lib (see MS KnowledgeBase article Q150956)\n        try:\n            import win32pipe\n        except ImportError:\n            pass\n        else:\n            popen = win32pipe.popen\n    if popen is None:\n        if hasattr(os,'popen'):\n            popen = os.popen\n            # Check whether it works... it doesn't in GUI programs\n            # on Windows platforms\n            if sys.platform == 'win32': # XXX Others too ?\n                try:\n                    popen('')\n                except os.error:\n                    popen = _popen\n        else:\n            popen = _popen\n    if bufsize is None:\n        return popen(cmd,mode)\n    else:\n        return popen(cmd,mode,bufsize)\n\ndef _norm_version(version, build=''):\n\n    \"\"\" Normalize the version and build strings and return a single\n        version string using the format major.minor.build (or patchlevel).\n    \"\"\"\n    l = string.split(version,'.')\n    if build:\n        l.append(build)\n    try:\n        ints = map(int,l)\n    except ValueError:\n        strings = l\n    else:\n        strings = map(str,ints)\n    version = string.join(strings[:3],'.')\n    return version\n\n_ver_output = re.compile(r'(?:([\\w ]+) ([\\w.]+) '\n                         '.*'\n                         '\\[.* ([\\d.]+)\\])')\n\n# Examples of VER command output:\n#\n#   Windows 2000:  Microsoft Windows 2000 [Version 5.00.2195]\n#   Windows XP:    Microsoft Windows XP [Version 5.1.2600]\n#   Windows Vista: Microsoft Windows [Version 6.0.6002]\n#\n# Note that the \"Version\" string gets localized on different\n# Windows versions.\n\ndef _syscmd_ver(system='', release='', version='',\n\n               supported_platforms=('win32','win16','dos','os2')):\n\n    \"\"\" Tries to figure out the OS version used and returns\n        a tuple (system,release,version).\n\n        It uses the \"ver\" shell command for this which is known\n        to exists on Windows, DOS and OS/2. XXX Others too ?\n\n        In case this fails, the given parameters are used as\n        defaults.\n\n    \"\"\"\n    if sys.platform not in supported_platforms:\n        return system,release,version\n\n    # Try some common cmd strings\n    for cmd in ('ver','command /c ver','cmd /c ver'):\n        try:\n            pipe = popen(cmd)\n            info = pipe.read()\n            if pipe.close():\n                raise os.error,'command failed'\n            # XXX How can I suppress shell errors from being written\n            #     to stderr ?\n        except os.error,why:\n            #print 'Command %s failed: %s' % (cmd,why)\n            continue\n        except IOError,why:\n            #print 'Command %s failed: %s' % (cmd,why)\n            continue\n        else:\n            break\n    else:\n        return system,release,version\n\n    # Parse the output\n    info = string.strip(info)\n    m = _ver_output.match(info)\n    if m is not None:\n        system,release,version = m.groups()\n        # Strip trailing dots from version and release\n        if release[-1] == '.':\n            release = release[:-1]\n        if version[-1] == '.':\n            version = version[:-1]\n        # Normalize the version and build strings (eliminating additional\n        # zeros)\n        version = _norm_version(version)\n    return system,release,version\n\ndef _win32_getvalue(key,name,default=''):\n\n    \"\"\" Read a value for name from the registry key.\n\n        In case this fails, default is returned.\n\n    \"\"\"\n    try:\n        # Use win32api if available\n        from win32api import RegQueryValueEx\n    except ImportError:\n        # On Python 2.0 and later, emulate using _winreg\n        import _winreg\n        RegQueryValueEx = _winreg.QueryValueEx\n    try:\n        return RegQueryValueEx(key,name)\n    except:\n        return default\n\ndef win32_ver(release='',version='',csd='',ptype=''):\n\n    \"\"\" Get additional version information from the Windows Registry\n        and return a tuple (version,csd,ptype) referring to version\n        number, CSD level (service pack), and OS type (multi/single\n        processor).\n\n        As a hint: ptype returns 'Uniprocessor Free' on single\n        processor NT machines and 'Multiprocessor Free' on multi\n        processor machines. The 'Free' refers to the OS version being\n        free of debugging code. It could also state 'Checked' which\n        means the OS version uses debugging code, i.e. code that\n        checks arguments, ranges, etc. (Thomas Heller).\n\n        Note: this function works best with Mark Hammond's win32\n        package installed, but also on Python 2.3 and later. It\n        obviously only runs on Win32 compatible platforms.\n\n    \"\"\"\n    # XXX Is there any way to find out the processor type on WinXX ?\n    # XXX Is win32 available on Windows CE ?\n    #\n    # Adapted from code posted by Karl Putland to comp.lang.python.\n    #\n    # The mappings between reg. values and release names can be found\n    # here: http://msdn.microsoft.com/library/en-us/sysinfo/base/osversioninfo_str.asp\n\n    # Import the needed APIs\n    try:\n        import win32api\n        from win32api import RegQueryValueEx, RegOpenKeyEx, \\\n             RegCloseKey, GetVersionEx\n        from win32con import HKEY_LOCAL_MACHINE, VER_PLATFORM_WIN32_NT, \\\n             VER_PLATFORM_WIN32_WINDOWS, VER_NT_WORKSTATION\n    except ImportError:\n        # Emulate the win32api module using Python APIs\n        try:\n            sys.getwindowsversion\n        except AttributeError:\n            # No emulation possible, so return the defaults...\n            return release,version,csd,ptype\n        else:\n            # Emulation using _winreg (added in Python 2.0) and\n            # sys.getwindowsversion() (added in Python 2.3)\n            import _winreg\n            GetVersionEx = sys.getwindowsversion\n            RegQueryValueEx = _winreg.QueryValueEx\n            RegOpenKeyEx = _winreg.OpenKeyEx\n            RegCloseKey = _winreg.CloseKey\n            HKEY_LOCAL_MACHINE = _winreg.HKEY_LOCAL_MACHINE\n            VER_PLATFORM_WIN32_WINDOWS = 1\n            VER_PLATFORM_WIN32_NT = 2\n            VER_NT_WORKSTATION = 1\n            VER_NT_SERVER = 3\n            REG_SZ = 1\n\n    # Find out the registry key and some general version infos\n    winver = GetVersionEx()\n    maj,min,buildno,plat,csd = winver\n    version = '%i.%i.%i' % (maj,min,buildno & 0xFFFF)\n    if hasattr(winver, \"service_pack\"):\n        if winver.service_pack != \"\":\n            csd = 'SP%s' % winver.service_pack_major\n    else:\n        if csd[:13] == 'Service Pack ':\n            csd = 'SP' + csd[13:]\n\n    if plat == VER_PLATFORM_WIN32_WINDOWS:\n        regkey = 'SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion'\n        # Try to guess the release name\n        if maj == 4:\n            if min == 0:\n                release = '95'\n            elif min == 10:\n                release = '98'\n            elif min == 90:\n                release = 'Me'\n            else:\n                release = 'postMe'\n        elif maj == 5:\n            release = '2000'\n\n    elif plat == VER_PLATFORM_WIN32_NT:\n        regkey = 'SOFTWARE\\\\Microsoft\\\\Windows NT\\\\CurrentVersion'\n        if maj <= 4:\n            release = 'NT'\n        elif maj == 5:\n            if min == 0:\n                release = '2000'\n            elif min == 1:\n                release = 'XP'\n            elif min == 2:\n                release = '2003Server'\n            else:\n                release = 'post2003'\n        elif maj == 6:\n            if hasattr(winver, \"product_type\"):\n                product_type = winver.product_type\n            else:\n                product_type = VER_NT_WORKSTATION\n                # Without an OSVERSIONINFOEX capable sys.getwindowsversion(),\n                # or help from the registry, we cannot properly identify\n                # non-workstation versions.\n                try:\n                    key = RegOpenKeyEx(HKEY_LOCAL_MACHINE, regkey)\n                    name, type = RegQueryValueEx(key, \"ProductName\")\n                    # Discard any type that isn't REG_SZ\n                    if type == REG_SZ and name.find(\"Server\") != -1:\n                        product_type = VER_NT_SERVER\n                except WindowsError:\n                    # Use default of VER_NT_WORKSTATION\n                    pass\n\n            if min == 0:\n                if product_type == VER_NT_WORKSTATION:\n                    release = 'Vista'\n                else:\n                    release = '2008Server'\n            elif min == 1:\n                if product_type == VER_NT_WORKSTATION:\n                    release = '7'\n                else:\n                    release = '2008ServerR2'\n            elif min == 2:\n                if product_type == VER_NT_WORKSTATION:\n                    release = '8'\n                else:\n                    release = '2012Server'\n            else:\n                release = 'post2012Server'\n\n    else:\n        if not release:\n            # E.g. Win3.1 with win32s\n            release = '%i.%i' % (maj,min)\n        return release,version,csd,ptype\n\n    # Open the registry key\n    try:\n        keyCurVer = RegOpenKeyEx(HKEY_LOCAL_MACHINE, regkey)\n        # Get a value to make sure the key exists...\n        RegQueryValueEx(keyCurVer, 'SystemRoot')\n    except:\n        return release,version,csd,ptype\n\n    # Parse values\n    #subversion = _win32_getvalue(keyCurVer,\n    #                            'SubVersionNumber',\n    #                            ('',1))[0]\n    #if subversion:\n    #   release = release + subversion # 95a, 95b, etc.\n    build = _win32_getvalue(keyCurVer,\n                            'CurrentBuildNumber',\n                            ('',1))[0]\n    ptype = _win32_getvalue(keyCurVer,\n                           'CurrentType',\n                           (ptype,1))[0]\n\n    # Normalize version\n    version = _norm_version(version,build)\n\n    # Close key\n    RegCloseKey(keyCurVer)\n    return release,version,csd,ptype\n\ndef _mac_ver_lookup(selectors,default=None):\n\n    from gestalt import gestalt\n    import MacOS\n    l = []\n    append = l.append\n    for selector in selectors:\n        try:\n            append(gestalt(selector))\n        except (RuntimeError, MacOS.Error):\n            append(default)\n    return l\n\ndef _bcd2str(bcd):\n\n    return hex(bcd)[2:]\n\ndef _mac_ver_gestalt():\n    \"\"\"\n        Thanks to Mark R. Levinson for mailing documentation links and\n        code examples for this function. Documentation for the\n        gestalt() API is available online at:\n\n           http://www.rgaros.nl/gestalt/\n    \"\"\"\n    # Check whether the version info module is available\n    try:\n        import gestalt\n        import MacOS\n    except ImportError:\n        return None\n    # Get the infos\n    sysv,sysa = _mac_ver_lookup(('sysv','sysa'))\n    # Decode the infos\n    if sysv:\n        major = (sysv & 0xFF00) >> 8\n        minor = (sysv & 0x00F0) >> 4\n        patch = (sysv & 0x000F)\n\n        if (major, minor) >= (10, 4):\n            # the 'sysv' gestald cannot return patchlevels\n            # higher than 9. Apple introduced 3 new\n            # gestalt codes in 10.4 to deal with this\n            # issue (needed because patch levels can\n            # run higher than 9, such as 10.4.11)\n            major,minor,patch = _mac_ver_lookup(('sys1','sys2','sys3'))\n            release = '%i.%i.%i' %(major, minor, patch)\n        else:\n            release = '%s.%i.%i' % (_bcd2str(major),minor,patch)\n\n    if sysa:\n        machine = {0x1: '68k',\n                   0x2: 'PowerPC',\n                   0xa: 'i386'}.get(sysa,'')\n\n    versioninfo=('', '', '')\n    return release,versioninfo,machine\n\ndef _mac_ver_xml():\n    fn = '/System/Library/CoreServices/SystemVersion.plist'\n    if not os.path.exists(fn):\n        return None\n\n    try:\n        import plistlib\n    except ImportError:\n        return None\n\n    pl = plistlib.readPlist(fn)\n    release = pl['ProductVersion']\n    versioninfo=('', '', '')\n    machine = os.uname()[4]\n    if machine in ('ppc', 'Power Macintosh'):\n        # for compatibility with the gestalt based code\n        machine = 'PowerPC'\n\n    return release,versioninfo,machine\n\n\ndef mac_ver(release='',versioninfo=('','',''),machine=''):\n\n    \"\"\" Get MacOS version information and return it as tuple (release,\n        versioninfo, machine) with versioninfo being a tuple (version,\n        dev_stage, non_release_version).\n\n        Entries which cannot be determined are set to the parameter values\n        which default to ''. All tuple entries are strings.\n    \"\"\"\n\n    # First try reading the information from an XML file which should\n    # always be present\n    info = _mac_ver_xml()\n    if info is not None:\n        return info\n\n    # If that doesn't work for some reason fall back to reading the\n    # information using gestalt calls.\n    info = _mac_ver_gestalt()\n    if info is not None:\n        return info\n\n    # If that also doesn't work return the default values\n    return release,versioninfo,machine\n\ndef _java_getprop(name,default):\n\n    from java.lang import System\n    try:\n        value = System.getProperty(name)\n        if value is None:\n            return default\n        return value\n    except AttributeError:\n        return default\n\ndef java_ver(release='',vendor='',vminfo=('','',''),osinfo=('','','')):\n\n    \"\"\" Version interface for Jython.\n\n        Returns a tuple (release,vendor,vminfo,osinfo) with vminfo being\n        a tuple (vm_name,vm_release,vm_vendor) and osinfo being a\n        tuple (os_name,os_version,os_arch).\n\n        Values which cannot be determined are set to the defaults\n        given as parameters (which all default to '').\n\n    \"\"\"\n    # Import the needed APIs\n    try:\n        import java.lang\n    except ImportError:\n        return release,vendor,vminfo,osinfo\n\n    vendor = _java_getprop('java.vendor', vendor)\n    release = _java_getprop('java.version', release)\n    vm_name, vm_release, vm_vendor = vminfo\n    vm_name = _java_getprop('java.vm.name', vm_name)\n    vm_vendor = _java_getprop('java.vm.vendor', vm_vendor)\n    vm_release = _java_getprop('java.vm.version', vm_release)\n    vminfo = vm_name, vm_release, vm_vendor\n    os_name, os_version, os_arch = osinfo\n    os_arch = _java_getprop('java.os.arch', os_arch)\n    os_name = _java_getprop('java.os.name', os_name)\n    os_version = _java_getprop('java.os.version', os_version)\n    osinfo = os_name, os_version, os_arch\n\n    return release, vendor, vminfo, osinfo\n\n### System name aliasing\n\ndef system_alias(system,release,version):\n\n    \"\"\" Returns (system,release,version) aliased to common\n        marketing names used for some systems.\n\n        It also does some reordering of the information in some cases\n        where it would otherwise cause confusion.\n\n    \"\"\"\n    if system == 'Rhapsody':\n        # Apple's BSD derivative\n        # XXX How can we determine the marketing release number ?\n        return 'MacOS X Server',system+release,version\n\n    elif system == 'SunOS':\n        # Sun's OS\n        if release < '5':\n            # These releases use the old name SunOS\n            return system,release,version\n        # Modify release (marketing release = SunOS release - 3)\n        l = string.split(release,'.')\n        if l:\n            try:\n                major = int(l[0])\n            except ValueError:\n                pass\n            else:\n                major = major - 3\n                l[0] = str(major)\n                release = string.join(l,'.')\n        if release < '6':\n            system = 'Solaris'\n        else:\n            # XXX Whatever the new SunOS marketing name is...\n            system = 'Solaris'\n\n    elif system == 'IRIX64':\n        # IRIX reports IRIX64 on platforms with 64-bit support; yet it\n        # is really a version and not a different platform, since 32-bit\n        # apps are also supported..\n        system = 'IRIX'\n        if version:\n            version = version + ' (64bit)'\n        else:\n            version = '64bit'\n\n    elif system in ('win32','win16'):\n        # In case one of the other tricks\n        system = 'Windows'\n\n    return system,release,version\n\n### Various internal helpers\n\ndef _platform(*args):\n\n    \"\"\" Helper to format the platform string in a filename\n        compatible format e.g. \"system-version-machine\".\n    \"\"\"\n    # Format the platform string\n    platform = string.join(\n        map(string.strip,\n            filter(len, args)),\n        '-')\n\n    # Cleanup some possible filename obstacles...\n    replace = string.replace\n    platform = replace(platform,' ','_')\n    platform = replace(platform,'/','-')\n    platform = replace(platform,'\\\\','-')\n    platform = replace(platform,':','-')\n    platform = replace(platform,';','-')\n    platform = replace(platform,'\"','-')\n    platform = replace(platform,'(','-')\n    platform = replace(platform,')','-')\n\n    # No need to report 'unknown' information...\n    platform = replace(platform,'unknown','')\n\n    # Fold '--'s and remove trailing '-'\n    while 1:\n        cleaned = replace(platform,'--','-')\n        if cleaned == platform:\n            break\n        platform = cleaned\n    while platform[-1] == '-':\n        platform = platform[:-1]\n\n    return platform\n\ndef _node(default=''):\n\n    \"\"\" Helper to determine the node name of this machine.\n    \"\"\"\n    try:\n        import socket\n    except ImportError:\n        # No sockets...\n        return default\n    try:\n        return socket.gethostname()\n    except socket.error:\n        # Still not working...\n        return default\n\n# os.path.abspath is new in Python 1.5.2:\nif not hasattr(os.path,'abspath'):\n\n    def _abspath(path,\n\n                 isabs=os.path.isabs,join=os.path.join,getcwd=os.getcwd,\n                 normpath=os.path.normpath):\n\n        if not isabs(path):\n            path = join(getcwd(), path)\n        return normpath(path)\n\nelse:\n\n    _abspath = os.path.abspath\n\ndef _follow_symlinks(filepath):\n\n    \"\"\" In case filepath is a symlink, follow it until a\n        real file is reached.\n    \"\"\"\n    filepath = _abspath(filepath)\n    while os.path.islink(filepath):\n        filepath = os.path.normpath(\n            os.path.join(os.path.dirname(filepath),os.readlink(filepath)))\n    return filepath\n\ndef _syscmd_uname(option,default=''):\n\n    \"\"\" Interface to the system's uname command.\n    \"\"\"\n    if sys.platform in ('dos','win32','win16','os2'):\n        # XXX Others too ?\n        return default\n    try:\n        f = os.popen('uname %s 2> %s' % (option, DEV_NULL))\n    except (AttributeError,os.error):\n        return default\n    output = string.strip(f.read())\n    rc = f.close()\n    if not output or rc:\n        return default\n    else:\n        return output\n\ndef _syscmd_file(target,default=''):\n\n    \"\"\" Interface to the system's file command.\n\n        The function uses the -b option of the file command to have it\n        ommit the filename in its output and if possible the -L option\n        to have the command follow symlinks. It returns default in\n        case the command should fail.\n\n    \"\"\"\n\n    # We do the import here to avoid a bootstrap issue.\n    # See c73b90b6dadd changeset.\n    #\n    # [..]\n    # ranlib libpython2.7.a\n    # gcc   -o python \\\n    #        Modules/python.o \\\n    #        libpython2.7.a -lsocket -lnsl -ldl    -lm\n    # Traceback (most recent call last):\n    #  File \"./setup.py\", line 8, in <module>\n    #    from platform import machine as platform_machine\n    #  File \"[..]/build/Lib/platform.py\", line 116, in <module>\n    #    import sys,string,os,re,subprocess\n    #  File \"[..]/build/Lib/subprocess.py\", line 429, in <module>\n    #    import select\n    # ImportError: No module named select\n\n    import subprocess\n\n    if sys.platform in ('dos','win32','win16','os2'):\n        # XXX Others too ?\n        return default\n    target = _follow_symlinks(target)\n    try:\n        proc = subprocess.Popen(['file', target],\n                stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\n    except (AttributeError,os.error):\n        return default\n    output = proc.communicate()[0]\n    rc = proc.wait()\n    if not output or rc:\n        return default\n    else:\n        return output\n\n### Information about the used architecture\n\n# Default values for architecture; non-empty strings override the\n# defaults given as parameters\n_default_architecture = {\n    'win32': ('','WindowsPE'),\n    'win16': ('','Windows'),\n    'dos': ('','MSDOS'),\n}\n\n_architecture_split = re.compile(r'[\\s,]').split\n\ndef architecture(executable=sys.executable,bits='',linkage=''):\n\n    \"\"\" Queries the given executable (defaults to the Python interpreter\n        binary) for various architecture information.\n\n        Returns a tuple (bits,linkage) which contains information about\n        the bit architecture and the linkage format used for the\n        executable. Both values are returned as strings.\n\n        Values that cannot be determined are returned as given by the\n        parameter presets. If bits is given as '', the sizeof(pointer)\n        (or sizeof(long) on Python version < 1.5.2) is used as\n        indicator for the supported pointer size.\n\n        The function relies on the system's \"file\" command to do the\n        actual work. This is available on most if not all Unix\n        platforms. On some non-Unix platforms where the \"file\" command\n        does not exist and the executable is set to the Python interpreter\n        binary defaults from _default_architecture are used.\n\n    \"\"\"\n    # Use the sizeof(pointer) as default number of bits if nothing\n    # else is given as default.\n    if not bits:\n        import struct\n        try:\n            size = struct.calcsize('P')\n        except struct.error:\n            # Older installations can only query longs\n            size = struct.calcsize('l')\n        bits = str(size*8) + 'bit'\n\n    # Get data from the 'file' system command\n    if executable:\n        output = _syscmd_file(executable, '')\n    else:\n        output = ''\n\n    if not output and \\\n       executable == sys.executable:\n        # \"file\" command did not return anything; we'll try to provide\n        # some sensible defaults then...\n        if sys.platform in _default_architecture:\n            b, l = _default_architecture[sys.platform]\n            if b:\n                bits = b\n            if l:\n                linkage = l\n        return bits, linkage\n\n    # Split the output into a list of strings omitting the filename\n    fileout = _architecture_split(output)[1:]\n\n    if 'executable' not in fileout:\n        # Format not supported\n        return bits,linkage\n\n    # Bits\n    if '32-bit' in fileout:\n        bits = '32bit'\n    elif 'N32' in fileout:\n        # On Irix only\n        bits = 'n32bit'\n    elif '64-bit' in fileout:\n        bits = '64bit'\n\n    # Linkage\n    if 'ELF' in fileout:\n        linkage = 'ELF'\n    elif 'PE' in fileout:\n        # E.g. Windows uses this format\n        if 'Windows' in fileout:\n            linkage = 'WindowsPE'\n        else:\n            linkage = 'PE'\n    elif 'COFF' in fileout:\n        linkage = 'COFF'\n    elif 'MS-DOS' in fileout:\n        linkage = 'MSDOS'\n    else:\n        # XXX the A.OUT format also falls under this class...\n        pass\n\n    return bits,linkage\n\n### Portable uname() interface\n\n_uname_cache = None\n\ndef uname():\n\n    \"\"\" Fairly portable uname interface. Returns a tuple\n        of strings (system,node,release,version,machine,processor)\n        identifying the underlying platform.\n\n        Note that unlike the os.uname function this also returns\n        possible processor information as an additional tuple entry.\n\n        Entries which cannot be determined are set to ''.\n\n    \"\"\"\n    global _uname_cache\n    no_os_uname = 0\n\n    if _uname_cache is not None:\n        return _uname_cache\n\n    processor = ''\n\n    # Get some infos from the builtin os.uname API...\n    try:\n        system,node,release,version,machine = os.uname()\n    except AttributeError:\n        no_os_uname = 1\n\n    if no_os_uname or not filter(None, (system, node, release, version, machine)):\n        # Hmm, no there is either no uname or uname has returned\n        #'unknowns'... we'll have to poke around the system then.\n        if no_os_uname:\n            system = sys.platform\n            release = ''\n            version = ''\n            node = _node()\n            machine = ''\n\n        use_syscmd_ver = 1\n\n        # Try win32_ver() on win32 platforms\n        if system == 'win32':\n            release,version,csd,ptype = win32_ver()\n            if release and version:\n                use_syscmd_ver = 0\n            # Try to use the PROCESSOR_* environment variables\n            # available on Win XP and later; see\n            # http://support.microsoft.com/kb/888731 and\n            # http://www.geocities.com/rick_lively/MANUALS/ENV/MSWIN/PROCESSI.HTM\n            if not machine:\n                # WOW64 processes mask the native architecture\n                if \"PROCESSOR_ARCHITEW6432\" in os.environ:\n                    machine = os.environ.get(\"PROCESSOR_ARCHITEW6432\", '')\n                else:\n                    machine = os.environ.get('PROCESSOR_ARCHITECTURE', '')\n            if not processor:\n                processor = os.environ.get('PROCESSOR_IDENTIFIER', machine)\n\n        # Try the 'ver' system command available on some\n        # platforms\n        if use_syscmd_ver:\n            system,release,version = _syscmd_ver(system)\n            # Normalize system to what win32_ver() normally returns\n            # (_syscmd_ver() tends to return the vendor name as well)\n            if system == 'Microsoft Windows':\n                system = 'Windows'\n            elif system == 'Microsoft' and release == 'Windows':\n                # Under Windows Vista and Windows Server 2008,\n                # Microsoft changed the output of the ver command. The\n                # release is no longer printed.  This causes the\n                # system and release to be misidentified.\n                system = 'Windows'\n                if '6.0' == version[:3]:\n                    release = 'Vista'\n                else:\n                    release = ''\n\n        # In case we still don't know anything useful, we'll try to\n        # help ourselves\n        if system in ('win32','win16'):\n            if not version:\n                if system == 'win32':\n                    version = '32bit'\n                else:\n                    version = '16bit'\n            system = 'Windows'\n\n        elif system[:4] == 'java':\n            release,vendor,vminfo,osinfo = java_ver()\n            system = 'Java'\n            version = string.join(vminfo,', ')\n            if not version:\n                version = vendor\n\n    # System specific extensions\n    if system == 'OpenVMS':\n        # OpenVMS seems to have release and version mixed up\n        if not release or release == '0':\n            release = version\n            version = ''\n        # Get processor information\n        try:\n            import vms_lib\n        except ImportError:\n            pass\n        else:\n            csid, cpu_number = vms_lib.getsyi('SYI$_CPU',0)\n            if (cpu_number >= 128):\n                processor = 'Alpha'\n            else:\n                processor = 'VAX'\n    if not processor:\n        # Get processor information from the uname system command\n        processor = _syscmd_uname('-p','')\n\n    #If any unknowns still exist, replace them with ''s, which are more portable\n    if system == 'unknown':\n        system = ''\n    if node == 'unknown':\n        node = ''\n    if release == 'unknown':\n        release = ''\n    if version == 'unknown':\n        version = ''\n    if machine == 'unknown':\n        machine = ''\n    if processor == 'unknown':\n        processor = ''\n\n    #  normalize name\n    if system == 'Microsoft' and release == 'Windows':\n        system = 'Windows'\n        release = 'Vista'\n\n    _uname_cache = system,node,release,version,machine,processor\n    return _uname_cache\n\n### Direct interfaces to some of the uname() return values\n\ndef system():\n\n    \"\"\" Returns the system/OS name, e.g. 'Linux', 'Windows' or 'Java'.\n\n        An empty string is returned if the value cannot be determined.\n\n    \"\"\"\n    return uname()[0]\n\ndef node():\n\n    \"\"\" Returns the computer's network name (which may not be fully\n        qualified)\n\n        An empty string is returned if the value cannot be determined.\n\n    \"\"\"\n    return uname()[1]\n\ndef release():\n\n    \"\"\" Returns the system's release, e.g. '2.2.0' or 'NT'\n\n        An empty string is returned if the value cannot be determined.\n\n    \"\"\"\n    return uname()[2]\n\ndef version():\n\n    \"\"\" Returns the system's release version, e.g. '#3 on degas'\n\n        An empty string is returned if the value cannot be determined.\n\n    \"\"\"\n    return uname()[3]\n\ndef machine():\n\n    \"\"\" Returns the machine type, e.g. 'i386'\n\n        An empty string is returned if the value cannot be determined.\n\n    \"\"\"\n    return uname()[4]\n\ndef processor():\n\n    \"\"\" Returns the (true) processor name, e.g. 'amdk6'\n\n        An empty string is returned if the value cannot be\n        determined. Note that many platforms do not provide this\n        information or simply return the same value as for machine(),\n        e.g.  NetBSD does this.\n\n    \"\"\"\n    return uname()[5]\n\n### Various APIs for extracting information from sys.version\n\n_sys_version_parser = re.compile(\n    r'([\\w.+]+)\\s*'\n    '\\(#?([^,]+),\\s*([\\w ]+),\\s*([\\w :]+)\\)\\s*'\n    '\\[([^\\]]+)\\]?')\n\n_ironpython_sys_version_parser = re.compile(\n    r'IronPython\\s*'\n    '([\\d\\.]+)'\n    '(?: \\(([\\d\\.]+)\\))?'\n    ' on (.NET [\\d\\.]+)')\n\n# IronPython covering 2.6 and 2.7\n_ironpython26_sys_version_parser = re.compile(\n    r'([\\d.]+)\\s*'\n    '\\(IronPython\\s*'\n    '[\\d.]+\\s*'\n    '\\(([\\d.]+)\\) on ([\\w.]+ [\\d.]+(?: \\(\\d+-bit\\))?)\\)'\n)\n\n_pypy_sys_version_parser = re.compile(\n    r'([\\w.+]+)\\s*'\n    '\\(#?([^,]+),\\s*([\\w ]+),\\s*([\\w :]+)\\)\\s*'\n    '\\[PyPy [^\\]]+\\]?')\n\n_sys_version_cache = {}\n\ndef _sys_version(sys_version=None):\n\n    \"\"\" Returns a parsed version of Python's sys.version as tuple\n        (name, version, branch, revision, buildno, builddate, compiler)\n        referring to the Python implementation name, version, branch,\n        revision, build number, build date/time as string and the compiler\n        identification string.\n\n        Note that unlike the Python sys.version, the returned value\n        for the Python version will always include the patchlevel (it\n        defaults to '.0').\n\n        The function returns empty strings for tuple entries that\n        cannot be determined.\n\n        sys_version may be given to parse an alternative version\n        string, e.g. if the version was read from a different Python\n        interpreter.\n\n    \"\"\"\n    # Get the Python version\n    if sys_version is None:\n        sys_version = sys.version\n\n    # Try the cache first\n    result = _sys_version_cache.get(sys_version, None)\n    if result is not None:\n        return result\n\n    # Parse it\n    if 'IronPython' in sys_version:\n        # IronPython\n        name = 'IronPython'\n        if sys_version.startswith('IronPython'):\n            match = _ironpython_sys_version_parser.match(sys_version)\n        else:\n            match = _ironpython26_sys_version_parser.match(sys_version)\n\n        if match is None:\n            raise ValueError(\n                'failed to parse IronPython sys.version: %s' %\n                repr(sys_version))\n\n        version, alt_version, compiler = match.groups()\n        buildno = ''\n        builddate = ''\n\n    elif sys.platform.startswith('java'):\n        # Jython\n        name = 'Jython'\n        match = _sys_version_parser.match(sys_version)\n        if match is None:\n            raise ValueError(\n                'failed to parse Jython sys.version: %s' %\n                repr(sys_version))\n        version, buildno, builddate, buildtime, _ = match.groups()\n        compiler = sys.platform\n\n    elif \"PyPy\" in sys_version:\n        # PyPy\n        name = \"PyPy\"\n        match = _pypy_sys_version_parser.match(sys_version)\n        if match is None:\n            raise ValueError(\"failed to parse PyPy sys.version: %s\" %\n                             repr(sys_version))\n        version, buildno, builddate, buildtime = match.groups()\n        compiler = \"\"\n\n    else:\n        # CPython\n        match = _sys_version_parser.match(sys_version)\n        if match is None:\n            raise ValueError(\n                'failed to parse CPython sys.version: %s' %\n                repr(sys_version))\n        version, buildno, builddate, buildtime, compiler = \\\n              match.groups()\n        name = 'CPython'\n        builddate = builddate + ' ' + buildtime\n\n    if hasattr(sys, 'subversion'):\n        # sys.subversion was added in Python 2.5\n        _, branch, revision = sys.subversion\n    else:\n        branch = ''\n        revision = ''\n\n    # Add the patchlevel version if missing\n    l = string.split(version, '.')\n    if len(l) == 2:\n        l.append('0')\n        version = string.join(l, '.')\n\n    # Build and cache the result\n    result = (name, version, branch, revision, buildno, builddate, compiler)\n    _sys_version_cache[sys_version] = result\n    return result\n\ndef python_implementation():\n\n    \"\"\" Returns a string identifying the Python implementation.\n\n        Currently, the following implementations are identified:\n          'CPython' (C implementation of Python),\n          'IronPython' (.NET implementation of Python),\n          'Jython' (Java implementation of Python),\n          'PyPy' (Python implementation of Python).\n\n    \"\"\"\n    return _sys_version()[0]\n\ndef python_version():\n\n    \"\"\" Returns the Python version as string 'major.minor.patchlevel'\n\n        Note that unlike the Python sys.version, the returned value\n        will always include the patchlevel (it defaults to 0).\n\n    \"\"\"\n    return _sys_version()[1]\n\ndef python_version_tuple():\n\n    \"\"\" Returns the Python version as tuple (major, minor, patchlevel)\n        of strings.\n\n        Note that unlike the Python sys.version, the returned value\n        will always include the patchlevel (it defaults to 0).\n\n    \"\"\"\n    return tuple(string.split(_sys_version()[1], '.'))\n\ndef python_branch():\n\n    \"\"\" Returns a string identifying the Python implementation\n        branch.\n\n        For CPython this is the Subversion branch from which the\n        Python binary was built.\n\n        If not available, an empty string is returned.\n\n    \"\"\"\n\n    return _sys_version()[2]\n\ndef python_revision():\n\n    \"\"\" Returns a string identifying the Python implementation\n        revision.\n\n        For CPython this is the Subversion revision from which the\n        Python binary was built.\n\n        If not available, an empty string is returned.\n\n    \"\"\"\n    return _sys_version()[3]\n\ndef python_build():\n\n    \"\"\" Returns a tuple (buildno, builddate) stating the Python\n        build number and date as strings.\n\n    \"\"\"\n    return _sys_version()[4:6]\n\ndef python_compiler():\n\n    \"\"\" Returns a string identifying the compiler used for compiling\n        Python.\n\n    \"\"\"\n    return _sys_version()[6]\n\n### The Opus Magnum of platform strings :-)\n\n_platform_cache = {}\n\ndef platform(aliased=0, terse=0):\n\n    \"\"\" Returns a single string identifying the underlying platform\n        with as much useful information as possible (but no more :).\n\n        The output is intended to be human readable rather than\n        machine parseable. It may look different on different\n        platforms and this is intended.\n\n        If \"aliased\" is true, the function will use aliases for\n        various platforms that report system names which differ from\n        their common names, e.g. SunOS will be reported as\n        Solaris. The system_alias() function is used to implement\n        this.\n\n        Setting terse to true causes the function to return only the\n        absolute minimum information needed to identify the platform.\n\n    \"\"\"\n    result = _platform_cache.get((aliased, terse), None)\n    if result is not None:\n        return result\n\n    # Get uname information and then apply platform specific cosmetics\n    # to it...\n    system,node,release,version,machine,processor = uname()\n    if machine == processor:\n        processor = ''\n    if aliased:\n        system,release,version = system_alias(system,release,version)\n\n    if system == 'Windows':\n        # MS platforms\n        rel,vers,csd,ptype = win32_ver(version)\n        if terse:\n            platform = _platform(system,release)\n        else:\n            platform = _platform(system,release,version,csd)\n\n    elif system in ('Linux',):\n        # Linux based systems\n        distname,distversion,distid = dist('')\n        if distname and not terse:\n            platform = _platform(system,release,machine,processor,\n                                 'with',\n                                 distname,distversion,distid)\n        else:\n            # If the distribution name is unknown check for libc vs. glibc\n            libcname,libcversion = libc_ver(sys.executable)\n            platform = _platform(system,release,machine,processor,\n                                 'with',\n                                 libcname+libcversion)\n    elif system == 'Java':\n        # Java platforms\n        r,v,vminfo,(os_name,os_version,os_arch) = java_ver()\n        if terse or not os_name:\n            platform = _platform(system,release,version)\n        else:\n            platform = _platform(system,release,version,\n                                 'on',\n                                 os_name,os_version,os_arch)\n\n    elif system == 'MacOS':\n        # MacOS platforms\n        if terse:\n            platform = _platform(system,release)\n        else:\n            platform = _platform(system,release,machine)\n\n    else:\n        # Generic handler\n        if terse:\n            platform = _platform(system,release)\n        else:\n            bits,linkage = architecture(sys.executable)\n            platform = _platform(system,release,machine,processor,bits,linkage)\n\n    _platform_cache[(aliased, terse)] = platform\n    return platform\n\n### Command line interface\n\nif __name__ == '__main__':\n    # Default is to print the aliased verbose platform string\n    terse = ('terse' in sys.argv or '--terse' in sys.argv)\n    aliased = (not 'nonaliased' in sys.argv and not '--nonaliased' in sys.argv)\n    print platform(aliased,terse)\n    sys.exit(0)\n",
		"file_name": "platform.py"
	},
	{
		"content": "r\"\"\"plistlib.py -- a tool to generate and parse MacOSX .plist files.\n\nThe PropertyList (.plist) file format is a simple XML pickle supporting\nbasic object types, like dictionaries, lists, numbers and strings.\nUsually the top level object is a dictionary.\n\nTo write out a plist file, use the writePlist(rootObject, pathOrFile)\nfunction. 'rootObject' is the top level object, 'pathOrFile' is a\nfilename or a (writable) file object.\n\nTo parse a plist from a file, use the readPlist(pathOrFile) function,\nwith a file name or a (readable) file object as the only argument. It\nreturns the top level object (again, usually a dictionary).\n\nTo work with plist data in strings, you can use readPlistFromString()\nand writePlistToString().\n\nValues can be strings, integers, floats, booleans, tuples, lists,\ndictionaries, Data or datetime.datetime objects. String values (including\ndictionary keys) may be unicode strings -- they will be written out as\nUTF-8.\n\nThe <data> plist type is supported through the Data class. This is a\nthin wrapper around a Python string.\n\nGenerate Plist example:\n\n    pl = dict(\n        aString=\"Doodah\",\n        aList=[\"A\", \"B\", 12, 32.1, [1, 2, 3]],\n        aFloat=0.1,\n        anInt=728,\n        aDict=dict(\n            anotherString=\"<hello & hi there!>\",\n            aUnicodeValue=u'M\\xe4ssig, Ma\\xdf',\n            aTrueValue=True,\n            aFalseValue=False,\n        ),\n        someData=Data(\"<binary gunk>\"),\n        someMoreData=Data(\"<lots of binary gunk>\" * 10),\n        aDate=datetime.datetime.fromtimestamp(time.mktime(time.gmtime())),\n    )\n    # unicode keys are possible, but a little awkward to use:\n    pl[u'\\xc5benraa'] = \"That was a unicode key.\"\n    writePlist(pl, fileName)\n\nParse Plist example:\n\n    pl = readPlist(pathOrFile)\n    print pl[\"aKey\"]\n\"\"\"\n\n\n__all__ = [\n    \"readPlist\", \"writePlist\", \"readPlistFromString\", \"writePlistToString\",\n    \"readPlistFromResource\", \"writePlistToResource\",\n    \"Plist\", \"Data\", \"Dict\"\n]\n# Note: the Plist and Dict classes have been deprecated.\n\nimport binascii\nimport datetime\nfrom cStringIO import StringIO\nimport re\nimport warnings\n\n\ndef readPlist(pathOrFile):\n    \"\"\"Read a .plist file. 'pathOrFile' may either be a file name or a\n    (readable) file object. Return the unpacked root object (which\n    usually is a dictionary).\n    \"\"\"\n    didOpen = 0\n    if isinstance(pathOrFile, (str, unicode)):\n        pathOrFile = open(pathOrFile)\n        didOpen = 1\n    p = PlistParser()\n    rootObject = p.parse(pathOrFile)\n    if didOpen:\n        pathOrFile.close()\n    return rootObject\n\n\ndef writePlist(rootObject, pathOrFile):\n    \"\"\"Write 'rootObject' to a .plist file. 'pathOrFile' may either be a\n    file name or a (writable) file object.\n    \"\"\"\n    didOpen = 0\n    if isinstance(pathOrFile, (str, unicode)):\n        pathOrFile = open(pathOrFile, \"w\")\n        didOpen = 1\n    writer = PlistWriter(pathOrFile)\n    writer.writeln(\"<plist version=\\\"1.0\\\">\")\n    writer.writeValue(rootObject)\n    writer.writeln(\"</plist>\")\n    if didOpen:\n        pathOrFile.close()\n\n\ndef readPlistFromString(data):\n    \"\"\"Read a plist data from a string. Return the root object.\n    \"\"\"\n    return readPlist(StringIO(data))\n\n\ndef writePlistToString(rootObject):\n    \"\"\"Return 'rootObject' as a plist-formatted string.\n    \"\"\"\n    f = StringIO()\n    writePlist(rootObject, f)\n    return f.getvalue()\n\n\ndef readPlistFromResource(path, restype='plst', resid=0):\n    \"\"\"Read plst resource from the resource fork of path.\n    \"\"\"\n    warnings.warnpy3k(\"In 3.x, readPlistFromResource is removed.\",\n                      stacklevel=2)\n    from Carbon.File import FSRef, FSGetResourceForkName\n    from Carbon.Files import fsRdPerm\n    from Carbon import Res\n    fsRef = FSRef(path)\n    resNum = Res.FSOpenResourceFile(fsRef, FSGetResourceForkName(), fsRdPerm)\n    Res.UseResFile(resNum)\n    plistData = Res.Get1Resource(restype, resid).data\n    Res.CloseResFile(resNum)\n    return readPlistFromString(plistData)\n\n\ndef writePlistToResource(rootObject, path, restype='plst', resid=0):\n    \"\"\"Write 'rootObject' as a plst resource to the resource fork of path.\n    \"\"\"\n    warnings.warnpy3k(\"In 3.x, writePlistToResource is removed.\", stacklevel=2)\n    from Carbon.File import FSRef, FSGetResourceForkName\n    from Carbon.Files import fsRdWrPerm\n    from Carbon import Res\n    plistData = writePlistToString(rootObject)\n    fsRef = FSRef(path)\n    resNum = Res.FSOpenResourceFile(fsRef, FSGetResourceForkName(), fsRdWrPerm)\n    Res.UseResFile(resNum)\n    try:\n        Res.Get1Resource(restype, resid).RemoveResource()\n    except Res.Error:\n        pass\n    res = Res.Resource(plistData)\n    res.AddResource(restype, resid, '')\n    res.WriteResource()\n    Res.CloseResFile(resNum)\n\n\nclass DumbXMLWriter:\n\n    def __init__(self, file, indentLevel=0, indent=\"\\t\"):\n        self.file = file\n        self.stack = []\n        self.indentLevel = indentLevel\n        self.indent = indent\n\n    def beginElement(self, element):\n        self.stack.append(element)\n        self.writeln(\"<%s>\" % element)\n        self.indentLevel += 1\n\n    def endElement(self, element):\n        assert self.indentLevel > 0\n        assert self.stack.pop() == element\n        self.indentLevel -= 1\n        self.writeln(\"</%s>\" % element)\n\n    def simpleElement(self, element, value=None):\n        if value is not None:\n            value = _escapeAndEncode(value)\n            self.writeln(\"<%s>%s</%s>\" % (element, value, element))\n        else:\n            self.writeln(\"<%s/>\" % element)\n\n    def writeln(self, line):\n        if line:\n            self.file.write(self.indentLevel * self.indent + line + \"\\n\")\n        else:\n            self.file.write(\"\\n\")\n\n\n# Contents should conform to a subset of ISO 8601\n# (in particular, YYYY '-' MM '-' DD 'T' HH ':' MM ':' SS 'Z'.  Smaller units may be omitted with\n#  a loss of precision)\n_dateParser = re.compile(r\"(?P<year>\\d\\d\\d\\d)(?:-(?P<month>\\d\\d)(?:-(?P<day>\\d\\d)(?:T(?P<hour>\\d\\d)(?::(?P<minute>\\d\\d)(?::(?P<second>\\d\\d))?)?)?)?)?Z\")\n\ndef _dateFromString(s):\n    order = ('year', 'month', 'day', 'hour', 'minute', 'second')\n    gd = _dateParser.match(s).groupdict()\n    lst = []\n    for key in order:\n        val = gd[key]\n        if val is None:\n            break\n        lst.append(int(val))\n    return datetime.datetime(*lst)\n\ndef _dateToString(d):\n    return '%04d-%02d-%02dT%02d:%02d:%02dZ' % (\n        d.year, d.month, d.day,\n        d.hour, d.minute, d.second\n    )\n\n\n# Regex to find any control chars, except for \\t \\n and \\r\n_controlCharPat = re.compile(\n    r\"[\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x0b\\x0c\\x0e\\x0f\"\n    r\"\\x10\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a\\x1b\\x1c\\x1d\\x1e\\x1f]\")\n\ndef _escapeAndEncode(text):\n    m = _controlCharPat.search(text)\n    if m is not None:\n        raise ValueError(\"strings can't contains control characters; \"\n                         \"use plistlib.Data instead\")\n    text = text.replace(\"\\r\\n\", \"\\n\")       # convert DOS line endings\n    text = text.replace(\"\\r\", \"\\n\")         # convert Mac line endings\n    text = text.replace(\"&\", \"&amp;\")       # escape '&'\n    text = text.replace(\"<\", \"&lt;\")        # escape '<'\n    text = text.replace(\">\", \"&gt;\")        # escape '>'\n    return text.encode(\"utf-8\")             # encode as UTF-8\n\n\nPLISTHEADER = \"\"\"\\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n\"\"\"\n\nclass PlistWriter(DumbXMLWriter):\n\n    def __init__(self, file, indentLevel=0, indent=\"\\t\", writeHeader=1):\n        if writeHeader:\n            file.write(PLISTHEADER)\n        DumbXMLWriter.__init__(self, file, indentLevel, indent)\n\n    def writeValue(self, value):\n        if isinstance(value, (str, unicode)):\n            self.simpleElement(\"string\", value)\n        elif isinstance(value, bool):\n            # must switch for bool before int, as bool is a\n            # subclass of int...\n            if value:\n                self.simpleElement(\"true\")\n            else:\n                self.simpleElement(\"false\")\n        elif isinstance(value, (int, long)):\n            self.simpleElement(\"integer\", \"%d\" % value)\n        elif isinstance(value, float):\n            self.simpleElement(\"real\", repr(value))\n        elif isinstance(value, dict):\n            self.writeDict(value)\n        elif isinstance(value, Data):\n            self.writeData(value)\n        elif isinstance(value, datetime.datetime):\n            self.simpleElement(\"date\", _dateToString(value))\n        elif isinstance(value, (tuple, list)):\n            self.writeArray(value)\n        else:\n            raise TypeError(\"unsuported type: %s\" % type(value))\n\n    def writeData(self, data):\n        self.beginElement(\"data\")\n        self.indentLevel -= 1\n        maxlinelength = max(16, 76 - len(self.indent.replace(\"\\t\", \" \" * 8) *\n                                 self.indentLevel))\n        for line in data.asBase64(maxlinelength).split(\"\\n\"):\n            if line:\n                self.writeln(line)\n        self.indentLevel += 1\n        self.endElement(\"data\")\n\n    def writeDict(self, d):\n        self.beginElement(\"dict\")\n        items = d.items()\n        items.sort()\n        for key, value in items:\n            if not isinstance(key, (str, unicode)):\n                raise TypeError(\"keys must be strings\")\n            self.simpleElement(\"key\", key)\n            self.writeValue(value)\n        self.endElement(\"dict\")\n\n    def writeArray(self, array):\n        self.beginElement(\"array\")\n        for value in array:\n            self.writeValue(value)\n        self.endElement(\"array\")\n\n\nclass _InternalDict(dict):\n\n    # This class is needed while Dict is scheduled for deprecation:\n    # we only need to warn when a *user* instantiates Dict or when\n    # the \"attribute notation for dict keys\" is used.\n\n    def __getattr__(self, attr):\n        try:\n            value = self[attr]\n        except KeyError:\n            raise AttributeError, attr\n        from warnings import warn\n        warn(\"Attribute access from plist dicts is deprecated, use d[key] \"\n             \"notation instead\", PendingDeprecationWarning, 2)\n        return value\n\n    def __setattr__(self, attr, value):\n        from warnings import warn\n        warn(\"Attribute access from plist dicts is deprecated, use d[key] \"\n             \"notation instead\", PendingDeprecationWarning, 2)\n        self[attr] = value\n\n    def __delattr__(self, attr):\n        try:\n            del self[attr]\n        except KeyError:\n            raise AttributeError, attr\n        from warnings import warn\n        warn(\"Attribute access from plist dicts is deprecated, use d[key] \"\n             \"notation instead\", PendingDeprecationWarning, 2)\n\nclass Dict(_InternalDict):\n\n    def __init__(self, **kwargs):\n        from warnings import warn\n        warn(\"The plistlib.Dict class is deprecated, use builtin dict instead\",\n             PendingDeprecationWarning, 2)\n        super(Dict, self).__init__(**kwargs)\n\n\nclass Plist(_InternalDict):\n\n    \"\"\"This class has been deprecated. Use readPlist() and writePlist()\n    functions instead, together with regular dict objects.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        from warnings import warn\n        warn(\"The Plist class is deprecated, use the readPlist() and \"\n             \"writePlist() functions instead\", PendingDeprecationWarning, 2)\n        super(Plist, self).__init__(**kwargs)\n\n    def fromFile(cls, pathOrFile):\n        \"\"\"Deprecated. Use the readPlist() function instead.\"\"\"\n        rootObject = readPlist(pathOrFile)\n        plist = cls()\n        plist.update(rootObject)\n        return plist\n    fromFile = classmethod(fromFile)\n\n    def write(self, pathOrFile):\n        \"\"\"Deprecated. Use the writePlist() function instead.\"\"\"\n        writePlist(self, pathOrFile)\n\n\ndef _encodeBase64(s, maxlinelength=76):\n    # copied from base64.encodestring(), with added maxlinelength argument\n    maxbinsize = (maxlinelength//4)*3\n    pieces = []\n    for i in range(0, len(s), maxbinsize):\n        chunk = s[i : i + maxbinsize]\n        pieces.append(binascii.b2a_base64(chunk))\n    return \"\".join(pieces)\n\nclass Data:\n\n    \"\"\"Wrapper for binary data.\"\"\"\n\n    def __init__(self, data):\n        self.data = data\n\n    def fromBase64(cls, data):\n        # base64.decodestring just calls binascii.a2b_base64;\n        # it seems overkill to use both base64 and binascii.\n        return cls(binascii.a2b_base64(data))\n    fromBase64 = classmethod(fromBase64)\n\n    def asBase64(self, maxlinelength=76):\n        return _encodeBase64(self.data, maxlinelength)\n\n    def __cmp__(self, other):\n        if isinstance(other, self.__class__):\n            return cmp(self.data, other.data)\n        elif isinstance(other, str):\n            return cmp(self.data, other)\n        else:\n            return cmp(id(self), id(other))\n\n    def __repr__(self):\n        return \"%s(%s)\" % (self.__class__.__name__, repr(self.data))\n\n\nclass PlistParser:\n\n    def __init__(self):\n        self.stack = []\n        self.currentKey = None\n        self.root = None\n\n    def parse(self, fileobj):\n        from xml.parsers.expat import ParserCreate\n        parser = ParserCreate()\n        parser.StartElementHandler = self.handleBeginElement\n        parser.EndElementHandler = self.handleEndElement\n        parser.CharacterDataHandler = self.handleData\n        parser.ParseFile(fileobj)\n        return self.root\n\n    def handleBeginElement(self, element, attrs):\n        self.data = []\n        handler = getattr(self, \"begin_\" + element, None)\n        if handler is not None:\n            handler(attrs)\n\n    def handleEndElement(self, element):\n        handler = getattr(self, \"end_\" + element, None)\n        if handler is not None:\n            handler()\n\n    def handleData(self, data):\n        self.data.append(data)\n\n    def addObject(self, value):\n        if self.currentKey is not None:\n            self.stack[-1][self.currentKey] = value\n            self.currentKey = None\n        elif not self.stack:\n            # this is the root object\n            self.root = value\n        else:\n            self.stack[-1].append(value)\n\n    def getData(self):\n        data = \"\".join(self.data)\n        try:\n            data = data.encode(\"ascii\")\n        except UnicodeError:\n            pass\n        self.data = []\n        return data\n\n    # element handlers\n\n    def begin_dict(self, attrs):\n        d = _InternalDict()\n        self.addObject(d)\n        self.stack.append(d)\n    def end_dict(self):\n        self.stack.pop()\n\n    def end_key(self):\n        self.currentKey = self.getData()\n\n    def begin_array(self, attrs):\n        a = []\n        self.addObject(a)\n        self.stack.append(a)\n    def end_array(self):\n        self.stack.pop()\n\n    def end_true(self):\n        self.addObject(True)\n    def end_false(self):\n        self.addObject(False)\n    def end_integer(self):\n        self.addObject(int(self.getData()))\n    def end_real(self):\n        self.addObject(float(self.getData()))\n    def end_string(self):\n        self.addObject(self.getData())\n    def end_data(self):\n        self.addObject(Data.fromBase64(self.getData()))\n    def end_date(self):\n        self.addObject(_dateFromString(self.getData()))\n",
		"file_name": "plistlib.py"
	},
	{
		"content": "\"\"\"Interface to the Expat non-validating XML parser.\"\"\"\n__version__ = '$Revision: 17640 $'\n\nfrom pyexpat import *\n",
		"file_name": "xml/parsers/expat.py"
	},
	{
		"content": "\"\"\"Python interfaces to XML parsers.\n\nThis package contains one module:\n\nexpat -- Python wrapper for James Clark's Expat parser, with namespace\n         support.\n\n\"\"\"\n",
		"file_name": "xml/parsers/__init__.py"
	},
	{
		"content": "\"\"\"Core XML support for Python.\n\nThis package contains four sub-packages:\n\ndom -- The W3C Document Object Model.  This supports DOM Level 1 +\n       Namespaces.\n\nparsers -- Python wrappers for XML parsers (currently only supports Expat).\n\nsax -- The Simple API for XML, developed by XML-Dev, led by David\n       Megginson and ported to Python by Lars Marius Garshol.  This\n       supports the SAX 2 API.\n\netree -- The ElementTree XML library.  This is a subset of the full\n       ElementTree XML release.\n\n\"\"\"\n\n\n__all__ = [\"dom\", \"parsers\", \"sax\", \"etree\"]\n\n_MINIMUM_XMLPLUS_VERSION = (0, 8, 4)\n\n\ntry:\n    import _xmlplus\nexcept ImportError:\n    pass\nelse:\n    try:\n        v = _xmlplus.version_info\n    except AttributeError:\n        # _xmlplus is too old; ignore it\n        pass\n    else:\n        if v >= _MINIMUM_XMLPLUS_VERSION:\n            import sys\n            _xmlplus.__path__.extend(__path__)\n            sys.modules[__name__] = _xmlplus\n        else:\n            del v\n",
		"file_name": "xml/__init__.py"
	},
	{
		"content": "\"\"\"Concrete date/time and related types -- prototype implemented in Python.\n\nSee http://www.zope.org/Members/fdrake/DateTimeWiki/FrontPage\n\nSee also http://dir.yahoo.com/Reference/calendars/\n\nFor a primer on DST, including many current DST rules, see\nhttp://webexhibits.org/daylightsaving/\n\nFor more about DST than you ever wanted to know, see\nftp://elsie.nci.nih.gov/pub/\n\nSources for time zone and DST data: http://www.twinsun.com/tz/tz-link.htm\n\nThis was originally copied from the sandbox of the CPython CVS repository.\nThanks to Tim Peters for suggesting using it.\n\"\"\"\n\nfrom __future__ import division\nimport time as _time\nimport math as _math\nimport struct as _struct\n\ndef _cmp(x, y):\n    return 0 if x == y else 1 if x > y else -1\n\ndef _round(x):\n    return int(_math.floor(x + 0.5) if x >= 0.0 else _math.ceil(x - 0.5))\n\nMINYEAR = 1\nMAXYEAR = 9999\n_MINYEARFMT = 1900\n\n# Utility functions, adapted from Python's Demo/classes/Dates.py, which\n# also assumes the current Gregorian calendar indefinitely extended in\n# both directions.  Difference:  Dates.py calls January 1 of year 0 day\n# number 1.  The code here calls January 1 of year 1 day number 1.  This is\n# to match the definition of the \"proleptic Gregorian\" calendar in Dershowitz\n# and Reingold's \"Calendrical Calculations\", where it's the base calendar\n# for all computations.  See the book for algorithms for converting between\n# proleptic Gregorian ordinals and many other calendar systems.\n\n_DAYS_IN_MONTH = [-1, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n\n_DAYS_BEFORE_MONTH = [-1]\ndbm = 0\nfor dim in _DAYS_IN_MONTH[1:]:\n    _DAYS_BEFORE_MONTH.append(dbm)\n    dbm += dim\ndel dbm, dim\n\ndef _is_leap(year):\n    \"year -> 1 if leap year, else 0.\"\n    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n\ndef _days_before_year(year):\n    \"year -> number of days before January 1st of year.\"\n    y = year - 1\n    return y*365 + y//4 - y//100 + y//400\n\ndef _days_in_month(year, month):\n    \"year, month -> number of days in that month in that year.\"\n    assert 1 <= month <= 12, month\n    if month == 2 and _is_leap(year):\n        return 29\n    return _DAYS_IN_MONTH[month]\n\ndef _days_before_month(year, month):\n    \"year, month -> number of days in year preceding first day of month.\"\n    assert 1 <= month <= 12, 'month must be in 1..12'\n    return _DAYS_BEFORE_MONTH[month] + (month > 2 and _is_leap(year))\n\ndef _ymd2ord(year, month, day):\n    \"year, month, day -> ordinal, considering 01-Jan-0001 as day 1.\"\n    assert 1 <= month <= 12, 'month must be in 1..12'\n    dim = _days_in_month(year, month)\n    assert 1 <= day <= dim, ('day must be in 1..%d' % dim)\n    return (_days_before_year(year) +\n            _days_before_month(year, month) +\n            day)\n\n_DI400Y = _days_before_year(401)    # number of days in 400 years\n_DI100Y = _days_before_year(101)    #    \"    \"   \"   \" 100   \"\n_DI4Y   = _days_before_year(5)      #    \"    \"   \"   \"   4   \"\n\n# A 4-year cycle has an extra leap day over what we'd get from pasting\n# together 4 single years.\nassert _DI4Y == 4 * 365 + 1\n\n# Similarly, a 400-year cycle has an extra leap day over what we'd get from\n# pasting together 4 100-year cycles.\nassert _DI400Y == 4 * _DI100Y + 1\n\n# OTOH, a 100-year cycle has one fewer leap day than we'd get from\n# pasting together 25 4-year cycles.\nassert _DI100Y == 25 * _DI4Y - 1\n\ndef _ord2ymd(n):\n    \"ordinal -> (year, month, day), considering 01-Jan-0001 as day 1.\"\n\n    # n is a 1-based index, starting at 1-Jan-1.  The pattern of leap years\n    # repeats exactly every 400 years.  The basic strategy is to find the\n    # closest 400-year boundary at or before n, then work with the offset\n    # from that boundary to n.  Life is much clearer if we subtract 1 from\n    # n first -- then the values of n at 400-year boundaries are exactly\n    # those divisible by _DI400Y:\n    #\n    #     D  M   Y            n              n-1\n    #     -- --- ----        ----------     ----------------\n    #     31 Dec -400        -_DI400Y       -_DI400Y -1\n    #      1 Jan -399         -_DI400Y +1   -_DI400Y      400-year boundary\n    #     ...\n    #     30 Dec  000        -1             -2\n    #     31 Dec  000         0             -1\n    #      1 Jan  001         1              0            400-year boundary\n    #      2 Jan  001         2              1\n    #      3 Jan  001         3              2\n    #     ...\n    #     31 Dec  400         _DI400Y        _DI400Y -1\n    #      1 Jan  401         _DI400Y +1     _DI400Y      400-year boundary\n    n -= 1\n    n400, n = divmod(n, _DI400Y)\n    year = n400 * 400 + 1   # ..., -399, 1, 401, ...\n\n    # Now n is the (non-negative) offset, in days, from January 1 of year, to\n    # the desired date.  Now compute how many 100-year cycles precede n.\n    # Note that it's possible for n100 to equal 4!  In that case 4 full\n    # 100-year cycles precede the desired day, which implies the desired\n    # day is December 31 at the end of a 400-year cycle.\n    n100, n = divmod(n, _DI100Y)\n\n    # Now compute how many 4-year cycles precede it.\n    n4, n = divmod(n, _DI4Y)\n\n    # And now how many single years.  Again n1 can be 4, and again meaning\n    # that the desired day is December 31 at the end of the 4-year cycle.\n    n1, n = divmod(n, 365)\n\n    year += n100 * 100 + n4 * 4 + n1\n    if n1 == 4 or n100 == 4:\n        assert n == 0\n        return year-1, 12, 31\n\n    # Now the year is correct, and n is the offset from January 1.  We find\n    # the month via an estimate that's either exact or one too large.\n    leapyear = n1 == 3 and (n4 != 24 or n100 == 3)\n    assert leapyear == _is_leap(year)\n    month = (n + 50) >> 5\n    preceding = _DAYS_BEFORE_MONTH[month] + (month > 2 and leapyear)\n    if preceding > n:  # estimate is too large\n        month -= 1\n        preceding -= _DAYS_IN_MONTH[month] + (month == 2 and leapyear)\n    n -= preceding\n    assert 0 <= n < _days_in_month(year, month)\n\n    # Now the year and month are correct, and n is the offset from the\n    # start of that month:  we're done!\n    return year, month, n+1\n\n# Month and day names.  For localized versions, see the calendar module.\n_MONTHNAMES = [None, \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n                     \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n_DAYNAMES = [None, \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n\n\ndef _build_struct_time(y, m, d, hh, mm, ss, dstflag):\n    wday = (_ymd2ord(y, m, d) + 6) % 7\n    dnum = _days_before_month(y, m) + d\n    return _time.struct_time((y, m, d, hh, mm, ss, wday, dnum, dstflag))\n\ndef _format_time(hh, mm, ss, us):\n    # Skip trailing microseconds when us==0.\n    result = \"%02d:%02d:%02d\" % (hh, mm, ss)\n    if us:\n        result += \".%06d\" % us\n    return result\n\n# Correctly substitute for %z and %Z escapes in strftime formats.\ndef _wrap_strftime(object, format, timetuple):\n    year = timetuple[0]\n    if year < _MINYEARFMT:\n        raise ValueError(\"year=%d is before %d; the datetime strftime() \"\n                         \"methods require year >= %d\" %\n                         (year, _MINYEARFMT, _MINYEARFMT))\n    # Don't call utcoffset() or tzname() unless actually needed.\n    freplace = None  # the string to use for %f\n    zreplace = None  # the string to use for %z\n    Zreplace = None  # the string to use for %Z\n\n    # Scan format for %z and %Z escapes, replacing as needed.\n    newformat = []\n    push = newformat.append\n    i, n = 0, len(format)\n    while i < n:\n        ch = format[i]\n        i += 1\n        if ch == '%':\n            if i < n:\n                ch = format[i]\n                i += 1\n                if ch == 'f':\n                    if freplace is None:\n                        freplace = '%06d' % getattr(object,\n                                                    'microsecond', 0)\n                    newformat.append(freplace)\n                elif ch == 'z':\n                    if zreplace is None:\n                        zreplace = \"\"\n                        if hasattr(object, \"_utcoffset\"):\n                            offset = object._utcoffset()\n                            if offset is not None:\n                                sign = '+'\n                                if offset < 0:\n                                    offset = -offset\n                                    sign = '-'\n                                h, m = divmod(offset, 60)\n                                zreplace = '%c%02d%02d' % (sign, h, m)\n                    assert '%' not in zreplace\n                    newformat.append(zreplace)\n                elif ch == 'Z':\n                    if Zreplace is None:\n                        Zreplace = \"\"\n                        if hasattr(object, \"tzname\"):\n                            s = object.tzname()\n                            if s is not None:\n                                # strftime is going to have at this: escape %\n                                Zreplace = s.replace('%', '%%')\n                    newformat.append(Zreplace)\n                else:\n                    push('%')\n                    push(ch)\n            else:\n                push('%')\n        else:\n            push(ch)\n    newformat = \"\".join(newformat)\n    return _time.strftime(newformat, timetuple)\n\n# Just raise TypeError if the arg isn't None or a string.\ndef _check_tzname(name):\n    if name is not None and not isinstance(name, str):\n        raise TypeError(\"tzinfo.tzname() must return None or string, \"\n                        \"not '%s'\" % type(name))\n\n# name is the offset-producing method, \"utcoffset\" or \"dst\".\n# offset is what it returned.\n# If offset isn't None or timedelta, raises TypeError.\n# If offset is None, returns None.\n# Else offset is checked for being in range, and a whole # of minutes.\n# If it is, its integer value is returned.  Else ValueError is raised.\ndef _check_utc_offset(name, offset):\n    assert name in (\"utcoffset\", \"dst\")\n    if offset is None:\n        return\n    if not isinstance(offset, timedelta):\n        raise TypeError(\"tzinfo.%s() must return None \"\n                        \"or timedelta, not '%s'\" % (name, type(offset)))\n    days = offset.days\n    if days < -1 or days > 0:\n        offset = 1440  # trigger out-of-range\n    else:\n        seconds = days * 86400 + offset.seconds\n        minutes, seconds = divmod(seconds, 60)\n        if seconds or offset.microseconds:\n            raise ValueError(\"tzinfo.%s() must return a whole number \"\n                             \"of minutes\" % name)\n        offset = minutes\n    if not -1440 < offset < 1440:\n        raise ValueError(\"%s()=%d, must be in -1439..1439\" % (name, offset))\n    return offset\n\ndef _check_int_field(value):\n    if isinstance(value, int):\n        return value\n    if not isinstance(value, float):\n        try:\n            value = value.__int__()\n        except AttributeError:\n            pass\n        else:\n            if isinstance(value, (int, long)):\n                return value\n            raise TypeError('__int__ method should return an integer')\n        raise TypeError('an integer is required')\n    raise TypeError('integer argument expected, got float')\n\ndef _check_date_fields(year, month, day):\n    year = _check_int_field(year)\n    month = _check_int_field(month)\n    day = _check_int_field(day)\n    if not MINYEAR <= year <= MAXYEAR:\n        raise ValueError('year must be in %d..%d' % (MINYEAR, MAXYEAR), year)\n    if not 1 <= month <= 12:\n        raise ValueError('month must be in 1..12', month)\n    dim = _days_in_month(year, month)\n    if not 1 <= day <= dim:\n        raise ValueError('day must be in 1..%d' % dim, day)\n    return year, month, day\n\ndef _check_time_fields(hour, minute, second, microsecond):\n    hour = _check_int_field(hour)\n    minute = _check_int_field(minute)\n    second = _check_int_field(second)\n    microsecond = _check_int_field(microsecond)\n    if not 0 <= hour <= 23:\n        raise ValueError('hour must be in 0..23', hour)\n    if not 0 <= minute <= 59:\n        raise ValueError('minute must be in 0..59', minute)\n    if not 0 <= second <= 59:\n        raise ValueError('second must be in 0..59', second)\n    if not 0 <= microsecond <= 999999:\n        raise ValueError('microsecond must be in 0..999999', microsecond)\n    return hour, minute, second, microsecond\n\ndef _check_tzinfo_arg(tz):\n    if tz is not None and not isinstance(tz, tzinfo):\n        raise TypeError(\"tzinfo argument must be None or of a tzinfo subclass\")\n\n\n# Notes on comparison:  In general, datetime module comparison operators raise\n# TypeError when they don't know how to do a comparison themself.  If they\n# returned NotImplemented instead, comparison could (silently) fall back to\n# the default compare-objects-by-comparing-their-memory-addresses strategy,\n# and that's not helpful.  There are two exceptions:\n#\n# 1. For date and datetime, if the other object has a \"timetuple\" attr,\n#    NotImplemented is returned.  This is a hook to allow other kinds of\n#    datetime-like objects a chance to intercept the comparison.\n#\n# 2. Else __eq__ and __ne__ return False and True, respectively.  This is\n#    so opertaions like\n#\n#        x == y\n#        x != y\n#        x in sequence\n#        x not in sequence\n#        dict[x] = y\n#\n#    don't raise annoying TypeErrors just because a datetime object\n#    is part of a heterogeneous collection.  If there's no known way to\n#    compare X to a datetime, saying they're not equal is reasonable.\n\ndef _cmperror(x, y):\n    raise TypeError(\"can't compare '%s' to '%s'\" % (\n                    type(x).__name__, type(y).__name__))\n\n# This is a start at a struct tm workalike.  Goals:\n#\n# + Works the same way across platforms.\n# + Handles all the fields datetime needs handled, without 1970-2038 glitches.\n#\n# Note:  I suspect it's best if this flavor of tm does *not* try to\n# second-guess timezones or DST.  Instead fold whatever adjustments you want\n# into the minutes argument (and the constructor will normalize).\n\nclass _tmxxx:\n\n    ordinal = None\n\n    def __init__(self, year, month, day, hour=0, minute=0, second=0,\n                 microsecond=0):\n        # Normalize all the inputs, and store the normalized values.\n        if not 0 <= microsecond <= 999999:\n            carry, microsecond = divmod(microsecond, 1000000)\n            second += carry\n        if not 0 <= second <= 59:\n            carry, second = divmod(second, 60)\n            minute += carry\n        if not 0 <= minute <= 59:\n            carry, minute = divmod(minute, 60)\n            hour += carry\n        if not 0 <= hour <= 23:\n            carry, hour = divmod(hour, 24)\n            day += carry\n\n        # That was easy.  Now it gets muddy:  the proper range for day\n        # can't be determined without knowing the correct month and year,\n        # but if day is, e.g., plus or minus a million, the current month\n        # and year values make no sense (and may also be out of bounds\n        # themselves).\n        # Saying 12 months == 1 year should be non-controversial.\n        if not 1 <= month <= 12:\n            carry, month = divmod(month-1, 12)\n            year += carry\n            month += 1\n            assert 1 <= month <= 12\n\n        # Now only day can be out of bounds (year may also be out of bounds\n        # for a datetime object, but we don't care about that here).\n        # If day is out of bounds, what to do is arguable, but at least the\n        # method here is principled and explainable.\n        dim = _days_in_month(year, month)\n        if not 1 <= day <= dim:\n            # Move day-1 days from the first of the month.  First try to\n            # get off cheap if we're only one day out of range (adjustments\n            # for timezone alone can't be worse than that).\n            if day == 0:    # move back a day\n                month -= 1\n                if month > 0:\n                    day = _days_in_month(year, month)\n                else:\n                    year, month, day = year-1, 12, 31\n            elif day == dim + 1:    # move forward a day\n                month += 1\n                day = 1\n                if month > 12:\n                    month = 1\n                    year += 1\n            else:\n                self.ordinal = _ymd2ord(year, month, 1) + (day - 1)\n                year, month, day = _ord2ymd(self.ordinal)\n\n        self.year, self.month, self.day = year, month, day\n        self.hour, self.minute, self.second = hour, minute, second\n        self.microsecond = microsecond\n\nclass timedelta(object):\n    \"\"\"Represent the difference between two datetime objects.\n\n    Supported operators:\n\n    - add, subtract timedelta\n    - unary plus, minus, abs\n    - compare to timedelta\n    - multiply, divide by int/long\n\n    In addition, datetime supports subtraction of two datetime objects\n    returning a timedelta, and addition or subtraction of a datetime\n    and a timedelta giving a datetime.\n\n    Representation: (days, seconds, microseconds).  Why?  Because I\n    felt like it.\n    \"\"\"\n    __slots__ = '_days', '_seconds', '_microseconds', '_hashcode'\n\n    def __new__(cls, days=0, seconds=0, microseconds=0,\n                milliseconds=0, minutes=0, hours=0, weeks=0):\n        # Doing this efficiently and accurately in C is going to be difficult\n        # and error-prone, due to ubiquitous overflow possibilities, and that\n        # C double doesn't have enough bits of precision to represent\n        # microseconds over 10K years faithfully.  The code here tries to make\n        # explicit where go-fast assumptions can be relied on, in order to\n        # guide the C implementation; it's way more convoluted than speed-\n        # ignoring auto-overflow-to-long idiomatic Python could be.\n\n        # XXX Check that all inputs are ints, longs or floats.\n\n        # Final values, all integer.\n        # s and us fit in 32-bit signed ints; d isn't bounded.\n        d = s = us = 0\n\n        # Normalize everything to days, seconds, microseconds.\n        days += weeks*7\n        seconds += minutes*60 + hours*3600\n        microseconds += milliseconds*1000\n\n        # Get rid of all fractions, and normalize s and us.\n        # Take a deep breath <wink>.\n        if isinstance(days, float):\n            dayfrac, days = _math.modf(days)\n            daysecondsfrac, daysecondswhole = _math.modf(dayfrac * (24.*3600.))\n            assert daysecondswhole == int(daysecondswhole)  # can't overflow\n            s = int(daysecondswhole)\n            assert days == int(days)\n            d = int(days)\n        else:\n            daysecondsfrac = 0.0\n            d = days\n        assert isinstance(daysecondsfrac, float)\n        assert abs(daysecondsfrac) <= 1.0\n        assert isinstance(d, (int, long))\n        assert abs(s) <= 24 * 3600\n        # days isn't referenced again before redefinition\n\n        if isinstance(seconds, float):\n            secondsfrac, seconds = _math.modf(seconds)\n            assert seconds == int(seconds)\n            seconds = int(seconds)\n            secondsfrac += daysecondsfrac\n            assert abs(secondsfrac) <= 2.0\n        else:\n            secondsfrac = daysecondsfrac\n        # daysecondsfrac isn't referenced again\n        assert isinstance(secondsfrac, float)\n        assert abs(secondsfrac) <= 2.0\n\n        assert isinstance(seconds, (int, long))\n        days, seconds = divmod(seconds, 24*3600)\n        d += days\n        s += int(seconds)    # can't overflow\n        assert isinstance(s, int)\n        assert abs(s) <= 2 * 24 * 3600\n        # seconds isn't referenced again before redefinition\n\n        usdouble = secondsfrac * 1e6\n        assert abs(usdouble) < 2.1e6    # exact value not critical\n        # secondsfrac isn't referenced again\n\n        if isinstance(microseconds, float):\n            microseconds = _round(microseconds + usdouble)\n            seconds, microseconds = divmod(microseconds, 1000000)\n            days, seconds = divmod(seconds, 24*3600)\n            d += days\n            s += int(seconds)\n            microseconds = int(microseconds)\n        else:\n            microseconds = int(microseconds)\n            seconds, microseconds = divmod(microseconds, 1000000)\n            days, seconds = divmod(seconds, 24*3600)\n            d += days\n            s += int(seconds)\n            microseconds = _round(microseconds + usdouble)\n        assert isinstance(s, int)\n        assert isinstance(microseconds, int)\n        assert abs(s) <= 3 * 24 * 3600\n        assert abs(microseconds) < 3.1e6\n\n        # Just a little bit of carrying possible for microseconds and seconds.\n        seconds, us = divmod(microseconds, 1000000)\n        s += seconds\n        days, s = divmod(s, 24*3600)\n        d += days\n\n        assert isinstance(d, (int, long))\n        assert isinstance(s, int) and 0 <= s < 24*3600\n        assert isinstance(us, int) and 0 <= us < 1000000\n\n        if abs(d) > 999999999:\n            raise OverflowError(\"timedelta # of days is too large: %d\" % d)\n\n        self = object.__new__(cls)\n        self._days = d\n        self._seconds = s\n        self._microseconds = us\n        self._hashcode = -1\n        return self\n\n    def __repr__(self):\n        if self._microseconds:\n            return \"%s(%d, %d, %d)\" % ('datetime.' + self.__class__.__name__,\n                                       self._days,\n                                       self._seconds,\n                                       self._microseconds)\n        if self._seconds:\n            return \"%s(%d, %d)\" % ('datetime.' + self.__class__.__name__,\n                                   self._days,\n                                   self._seconds)\n        return \"%s(%d)\" % ('datetime.' + self.__class__.__name__, self._days)\n\n    def __str__(self):\n        mm, ss = divmod(self._seconds, 60)\n        hh, mm = divmod(mm, 60)\n        s = \"%d:%02d:%02d\" % (hh, mm, ss)\n        if self._days:\n            def plural(n):\n                return n, abs(n) != 1 and \"s\" or \"\"\n            s = (\"%d day%s, \" % plural(self._days)) + s\n        if self._microseconds:\n            s = s + \".%06d\" % self._microseconds\n        return s\n\n    def total_seconds(self):\n        \"\"\"Total seconds in the duration.\"\"\"\n        return ((self.days * 86400 + self.seconds) * 10**6 +\n                self.microseconds) / 10**6\n\n    # Read-only field accessors\n    @property\n    def days(self):\n        \"\"\"days\"\"\"\n        return self._days\n\n    @property\n    def seconds(self):\n        \"\"\"seconds\"\"\"\n        return self._seconds\n\n    @property\n    def microseconds(self):\n        \"\"\"microseconds\"\"\"\n        return self._microseconds\n\n    def __add__(self, other):\n        if isinstance(other, timedelta):\n            # for CPython compatibility, we cannot use\n            # our __class__ here, but need a real timedelta\n            return timedelta(self._days + other._days,\n                             self._seconds + other._seconds,\n                             self._microseconds + other._microseconds)\n        return NotImplemented\n\n    __radd__ = __add__\n\n    def __sub__(self, other):\n        if isinstance(other, timedelta):\n            # for CPython compatibility, we cannot use\n            # our __class__ here, but need a real timedelta\n            return timedelta(self._days - other._days,\n                             self._seconds - other._seconds,\n                             self._microseconds - other._microseconds)\n        return NotImplemented\n\n    def __rsub__(self, other):\n        if isinstance(other, timedelta):\n            return -self + other\n        return NotImplemented\n\n    def __neg__(self):\n        # for CPython compatibility, we cannot use\n        # our __class__ here, but need a real timedelta\n        return timedelta(-self._days,\n                         -self._seconds,\n                         -self._microseconds)\n\n    def __pos__(self):\n        return self\n\n    def __abs__(self):\n        if self._days < 0:\n            return -self\n        else:\n            return self\n\n    def __mul__(self, other):\n        if isinstance(other, (int, long)):\n            # for CPython compatibility, we cannot use\n            # our __class__ here, but need a real timedelta\n            return timedelta(self._days * other,\n                             self._seconds * other,\n                             self._microseconds * other)\n        return NotImplemented\n\n    __rmul__ = __mul__\n\n    def _to_microseconds(self):\n        return ((self._days * (24*3600) + self._seconds) * 1000000 +\n                self._microseconds)\n\n    def __div__(self, other):\n        if not isinstance(other, (int, long)):\n            return NotImplemented\n        usec = self._to_microseconds()\n        return timedelta(0, 0, usec // other)\n\n    __floordiv__ = __div__\n\n    # Comparisons of timedelta objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) == 0\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) != 0\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) <= 0\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) < 0\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) >= 0\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) > 0\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, timedelta)\n        return _cmp(self._getstate(), other._getstate())\n\n    def __hash__(self):\n        if self._hashcode == -1:\n            self._hashcode = hash(self._getstate())\n        return self._hashcode\n\n    def __nonzero__(self):\n        return (self._days != 0 or\n                self._seconds != 0 or\n                self._microseconds != 0)\n\n    # Pickle support.\n\n    def _getstate(self):\n        return (self._days, self._seconds, self._microseconds)\n\n    def __reduce__(self):\n        return (self.__class__, self._getstate())\n\ntimedelta.min = timedelta(-999999999)\ntimedelta.max = timedelta(days=999999999, hours=23, minutes=59, seconds=59,\n                          microseconds=999999)\ntimedelta.resolution = timedelta(microseconds=1)\n\nclass date(object):\n    \"\"\"Concrete date type.\n\n    Constructors:\n\n    __new__()\n    fromtimestamp()\n    today()\n    fromordinal()\n\n    Operators:\n\n    __repr__, __str__\n    __cmp__, __hash__\n    __add__, __radd__, __sub__ (add/radd only with timedelta arg)\n\n    Methods:\n\n    timetuple()\n    toordinal()\n    weekday()\n    isoweekday(), isocalendar(), isoformat()\n    ctime()\n    strftime()\n\n    Properties (readonly):\n    year, month, day\n    \"\"\"\n    __slots__ = '_year', '_month', '_day', '_hashcode'\n\n    def __new__(cls, year, month=None, day=None):\n        \"\"\"Constructor.\n\n        Arguments:\n\n        year, month, day (required, base 1)\n        \"\"\"\n        if month is None and isinstance(year, bytes) and len(year) == 4 and \\\n                1 <= ord(year[2]) <= 12:\n            # Pickle support\n            self = object.__new__(cls)\n            self.__setstate(year)\n            self._hashcode = -1\n            return self\n        year, month, day = _check_date_fields(year, month, day)\n        self = object.__new__(cls)\n        self._year = year\n        self._month = month\n        self._day = day\n        self._hashcode = -1\n        return self\n\n    # Additional constructors\n\n    @classmethod\n    def fromtimestamp(cls, t):\n        \"Construct a date from a POSIX timestamp (like time.time()).\"\n        y, m, d, hh, mm, ss, weekday, jday, dst = _time.localtime(t)\n        return cls(y, m, d)\n\n    @classmethod\n    def today(cls):\n        \"Construct a date from time.time().\"\n        t = _time.time()\n        return cls.fromtimestamp(t)\n\n    @classmethod\n    def fromordinal(cls, n):\n        \"\"\"Contruct a date from a proleptic Gregorian ordinal.\n\n        January 1 of year 1 is day 1.  Only the year, month and day are\n        non-zero in the result.\n        \"\"\"\n        y, m, d = _ord2ymd(n)\n        return cls(y, m, d)\n\n    # Conversions to string\n\n    def __repr__(self):\n        \"\"\"Convert to formal string, for repr().\n\n        >>> dt = datetime(2010, 1, 1)\n        >>> repr(dt)\n        'datetime.datetime(2010, 1, 1, 0, 0)'\n\n        >>> dt = datetime(2010, 1, 1, tzinfo=timezone.utc)\n        >>> repr(dt)\n        'datetime.datetime(2010, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)'\n        \"\"\"\n        return \"%s(%d, %d, %d)\" % ('datetime.' + self.__class__.__name__,\n                                   self._year,\n                                   self._month,\n                                   self._day)\n\n    # XXX These shouldn't depend on time.localtime(), because that\n    # clips the usable dates to [1970 .. 2038).  At least ctime() is\n    # easily done without using strftime() -- that's better too because\n    # strftime(\"%c\", ...) is locale specific.\n\n    def ctime(self):\n        \"Return ctime() style string.\"\n        weekday = self.toordinal() % 7 or 7\n        return \"%s %s %2d 00:00:00 %04d\" % (\n            _DAYNAMES[weekday],\n            _MONTHNAMES[self._month],\n            self._day, self._year)\n\n    def strftime(self, fmt):\n        \"Format using strftime().\"\n        return _wrap_strftime(self, fmt, self.timetuple())\n\n    def __format__(self, fmt):\n        if not isinstance(fmt, (str, unicode)):\n            raise ValueError(\"__format__ expects str or unicode, not %s\" %\n                             fmt.__class__.__name__)\n        if len(fmt) != 0:\n            return self.strftime(fmt)\n        return str(self)\n\n    def isoformat(self):\n        \"\"\"Return the date formatted according to ISO.\n\n        This is 'YYYY-MM-DD'.\n\n        References:\n        - http://www.w3.org/TR/NOTE-datetime\n        - http://www.cl.cam.ac.uk/~mgk25/iso-time.html\n        \"\"\"\n        return \"%04d-%02d-%02d\" % (self._year, self._month, self._day)\n\n    __str__ = isoformat\n\n    # Read-only field accessors\n    @property\n    def year(self):\n        \"\"\"year (1-9999)\"\"\"\n        return self._year\n\n    @property\n    def month(self):\n        \"\"\"month (1-12)\"\"\"\n        return self._month\n\n    @property\n    def day(self):\n        \"\"\"day (1-31)\"\"\"\n        return self._day\n\n    # Standard conversions, __cmp__, __hash__ (and helpers)\n\n    def timetuple(self):\n        \"Return local time tuple compatible with time.localtime().\"\n        return _build_struct_time(self._year, self._month, self._day,\n                                  0, 0, 0, -1)\n\n    def toordinal(self):\n        \"\"\"Return proleptic Gregorian ordinal for the year, month and day.\n\n        January 1 of year 1 is day 1.  Only the year, month and day values\n        contribute to the result.\n        \"\"\"\n        return _ymd2ord(self._year, self._month, self._day)\n\n    def replace(self, year=None, month=None, day=None):\n        \"\"\"Return a new date with new values for the specified fields.\"\"\"\n        if year is None:\n            year = self._year\n        if month is None:\n            month = self._month\n        if day is None:\n            day = self._day\n        return date(year, month, day)\n\n    # Comparisons of date objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) == 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) != 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) <= 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) < 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) >= 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) > 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, date)\n        y, m, d = self._year, self._month, self._day\n        y2, m2, d2 = other._year, other._month, other._day\n        return _cmp((y, m, d), (y2, m2, d2))\n\n    def __hash__(self):\n        \"Hash.\"\n        if self._hashcode == -1:\n            self._hashcode = hash(self._getstate())\n        return self._hashcode\n\n    # Computations\n\n    def _checkOverflow(self, year):\n        if not MINYEAR <= year <= MAXYEAR:\n            raise OverflowError(\"date +/-: result year %d not in %d..%d\" %\n                                (year, MINYEAR, MAXYEAR))\n\n    def __add__(self, other):\n        \"Add a date to a timedelta.\"\n        if isinstance(other, timedelta):\n            t = _tmxxx(self._year,\n                      self._month,\n                      self._day + other.days)\n            self._checkOverflow(t.year)\n            result = date(t.year, t.month, t.day)\n            return result\n        return NotImplemented\n\n    __radd__ = __add__\n\n    def __sub__(self, other):\n        \"\"\"Subtract two dates, or a date and a timedelta.\"\"\"\n        if isinstance(other, timedelta):\n            return self + timedelta(-other.days)\n        if isinstance(other, date):\n            days1 = self.toordinal()\n            days2 = other.toordinal()\n            return timedelta(days1 - days2)\n        return NotImplemented\n\n    def weekday(self):\n        \"Return day of the week, where Monday == 0 ... Sunday == 6.\"\n        return (self.toordinal() + 6) % 7\n\n    # Day-of-the-week and week-of-the-year, according to ISO\n\n    def isoweekday(self):\n        \"Return day of the week, where Monday == 1 ... Sunday == 7.\"\n        # 1-Jan-0001 is a Monday\n        return self.toordinal() % 7 or 7\n\n    def isocalendar(self):\n        \"\"\"Return a 3-tuple containing ISO year, week number, and weekday.\n\n        The first ISO week of the year is the (Mon-Sun) week\n        containing the year's first Thursday; everything else derives\n        from that.\n\n        The first week is 1; Monday is 1 ... Sunday is 7.\n\n        ISO calendar algorithm taken from\n        http://www.phys.uu.nl/~vgent/calendar/isocalendar.htm\n        \"\"\"\n        year = self._year\n        week1monday = _isoweek1monday(year)\n        today = _ymd2ord(self._year, self._month, self._day)\n        # Internally, week and day have origin 0\n        week, day = divmod(today - week1monday, 7)\n        if week < 0:\n            year -= 1\n            week1monday = _isoweek1monday(year)\n            week, day = divmod(today - week1monday, 7)\n        elif week >= 52:\n            if today >= _isoweek1monday(year+1):\n                year += 1\n                week = 0\n        return year, week+1, day+1\n\n    # Pickle support.\n\n    def _getstate(self):\n        yhi, ylo = divmod(self._year, 256)\n        return (_struct.pack('4B', yhi, ylo, self._month, self._day),)\n\n    def __setstate(self, string):\n        yhi, ylo, self._month, self._day = (ord(string[0]), ord(string[1]),\n                                            ord(string[2]), ord(string[3]))\n        self._year = yhi * 256 + ylo\n\n    def __reduce__(self):\n        return (self.__class__, self._getstate())\n\n_date_class = date  # so functions w/ args named \"date\" can get at the class\n\ndate.min = date(1, 1, 1)\ndate.max = date(9999, 12, 31)\ndate.resolution = timedelta(days=1)\n\nclass tzinfo(object):\n    \"\"\"Abstract base class for time zone info classes.\n\n    Subclasses must override the name(), utcoffset() and dst() methods.\n    \"\"\"\n    __slots__ = ()\n\n    def tzname(self, dt):\n        \"datetime -> string name of time zone.\"\n        raise NotImplementedError(\"tzinfo subclass must override tzname()\")\n\n    def utcoffset(self, dt):\n        \"datetime -> minutes east of UTC (negative for west of UTC)\"\n        raise NotImplementedError(\"tzinfo subclass must override utcoffset()\")\n\n    def dst(self, dt):\n        \"\"\"datetime -> DST offset in minutes east of UTC.\n\n        Return 0 if DST not in effect.  utcoffset() must include the DST\n        offset.\n        \"\"\"\n        raise NotImplementedError(\"tzinfo subclass must override dst()\")\n\n    def fromutc(self, dt):\n        \"datetime in UTC -> datetime in local time.\"\n\n        if not isinstance(dt, datetime):\n            raise TypeError(\"fromutc() requires a datetime argument\")\n        if dt.tzinfo is not self:\n            raise ValueError(\"dt.tzinfo is not self\")\n\n        dtoff = dt.utcoffset()\n        if dtoff is None:\n            raise ValueError(\"fromutc() requires a non-None utcoffset() \"\n                             \"result\")\n\n        # See the long comment block at the end of this file for an\n        # explanation of this algorithm.\n        dtdst = dt.dst()\n        if dtdst is None:\n            raise ValueError(\"fromutc() requires a non-None dst() result\")\n        delta = dtoff - dtdst\n        if delta:\n            dt += delta\n            dtdst = dt.dst()\n            if dtdst is None:\n                raise ValueError(\"fromutc(): dt.dst gave inconsistent \"\n                                 \"results; cannot convert\")\n        if dtdst:\n            return dt + dtdst\n        else:\n            return dt\n\n    # Pickle support.\n\n    def __reduce__(self):\n        getinitargs = getattr(self, \"__getinitargs__\", None)\n        if getinitargs:\n            args = getinitargs()\n        else:\n            args = ()\n        getstate = getattr(self, \"__getstate__\", None)\n        if getstate:\n            state = getstate()\n        else:\n            state = getattr(self, \"__dict__\", None) or None\n        if state is None:\n            return (self.__class__, args)\n        else:\n            return (self.__class__, args, state)\n\n_tzinfo_class = tzinfo\n\nclass time(object):\n    \"\"\"Time with time zone.\n\n    Constructors:\n\n    __new__()\n\n    Operators:\n\n    __repr__, __str__\n    __cmp__, __hash__\n\n    Methods:\n\n    strftime()\n    isoformat()\n    utcoffset()\n    tzname()\n    dst()\n\n    Properties (readonly):\n    hour, minute, second, microsecond, tzinfo\n    \"\"\"\n    __slots__ = '_hour', '_minute', '_second', '_microsecond', '_tzinfo', '_hashcode'\n\n    def __new__(cls, hour=0, minute=0, second=0, microsecond=0, tzinfo=None):\n        \"\"\"Constructor.\n\n        Arguments:\n\n        hour, minute (required)\n        second, microsecond (default to zero)\n        tzinfo (default to None)\n        \"\"\"\n        if isinstance(hour, bytes) and len(hour) == 6 and ord(hour[0]) < 24:\n            # Pickle support\n            self = object.__new__(cls)\n            self.__setstate(hour, minute or None)\n            self._hashcode = -1\n            return self\n        hour, minute, second, microsecond = _check_time_fields(\n            hour, minute, second, microsecond)\n        _check_tzinfo_arg(tzinfo)\n        self = object.__new__(cls)\n        self._hour = hour\n        self._minute = minute\n        self._second = second\n        self._microsecond = microsecond\n        self._tzinfo = tzinfo\n        self._hashcode = -1\n        return self\n\n    # Read-only field accessors\n    @property\n    def hour(self):\n        \"\"\"hour (0-23)\"\"\"\n        return self._hour\n\n    @property\n    def minute(self):\n        \"\"\"minute (0-59)\"\"\"\n        return self._minute\n\n    @property\n    def second(self):\n        \"\"\"second (0-59)\"\"\"\n        return self._second\n\n    @property\n    def microsecond(self):\n        \"\"\"microsecond (0-999999)\"\"\"\n        return self._microsecond\n\n    @property\n    def tzinfo(self):\n        \"\"\"timezone info object\"\"\"\n        return self._tzinfo\n\n    # Standard conversions, __hash__ (and helpers)\n\n    # Comparisons of time objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) == 0\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) != 0\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) <= 0\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) < 0\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) >= 0\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) > 0\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, time)\n        mytz = self._tzinfo\n        ottz = other._tzinfo\n        myoff = otoff = None\n\n        if mytz is ottz:\n            base_compare = True\n        else:\n            myoff = self._utcoffset()\n            otoff = other._utcoffset()\n            base_compare = myoff == otoff\n\n        if base_compare:\n            return _cmp((self._hour, self._minute, self._second,\n                         self._microsecond),\n                        (other._hour, other._minute, other._second,\n                         other._microsecond))\n        if myoff is None or otoff is None:\n            raise TypeError(\"can't compare offset-naive and offset-aware times\")\n        myhhmm = self._hour * 60 + self._minute - myoff\n        othhmm = other._hour * 60 + other._minute - otoff\n        return _cmp((myhhmm, self._second, self._microsecond),\n                    (othhmm, other._second, other._microsecond))\n\n    def __hash__(self):\n        \"\"\"Hash.\"\"\"\n        if self._hashcode == -1:\n            tzoff = self._utcoffset()\n            if not tzoff:  # zero or None\n                self._hashcode = hash(self._getstate()[0])\n            else:\n                h, m = divmod(self.hour * 60 + self.minute - tzoff, 60)\n                if 0 <= h < 24:\n                    self._hashcode = hash(time(h, m, self.second, self.microsecond))\n                else:\n                    self._hashcode = hash((h, m, self.second, self.microsecond))\n        return self._hashcode\n\n    # Conversion to string\n\n    def _tzstr(self, sep=\":\"):\n        \"\"\"Return formatted timezone offset (+xx:xx) or None.\"\"\"\n        off = self._utcoffset()\n        if off is not None:\n            if off < 0:\n                sign = \"-\"\n                off = -off\n            else:\n                sign = \"+\"\n            hh, mm = divmod(off, 60)\n            assert 0 <= hh < 24\n            off = \"%s%02d%s%02d\" % (sign, hh, sep, mm)\n        return off\n\n    def __repr__(self):\n        \"\"\"Convert to formal string, for repr().\"\"\"\n        if self._microsecond != 0:\n            s = \", %d, %d\" % (self._second, self._microsecond)\n        elif self._second != 0:\n            s = \", %d\" % self._second\n        else:\n            s = \"\"\n        s= \"%s(%d, %d%s)\" % ('datetime.' + self.__class__.__name__,\n                             self._hour, self._minute, s)\n        if self._tzinfo is not None:\n            assert s[-1:] == \")\"\n            s = s[:-1] + \", tzinfo=%r\" % self._tzinfo + \")\"\n        return s\n\n    def isoformat(self):\n        \"\"\"Return the time formatted according to ISO.\n\n        This is 'HH:MM:SS.mmmmmm+zz:zz', or 'HH:MM:SS+zz:zz' if\n        self.microsecond == 0.\n        \"\"\"\n        s = _format_time(self._hour, self._minute, self._second,\n                         self._microsecond)\n        tz = self._tzstr()\n        if tz:\n            s += tz\n        return s\n\n    __str__ = isoformat\n\n    def strftime(self, fmt):\n        \"\"\"Format using strftime().  The date part of the timestamp passed\n        to underlying strftime should not be used.\n        \"\"\"\n        # The year must be >= _MINYEARFMT else Python's strftime implementation\n        # can raise a bogus exception.\n        timetuple = (1900, 1, 1,\n                     self._hour, self._minute, self._second,\n                     0, 1, -1)\n        return _wrap_strftime(self, fmt, timetuple)\n\n    def __format__(self, fmt):\n        if not isinstance(fmt, (str, unicode)):\n            raise ValueError(\"__format__ expects str or unicode, not %s\" %\n                             fmt.__class__.__name__)\n        if len(fmt) != 0:\n            return self.strftime(fmt)\n        return str(self)\n\n    # Timezone functions\n\n    def utcoffset(self):\n        \"\"\"Return the timezone offset in minutes east of UTC (negative west of\n        UTC).\"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(None)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _utcoffset(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(None)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        return offset\n\n    def tzname(self):\n        \"\"\"Return the timezone name.\n\n        Note that the name is 100% informational -- there's no requirement that\n        it mean anything in particular. For example, \"GMT\", \"UTC\", \"-500\",\n        \"-5:00\", \"EDT\", \"US/Eastern\", \"America/New York\" are all valid replies.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        name = self._tzinfo.tzname(None)\n        _check_tzname(name)\n        return name\n\n    def dst(self):\n        \"\"\"Return 0 if DST is not in effect, or the DST offset (in minutes\n        eastward) if DST is in effect.\n\n        This is purely informational; the DST offset has already been added to\n        the UTC offset returned by utcoffset() if applicable, so there's no\n        need to consult dst() unless you're interested in displaying the DST\n        info.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(None)\n        offset = _check_utc_offset(\"dst\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _dst(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(None)\n        offset = _check_utc_offset(\"dst\", offset)\n        return offset\n\n    def replace(self, hour=None, minute=None, second=None, microsecond=None,\n                tzinfo=True):\n        \"\"\"Return a new time with new values for the specified fields.\"\"\"\n        if hour is None:\n            hour = self.hour\n        if minute is None:\n            minute = self.minute\n        if second is None:\n            second = self.second\n        if microsecond is None:\n            microsecond = self.microsecond\n        if tzinfo is True:\n            tzinfo = self.tzinfo\n        return time(hour, minute, second, microsecond, tzinfo)\n\n    def __nonzero__(self):\n        if self.second or self.microsecond:\n            return True\n        offset = self._utcoffset() or 0\n        return self.hour * 60 + self.minute != offset\n\n    # Pickle support.\n\n    def _getstate(self):\n        us2, us3 = divmod(self._microsecond, 256)\n        us1, us2 = divmod(us2, 256)\n        basestate = _struct.pack('6B', self._hour, self._minute, self._second,\n                                       us1, us2, us3)\n        if self._tzinfo is None:\n            return (basestate,)\n        else:\n            return (basestate, self._tzinfo)\n\n    def __setstate(self, string, tzinfo):\n        if tzinfo is not None and not isinstance(tzinfo, _tzinfo_class):\n            raise TypeError(\"bad tzinfo state arg\")\n        self._hour, self._minute, self._second, us1, us2, us3 = (\n            ord(string[0]), ord(string[1]), ord(string[2]),\n            ord(string[3]), ord(string[4]), ord(string[5]))\n        self._microsecond = (((us1 << 8) | us2) << 8) | us3\n        self._tzinfo = tzinfo\n\n    def __reduce__(self):\n        return (time, self._getstate())\n\n_time_class = time  # so functions w/ args named \"time\" can get at the class\n\ntime.min = time(0, 0, 0)\ntime.max = time(23, 59, 59, 999999)\ntime.resolution = timedelta(microseconds=1)\n\nclass datetime(date):\n    \"\"\"datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\n    The year, month and day arguments are required. tzinfo may be None, or an\n    instance of a tzinfo subclass. The remaining arguments may be ints or longs.\n    \"\"\"\n    __slots__ = date.__slots__ + time.__slots__\n\n    def __new__(cls, year, month=None, day=None, hour=0, minute=0, second=0,\n                microsecond=0, tzinfo=None):\n        if isinstance(year, bytes) and len(year) == 10 and \\\n                1 <= ord(year[2]) <= 12:\n            # Pickle support\n            self = object.__new__(cls)\n            self.__setstate(year, month)\n            self._hashcode = -1\n            return self\n        year, month, day = _check_date_fields(year, month, day)\n        hour, minute, second, microsecond = _check_time_fields(\n            hour, minute, second, microsecond)\n        _check_tzinfo_arg(tzinfo)\n        self = object.__new__(cls)\n        self._year = year\n        self._month = month\n        self._day = day\n        self._hour = hour\n        self._minute = minute\n        self._second = second\n        self._microsecond = microsecond\n        self._tzinfo = tzinfo\n        self._hashcode = -1\n        return self\n\n    # Read-only field accessors\n    @property\n    def hour(self):\n        \"\"\"hour (0-23)\"\"\"\n        return self._hour\n\n    @property\n    def minute(self):\n        \"\"\"minute (0-59)\"\"\"\n        return self._minute\n\n    @property\n    def second(self):\n        \"\"\"second (0-59)\"\"\"\n        return self._second\n\n    @property\n    def microsecond(self):\n        \"\"\"microsecond (0-999999)\"\"\"\n        return self._microsecond\n\n    @property\n    def tzinfo(self):\n        \"\"\"timezone info object\"\"\"\n        return self._tzinfo\n\n    @classmethod\n    def fromtimestamp(cls, t, tz=None):\n        \"\"\"Construct a datetime from a POSIX timestamp (like time.time()).\n\n        A timezone info object may be passed in as well.\n        \"\"\"\n\n        _check_tzinfo_arg(tz)\n\n        converter = _time.localtime if tz is None else _time.gmtime\n\n        t, frac = divmod(t, 1.0)\n        us = _round(frac * 1e6)\n\n        # If timestamp is less than one microsecond smaller than a\n        # full second, us can be rounded up to 1000000.  In this case,\n        # roll over to seconds, otherwise, ValueError is raised\n        # by the constructor.\n        if us == 1000000:\n            t += 1\n            us = 0\n        y, m, d, hh, mm, ss, weekday, jday, dst = converter(t)\n        ss = min(ss, 59)    # clamp out leap seconds if the platform has them\n        result = cls(y, m, d, hh, mm, ss, us, tz)\n        if tz is not None:\n            result = tz.fromutc(result)\n        return result\n\n    @classmethod\n    def utcfromtimestamp(cls, t):\n        \"Construct a UTC datetime from a POSIX timestamp (like time.time()).\"\n        t, frac = divmod(t, 1.0)\n        us = _round(frac * 1e6)\n\n        # If timestamp is less than one microsecond smaller than a\n        # full second, us can be rounded up to 1000000.  In this case,\n        # roll over to seconds, otherwise, ValueError is raised\n        # by the constructor.\n        if us == 1000000:\n            t += 1\n            us = 0\n        y, m, d, hh, mm, ss, weekday, jday, dst = _time.gmtime(t)\n        ss = min(ss, 59)    # clamp out leap seconds if the platform has them\n        return cls(y, m, d, hh, mm, ss, us)\n\n    @classmethod\n    def now(cls, tz=None):\n        \"Construct a datetime from time.time() and optional time zone info.\"\n        t = _time.time()\n        return cls.fromtimestamp(t, tz)\n\n    @classmethod\n    def utcnow(cls):\n        \"Construct a UTC datetime from time.time().\"\n        t = _time.time()\n        return cls.utcfromtimestamp(t)\n\n    @classmethod\n    def combine(cls, date, time):\n        \"Construct a datetime from a given date and a given time.\"\n        if not isinstance(date, _date_class):\n            raise TypeError(\"date argument must be a date instance\")\n        if not isinstance(time, _time_class):\n            raise TypeError(\"time argument must be a time instance\")\n        return cls(date.year, date.month, date.day,\n                   time.hour, time.minute, time.second, time.microsecond,\n                   time.tzinfo)\n\n    def timetuple(self):\n        \"Return local time tuple compatible with time.localtime().\"\n        dst = self._dst()\n        if dst is None:\n            dst = -1\n        elif dst:\n            dst = 1\n        return _build_struct_time(self.year, self.month, self.day,\n                                  self.hour, self.minute, self.second,\n                                  dst)\n\n    def utctimetuple(self):\n        \"Return UTC time tuple compatible with time.gmtime().\"\n        y, m, d = self.year, self.month, self.day\n        hh, mm, ss = self.hour, self.minute, self.second\n        offset = self._utcoffset()\n        if offset:  # neither None nor 0\n            tm = _tmxxx(y, m, d, hh, mm - offset)\n            y, m, d = tm.year, tm.month, tm.day\n            hh, mm = tm.hour, tm.minute\n        return _build_struct_time(y, m, d, hh, mm, ss, 0)\n\n    def date(self):\n        \"Return the date part.\"\n        return date(self._year, self._month, self._day)\n\n    def time(self):\n        \"Return the time part, with tzinfo None.\"\n        return time(self.hour, self.minute, self.second, self.microsecond)\n\n    def timetz(self):\n        \"Return the time part, with same tzinfo.\"\n        return time(self.hour, self.minute, self.second, self.microsecond,\n                    self._tzinfo)\n\n    def replace(self, year=None, month=None, day=None, hour=None,\n                minute=None, second=None, microsecond=None, tzinfo=True):\n        \"\"\"Return a new datetime with new values for the specified fields.\"\"\"\n        if year is None:\n            year = self.year\n        if month is None:\n            month = self.month\n        if day is None:\n            day = self.day\n        if hour is None:\n            hour = self.hour\n        if minute is None:\n            minute = self.minute\n        if second is None:\n            second = self.second\n        if microsecond is None:\n            microsecond = self.microsecond\n        if tzinfo is True:\n            tzinfo = self.tzinfo\n        return datetime(year, month, day, hour, minute, second, microsecond,\n                        tzinfo)\n\n    def astimezone(self, tz):\n        if not isinstance(tz, tzinfo):\n            raise TypeError(\"tz argument must be an instance of tzinfo\")\n\n        mytz = self.tzinfo\n        if mytz is None:\n            raise ValueError(\"astimezone() requires an aware datetime\")\n\n        if tz is mytz:\n            return self\n\n        # Convert self to UTC, and attach the new time zone object.\n        myoffset = self.utcoffset()\n        if myoffset is None:\n            raise ValueError(\"astimezone() requires an aware datetime\")\n        utc = (self - myoffset).replace(tzinfo=tz)\n\n        # Convert from UTC to tz's local time.\n        return tz.fromutc(utc)\n\n    # Ways to produce a string.\n\n    def ctime(self):\n        \"Return ctime() style string.\"\n        weekday = self.toordinal() % 7 or 7\n        return \"%s %s %2d %02d:%02d:%02d %04d\" % (\n            _DAYNAMES[weekday],\n            _MONTHNAMES[self._month],\n            self._day,\n            self._hour, self._minute, self._second,\n            self._year)\n\n    def isoformat(self, sep='T'):\n        \"\"\"Return the time formatted according to ISO.\n\n        This is 'YYYY-MM-DD HH:MM:SS.mmmmmm', or 'YYYY-MM-DD HH:MM:SS' if\n        self.microsecond == 0.\n\n        If self.tzinfo is not None, the UTC offset is also attached, giving\n        'YYYY-MM-DD HH:MM:SS.mmmmmm+HH:MM' or 'YYYY-MM-DD HH:MM:SS+HH:MM'.\n\n        Optional argument sep specifies the separator between date and\n        time, default 'T'.\n        \"\"\"\n        s = (\"%04d-%02d-%02d%c\" % (self._year, self._month, self._day, sep) +\n             _format_time(self._hour, self._minute, self._second,\n                          self._microsecond))\n        off = self._utcoffset()\n        if off is not None:\n            if off < 0:\n                sign = \"-\"\n                off = -off\n            else:\n                sign = \"+\"\n            hh, mm = divmod(off, 60)\n            s += \"%s%02d:%02d\" % (sign, hh, mm)\n        return s\n\n    def __repr__(self):\n        \"\"\"Convert to formal string, for repr().\"\"\"\n        L = [self._year, self._month, self._day,  # These are never zero\n             self._hour, self._minute, self._second, self._microsecond]\n        if L[-1] == 0:\n            del L[-1]\n        if L[-1] == 0:\n            del L[-1]\n        s = \", \".join(map(str, L))\n        s = \"%s(%s)\" % ('datetime.' + self.__class__.__name__, s)\n        if self._tzinfo is not None:\n            assert s[-1:] == \")\"\n            s = s[:-1] + \", tzinfo=%r\" % self._tzinfo + \")\"\n        return s\n\n    def __str__(self):\n        \"Convert to string, for str().\"\n        return self.isoformat(sep=' ')\n\n    @classmethod\n    def strptime(cls, date_string, format):\n        'string, format -> new datetime parsed from a string (like time.strptime()).'\n        from _strptime import _strptime\n        # _strptime._strptime returns a two-element tuple.  The first\n        # element is a time.struct_time object.  The second is the\n        # microseconds (which are not defined for time.struct_time).\n        struct, micros = _strptime(date_string, format)\n        return cls(*(struct[0:6] + (micros,)))\n\n    def utcoffset(self):\n        \"\"\"Return the timezone offset in minutes east of UTC (negative west of\n        UTC).\"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(self)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _utcoffset(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(self)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        return offset\n\n    def tzname(self):\n        \"\"\"Return the timezone name.\n\n        Note that the name is 100% informational -- there's no requirement that\n        it mean anything in particular. For example, \"GMT\", \"UTC\", \"-500\",\n        \"-5:00\", \"EDT\", \"US/Eastern\", \"America/New York\" are all valid replies.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        name = self._tzinfo.tzname(self)\n        _check_tzname(name)\n        return name\n\n    def dst(self):\n        \"\"\"Return 0 if DST is not in effect, or the DST offset (in minutes\n        eastward) if DST is in effect.\n\n        This is purely informational; the DST offset has already been added to\n        the UTC offset returned by utcoffset() if applicable, so there's no\n        need to consult dst() unless you're interested in displaying the DST\n        info.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(self)\n        offset = _check_utc_offset(\"dst\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _dst(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(self)\n        offset = _check_utc_offset(\"dst\", offset)\n        return offset\n\n    # Comparisons of datetime objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) == 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) != 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) <= 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) < 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) >= 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) > 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, datetime)\n        mytz = self._tzinfo\n        ottz = other._tzinfo\n        myoff = otoff = None\n\n        if mytz is ottz:\n            base_compare = True\n        else:\n            if mytz is not None:\n                myoff = self._utcoffset()\n            if ottz is not None:\n                otoff = other._utcoffset()\n            base_compare = myoff == otoff\n\n        if base_compare:\n            return _cmp((self._year, self._month, self._day,\n                         self._hour, self._minute, self._second,\n                         self._microsecond),\n                        (other._year, other._month, other._day,\n                         other._hour, other._minute, other._second,\n                         other._microsecond))\n        if myoff is None or otoff is None:\n            raise TypeError(\"can't compare offset-naive and offset-aware datetimes\")\n        # XXX What follows could be done more efficiently...\n        diff = self - other     # this will take offsets into account\n        if diff.days < 0:\n            return -1\n        return diff and 1 or 0\n\n    def __add__(self, other):\n        \"Add a datetime and a timedelta.\"\n        if not isinstance(other, timedelta):\n            return NotImplemented\n        t = _tmxxx(self._year,\n                  self._month,\n                  self._day + other.days,\n                  self._hour,\n                  self._minute,\n                  self._second + other.seconds,\n                  self._microsecond + other.microseconds)\n        self._checkOverflow(t.year)\n        result = datetime(t.year, t.month, t.day,\n                                t.hour, t.minute, t.second,\n                                t.microsecond, tzinfo=self._tzinfo)\n        return result\n\n    __radd__ = __add__\n\n    def __sub__(self, other):\n        \"Subtract two datetimes, or a datetime and a timedelta.\"\n        if not isinstance(other, datetime):\n            if isinstance(other, timedelta):\n                return self + -other\n            return NotImplemented\n\n        days1 = self.toordinal()\n        days2 = other.toordinal()\n        secs1 = self._second + self._minute * 60 + self._hour * 3600\n        secs2 = other._second + other._minute * 60 + other._hour * 3600\n        base = timedelta(days1 - days2,\n                         secs1 - secs2,\n                         self._microsecond - other._microsecond)\n        if self._tzinfo is other._tzinfo:\n            return base\n        myoff = self._utcoffset()\n        otoff = other._utcoffset()\n        if myoff == otoff:\n            return base\n        if myoff is None or otoff is None:\n            raise TypeError(\"can't subtract offset-naive and offset-aware datetimes\")\n        return base + timedelta(minutes = otoff-myoff)\n\n    def __hash__(self):\n        if self._hashcode == -1:\n            tzoff = self._utcoffset()\n            if tzoff is None:\n                self._hashcode = hash(self._getstate()[0])\n            else:\n                days = _ymd2ord(self.year, self.month, self.day)\n                seconds = self.hour * 3600 + (self.minute - tzoff) * 60 + self.second\n                self._hashcode = hash(timedelta(days, seconds, self.microsecond))\n        return self._hashcode\n\n    # Pickle support.\n\n    def _getstate(self):\n        yhi, ylo = divmod(self._year, 256)\n        us2, us3 = divmod(self._microsecond, 256)\n        us1, us2 = divmod(us2, 256)\n        basestate = _struct.pack('10B', yhi, ylo, self._month, self._day,\n                                        self._hour, self._minute, self._second,\n                                        us1, us2, us3)\n        if self._tzinfo is None:\n            return (basestate,)\n        else:\n            return (basestate, self._tzinfo)\n\n    def __setstate(self, string, tzinfo):\n        if tzinfo is not None and not isinstance(tzinfo, _tzinfo_class):\n            raise TypeError(\"bad tzinfo state arg\")\n        (yhi, ylo, self._month, self._day, self._hour,\n            self._minute, self._second, us1, us2, us3) = (ord(string[0]),\n                ord(string[1]), ord(string[2]), ord(string[3]),\n                ord(string[4]), ord(string[5]), ord(string[6]),\n                ord(string[7]), ord(string[8]), ord(string[9]))\n        self._year = yhi * 256 + ylo\n        self._microsecond = (((us1 << 8) | us2) << 8) | us3\n        self._tzinfo = tzinfo\n\n    def __reduce__(self):\n        return (self.__class__, self._getstate())\n\n\ndatetime.min = datetime(1, 1, 1)\ndatetime.max = datetime(9999, 12, 31, 23, 59, 59, 999999)\ndatetime.resolution = timedelta(microseconds=1)\n\n\ndef _isoweek1monday(year):\n    # Helper to calculate the day number of the Monday starting week 1\n    # XXX This could be done more efficiently\n    THURSDAY = 3\n    firstday = _ymd2ord(year, 1, 1)\n    firstweekday = (firstday + 6) % 7  # See weekday() above\n    week1monday = firstday - firstweekday\n    if firstweekday > THURSDAY:\n        week1monday += 7\n    return week1monday\n\n\"\"\"\nSome time zone algebra.  For a datetime x, let\n    x.n = x stripped of its timezone -- its naive time.\n    x.o = x.utcoffset(), and assuming that doesn't raise an exception or\n          return None\n    x.d = x.dst(), and assuming that doesn't raise an exception or\n          return None\n    x.s = x's standard offset, x.o - x.d\n\nNow some derived rules, where k is a duration (timedelta).\n\n1. x.o = x.s + x.d\n   This follows from the definition of x.s.\n\n2. If x and y have the same tzinfo member, x.s = y.s.\n   This is actually a requirement, an assumption we need to make about\n   sane tzinfo classes.\n\n3. The naive UTC time corresponding to x is x.n - x.o.\n   This is again a requirement for a sane tzinfo class.\n\n4. (x+k).s = x.s\n   This follows from #2, and that datimetimetz+timedelta preserves tzinfo.\n\n5. (x+k).n = x.n + k\n   Again follows from how arithmetic is defined.\n\nNow we can explain tz.fromutc(x).  Let's assume it's an interesting case\n(meaning that the various tzinfo methods exist, and don't blow up or return\nNone when called).\n\nThe function wants to return a datetime y with timezone tz, equivalent to x.\nx is already in UTC.\n\nBy #3, we want\n\n    y.n - y.o = x.n                             [1]\n\nThe algorithm starts by attaching tz to x.n, and calling that y.  So\nx.n = y.n at the start.  Then it wants to add a duration k to y, so that [1]\nbecomes true; in effect, we want to solve [2] for k:\n\n   (y+k).n - (y+k).o = x.n                      [2]\n\nBy #1, this is the same as\n\n   (y+k).n - ((y+k).s + (y+k).d) = x.n          [3]\n\nBy #5, (y+k).n = y.n + k, which equals x.n + k because x.n=y.n at the start.\nSubstituting that into [3],\n\n   x.n + k - (y+k).s - (y+k).d = x.n; the x.n terms cancel, leaving\n   k - (y+k).s - (y+k).d = 0; rearranging,\n   k = (y+k).s - (y+k).d; by #4, (y+k).s == y.s, so\n   k = y.s - (y+k).d\n\nOn the RHS, (y+k).d can't be computed directly, but y.s can be, and we\napproximate k by ignoring the (y+k).d term at first.  Note that k can't be\nvery large, since all offset-returning methods return a duration of magnitude\nless than 24 hours.  For that reason, if y is firmly in std time, (y+k).d must\nbe 0, so ignoring it has no consequence then.\n\nIn any case, the new value is\n\n    z = y + y.s                                 [4]\n\nIt's helpful to step back at look at [4] from a higher level:  it's simply\nmapping from UTC to tz's standard time.\n\nAt this point, if\n\n    z.n - z.o = x.n                             [5]\n\nwe have an equivalent time, and are almost done.  The insecurity here is\nat the start of daylight time.  Picture US Eastern for concreteness.  The wall\ntime jumps from 1:59 to 3:00, and wall hours of the form 2:MM don't make good\nsense then.  The docs ask that an Eastern tzinfo class consider such a time to\nbe EDT (because it's \"after 2\"), which is a redundant spelling of 1:MM EST\non the day DST starts.  We want to return the 1:MM EST spelling because that's\nthe only spelling that makes sense on the local wall clock.\n\nIn fact, if [5] holds at this point, we do have the standard-time spelling,\nbut that takes a bit of proof.  We first prove a stronger result.  What's the\ndifference between the LHS and RHS of [5]?  Let\n\n    diff = x.n - (z.n - z.o)                    [6]\n\nNow\n    z.n =                       by [4]\n    (y + y.s).n =               by #5\n    y.n + y.s =                 since y.n = x.n\n    x.n + y.s =                 since z and y are have the same tzinfo member,\n                                    y.s = z.s by #2\n    x.n + z.s\n\nPlugging that back into [6] gives\n\n    diff =\n    x.n - ((x.n + z.s) - z.o) =     expanding\n    x.n - x.n - z.s + z.o =         cancelling\n    - z.s + z.o =                   by #2\n    z.d\n\nSo diff = z.d.\n\nIf [5] is true now, diff = 0, so z.d = 0 too, and we have the standard-time\nspelling we wanted in the endcase described above.  We're done.  Contrarily,\nif z.d = 0, then we have a UTC equivalent, and are also done.\n\nIf [5] is not true now, diff = z.d != 0, and z.d is the offset we need to\nadd to z (in effect, z is in tz's standard time, and we need to shift the\nlocal clock into tz's daylight time).\n\nLet\n\n    z' = z + z.d = z + diff                     [7]\n\nand we can again ask whether\n\n    z'.n - z'.o = x.n                           [8]\n\nIf so, we're done.  If not, the tzinfo class is insane, according to the\nassumptions we've made.  This also requires a bit of proof.  As before, let's\ncompute the difference between the LHS and RHS of [8] (and skipping some of\nthe justifications for the kinds of substitutions we've done several times\nalready):\n\n    diff' = x.n - (z'.n - z'.o) =           replacing z'.n via [7]\n            x.n  - (z.n + diff - z'.o) =    replacing diff via [6]\n            x.n - (z.n + x.n - (z.n - z.o) - z'.o) =\n            x.n - z.n - x.n + z.n - z.o + z'.o =    cancel x.n\n            - z.n + z.n - z.o + z'.o =              cancel z.n\n            - z.o + z'.o =                      #1 twice\n            -z.s - z.d + z'.s + z'.d =          z and z' have same tzinfo\n            z'.d - z.d\n\nSo z' is UTC-equivalent to x iff z'.d = z.d at this point.  If they are equal,\nwe've found the UTC-equivalent so are done.  In fact, we stop with [7] and\nreturn z', not bothering to compute z'.d.\n\nHow could z.d and z'd differ?  z' = z + z.d [7], so merely moving z' by\na dst() offset, and starting *from* a time already in DST (we know z.d != 0),\nwould have to change the result dst() returns:  we start in DST, and moving\na little further into it takes us out of DST.\n\nThere isn't a sane case where this can happen.  The closest it gets is at\nthe end of DST, where there's an hour in UTC with no spelling in a hybrid\ntzinfo class.  In US Eastern, that's 5:MM UTC = 0:MM EST = 1:MM EDT.  During\nthat hour, on an Eastern clock 1:MM is taken as being in standard time (6:MM\nUTC) because the docs insist on that, but 0:MM is taken as being in daylight\ntime (4:MM UTC).  There is no local time mapping to 5:MM UTC.  The local\nclock jumps from 1:59 back to 1:00 again, and repeats the 1:MM hour in\nstandard time.  Since that's what the local clock *does*, we want to map both\nUTC hours 5:MM and 6:MM to 1:MM Eastern.  The result is ambiguous\nin local time, but so it goes -- it's the way the local clock works.\n\nWhen x = 5:MM UTC is the input to this algorithm, x.o=0, y.o=-5 and y.d=0,\nso z=0:MM.  z.d=60 (minutes) then, so [5] doesn't hold and we keep going.\nz' = z + z.d = 1:MM then, and z'.d=0, and z'.d - z.d = -60 != 0 so [8]\n(correctly) concludes that z' is not UTC-equivalent to x.\n\nBecause we know z.d said z was in daylight time (else [5] would have held and\nwe would have stopped then), and we know z.d != z'.d (else [8] would have held\nand we have stopped then), and there are only 2 possible values dst() can\nreturn in Eastern, it follows that z'.d must be 0 (which it is in the example,\nbut the reasoning doesn't depend on the example -- it depends on there being\ntwo possible dst() outcomes, one zero and the other non-zero).  Therefore\nz' must be in standard time, and is the spelling we want in this case.\n\nNote again that z' is not UTC-equivalent as far as the hybrid tzinfo class is\nconcerned (because it takes z' as being in standard time rather than the\ndaylight time we intend here), but returning it gives the real-life \"local\nclock repeats an hour\" behavior when mapping the \"unspellable\" UTC hour into\ntz.\n\nWhen the input is 6:MM, z=1:MM and z.d=0, and we stop at once, again with\nthe 1:MM standard time spelling we want.\n\nSo how can this break?  One of the assumptions must be violated.  Two\npossibilities:\n\n1) [2] effectively says that y.s is invariant across all y belong to a given\n   time zone.  This isn't true if, for political reasons or continental drift,\n   a region decides to change its base offset from UTC.\n\n2) There may be versions of \"double daylight\" time where the tail end of\n   the analysis gives up a step too early.  I haven't thought about that\n   enough to say.\n\nIn any case, it's clear that the default fromutc() is strong enough to handle\n\"almost all\" time zones:  so long as the standard offset is invariant, it\ndoesn't matter if daylight time transition points change from year to year, or\nif daylight time is skipped in some years; it doesn't matter how large or\nsmall dst() may get within its bounds; and it doesn't even matter if some\nperverse time zone returns a negative dst()).  So a breaking case must be\npretty bizarre, and a tzinfo subclass can override fromutc() if it is.\n\"\"\"\n",
		"file_name": "datetime.py"
	},
	{
		"content": "\"\"\"Strptime-related classes and functions.\n\nCLASSES:\n    LocaleTime -- Discovers and stores locale-specific time information\n    TimeRE -- Creates regexes for pattern matching a string of text containing\n                time information\n\nFUNCTIONS:\n    _getlang -- Figure out what language is being used for the locale\n    strptime -- Calculates the time struct represented by the passed-in string\n\n\"\"\"\nimport time\nimport locale\nimport calendar\nfrom re import compile as re_compile\nfrom re import IGNORECASE\nfrom re import escape as re_escape\nfrom datetime import date as datetime_date\ntry:\n    from thread import allocate_lock as _thread_allocate_lock\nexcept:\n    from dummy_thread import allocate_lock as _thread_allocate_lock\n\n__all__ = []\n\ndef _getlang():\n    # Figure out what the current language is set to.\n    return locale.getlocale(locale.LC_TIME)\n\nclass LocaleTime(object):\n    \"\"\"Stores and handles locale-specific information related to time.\n\n    ATTRIBUTES:\n        f_weekday -- full weekday names (7-item list)\n        a_weekday -- abbreviated weekday names (7-item list)\n        f_month -- full month names (13-item list; dummy value in [0], which\n                    is added by code)\n        a_month -- abbreviated month names (13-item list, dummy value in\n                    [0], which is added by code)\n        am_pm -- AM/PM representation (2-item list)\n        LC_date_time -- format string for date/time representation (string)\n        LC_date -- format string for date representation (string)\n        LC_time -- format string for time representation (string)\n        timezone -- daylight- and non-daylight-savings timezone representation\n                    (2-item list of sets)\n        lang -- Language used by instance (2-item tuple)\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Set all attributes.\n\n        Order of methods called matters for dependency reasons.\n\n        The locale language is set at the offset and then checked again before\n        exiting.  This is to make sure that the attributes were not set with a\n        mix of information from more than one locale.  This would most likely\n        happen when using threads where one thread calls a locale-dependent\n        function while another thread changes the locale while the function in\n        the other thread is still running.  Proper coding would call for\n        locks to prevent changing the locale while locale-dependent code is\n        running.  The check here is done in case someone does not think about\n        doing this.\n\n        Only other possible issue is if someone changed the timezone and did\n        not call tz.tzset .  That is an issue for the programmer, though,\n        since changing the timezone is worthless without that call.\n\n        \"\"\"\n        self.lang = _getlang()\n        self.__calc_weekday()\n        self.__calc_month()\n        self.__calc_am_pm()\n        self.__calc_timezone()\n        self.__calc_date_time()\n        if _getlang() != self.lang:\n            raise ValueError(\"locale changed during initialization\")\n\n    def __pad(self, seq, front):\n        # Add '' to seq to either the front (is True), else the back.\n        seq = list(seq)\n        if front:\n            seq.insert(0, '')\n        else:\n            seq.append('')\n        return seq\n\n    def __calc_weekday(self):\n        # Set self.a_weekday and self.f_weekday using the calendar\n        # module.\n        a_weekday = [calendar.day_abbr[i].lower() for i in range(7)]\n        f_weekday = [calendar.day_name[i].lower() for i in range(7)]\n        self.a_weekday = a_weekday\n        self.f_weekday = f_weekday\n\n    def __calc_month(self):\n        # Set self.f_month and self.a_month using the calendar module.\n        a_month = [calendar.month_abbr[i].lower() for i in range(13)]\n        f_month = [calendar.month_name[i].lower() for i in range(13)]\n        self.a_month = a_month\n        self.f_month = f_month\n\n    def __calc_am_pm(self):\n        # Set self.am_pm by using time.strftime().\n\n        # The magic date (1999,3,17,hour,44,55,2,76,0) is not really that\n        # magical; just happened to have used it everywhere else where a\n        # static date was needed.\n        am_pm = []\n        for hour in (01,22):\n            time_tuple = time.struct_time((1999,3,17,hour,44,55,2,76,0))\n            am_pm.append(time.strftime(\"%p\", time_tuple).lower())\n        self.am_pm = am_pm\n\n    def __calc_date_time(self):\n        # Set self.date_time, self.date, & self.time by using\n        # time.strftime().\n\n        # Use (1999,3,17,22,44,55,2,76,0) for magic date because the amount of\n        # overloaded numbers is minimized.  The order in which searches for\n        # values within the format string is very important; it eliminates\n        # possible ambiguity for what something represents.\n        time_tuple = time.struct_time((1999,3,17,22,44,55,2,76,0))\n        date_time = [None, None, None]\n        date_time[0] = time.strftime(\"%c\", time_tuple).lower()\n        date_time[1] = time.strftime(\"%x\", time_tuple).lower()\n        date_time[2] = time.strftime(\"%X\", time_tuple).lower()\n        replacement_pairs = [('%', '%%'), (self.f_weekday[2], '%A'),\n                    (self.f_month[3], '%B'), (self.a_weekday[2], '%a'),\n                    (self.a_month[3], '%b'), (self.am_pm[1], '%p'),\n                    ('1999', '%Y'), ('99', '%y'), ('22', '%H'),\n                    ('44', '%M'), ('55', '%S'), ('76', '%j'),\n                    ('17', '%d'), ('03', '%m'), ('3', '%m'),\n                    # '3' needed for when no leading zero.\n                    ('2', '%w'), ('10', '%I')]\n        replacement_pairs.extend([(tz, \"%Z\") for tz_values in self.timezone\n                                                for tz in tz_values])\n        for offset,directive in ((0,'%c'), (1,'%x'), (2,'%X')):\n            current_format = date_time[offset]\n            for old, new in replacement_pairs:\n                # Must deal with possible lack of locale info\n                # manifesting itself as the empty string (e.g., Swedish's\n                # lack of AM/PM info) or a platform returning a tuple of empty\n                # strings (e.g., MacOS 9 having timezone as ('','')).\n                if old:\n                    current_format = current_format.replace(old, new)\n            # If %W is used, then Sunday, 2005-01-03 will fall on week 0 since\n            # 2005-01-03 occurs before the first Monday of the year.  Otherwise\n            # %U is used.\n            time_tuple = time.struct_time((1999,1,3,1,1,1,6,3,0))\n            if '00' in time.strftime(directive, time_tuple):\n                U_W = '%W'\n            else:\n                U_W = '%U'\n            date_time[offset] = current_format.replace('11', U_W)\n        self.LC_date_time = date_time[0]\n        self.LC_date = date_time[1]\n        self.LC_time = date_time[2]\n\n    def __calc_timezone(self):\n        # Set self.timezone by using time.tzname.\n        # Do not worry about possibility of time.tzname[0] == timetzname[1]\n        # and time.daylight; handle that in strptime .\n        try:\n            time.tzset()\n        except AttributeError:\n            pass\n        no_saving = frozenset([\"utc\", \"gmt\", time.tzname[0].lower()])\n        if time.daylight:\n            has_saving = frozenset([time.tzname[1].lower()])\n        else:\n            has_saving = frozenset()\n        self.timezone = (no_saving, has_saving)\n\n\nclass TimeRE(dict):\n    \"\"\"Handle conversion from format directives to regexes.\"\"\"\n\n    def __init__(self, locale_time=None):\n        \"\"\"Create keys/values.\n\n        Order of execution is important for dependency reasons.\n\n        \"\"\"\n        if locale_time:\n            self.locale_time = locale_time\n        else:\n            self.locale_time = LocaleTime()\n        base = super(TimeRE, self)\n        base.__init__({\n            # The \" \\d\" part of the regex is to make %c from ANSI C work\n            'd': r\"(?P<d>3[0-1]|[1-2]\\d|0[1-9]|[1-9]| [1-9])\",\n            'f': r\"(?P<f>[0-9]{1,6})\",\n            'H': r\"(?P<H>2[0-3]|[0-1]\\d|\\d)\",\n            'I': r\"(?P<I>1[0-2]|0[1-9]|[1-9])\",\n            'j': r\"(?P<j>36[0-6]|3[0-5]\\d|[1-2]\\d\\d|0[1-9]\\d|00[1-9]|[1-9]\\d|0[1-9]|[1-9])\",\n            'm': r\"(?P<m>1[0-2]|0[1-9]|[1-9])\",\n            'M': r\"(?P<M>[0-5]\\d|\\d)\",\n            'S': r\"(?P<S>6[0-1]|[0-5]\\d|\\d)\",\n            'U': r\"(?P<U>5[0-3]|[0-4]\\d|\\d)\",\n            'w': r\"(?P<w>[0-6])\",\n            # W is set below by using 'U'\n            'y': r\"(?P<y>\\d\\d)\",\n            #XXX: Does 'Y' need to worry about having less or more than\n            #     4 digits?\n            'Y': r\"(?P<Y>\\d\\d\\d\\d)\",\n            'A': self.__seqToRE(self.locale_time.f_weekday, 'A'),\n            'a': self.__seqToRE(self.locale_time.a_weekday, 'a'),\n            'B': self.__seqToRE(self.locale_time.f_month[1:], 'B'),\n            'b': self.__seqToRE(self.locale_time.a_month[1:], 'b'),\n            'p': self.__seqToRE(self.locale_time.am_pm, 'p'),\n            'Z': self.__seqToRE((tz for tz_names in self.locale_time.timezone\n                                        for tz in tz_names),\n                                'Z'),\n            '%': '%'})\n        base.__setitem__('W', base.__getitem__('U').replace('U', 'W'))\n        base.__setitem__('c', self.pattern(self.locale_time.LC_date_time))\n        base.__setitem__('x', self.pattern(self.locale_time.LC_date))\n        base.__setitem__('X', self.pattern(self.locale_time.LC_time))\n\n    def __seqToRE(self, to_convert, directive):\n        \"\"\"Convert a list to a regex string for matching a directive.\n\n        Want possible matching values to be from longest to shortest.  This\n        prevents the possibility of a match occurring for a value that also\n        a substring of a larger value that should have matched (e.g., 'abc'\n        matching when 'abcdef' should have been the match).\n\n        \"\"\"\n        to_convert = sorted(to_convert, key=len, reverse=True)\n        for value in to_convert:\n            if value != '':\n                break\n        else:\n            return ''\n        regex = '|'.join(re_escape(stuff) for stuff in to_convert)\n        regex = '(?P<%s>%s' % (directive, regex)\n        return '%s)' % regex\n\n    def pattern(self, format):\n        \"\"\"Return regex pattern for the format string.\n\n        Need to make sure that any characters that might be interpreted as\n        regex syntax are escaped.\n\n        \"\"\"\n        processed_format = ''\n        # The sub() call escapes all characters that might be misconstrued\n        # as regex syntax.  Cannot use re.escape since we have to deal with\n        # format directives (%m, etc.).\n        regex_chars = re_compile(r\"([\\\\.^$*+?\\(\\){}\\[\\]|])\")\n        format = regex_chars.sub(r\"\\\\\\1\", format)\n        whitespace_replacement = re_compile('\\s+')\n        format = whitespace_replacement.sub('\\s+', format)\n        while '%' in format:\n            directive_index = format.index('%')+1\n            processed_format = \"%s%s%s\" % (processed_format,\n                                           format[:directive_index-1],\n                                           self[format[directive_index]])\n            format = format[directive_index+1:]\n        return \"%s%s\" % (processed_format, format)\n\n    def compile(self, format):\n        \"\"\"Return a compiled re object for the format string.\"\"\"\n        return re_compile(self.pattern(format), IGNORECASE)\n\n_cache_lock = _thread_allocate_lock()\n# DO NOT modify _TimeRE_cache or _regex_cache without acquiring the cache lock\n# first!\n_TimeRE_cache = TimeRE()\n_CACHE_MAX_SIZE = 5 # Max number of regexes stored in _regex_cache\n_regex_cache = {}\n\ndef _calc_julian_from_U_or_W(year, week_of_year, day_of_week, week_starts_Mon):\n    \"\"\"Calculate the Julian day based on the year, week of the year, and day of\n    the week, with week_start_day representing whether the week of the year\n    assumes the week starts on Sunday or Monday (6 or 0).\"\"\"\n    first_weekday = datetime_date(year, 1, 1).weekday()\n    # If we are dealing with the %U directive (week starts on Sunday), it's\n    # easier to just shift the view to Sunday being the first day of the\n    # week.\n    if not week_starts_Mon:\n        first_weekday = (first_weekday + 1) % 7\n        day_of_week = (day_of_week + 1) % 7\n    # Need to watch out for a week 0 (when the first day of the year is not\n    # the same as that specified by %U or %W).\n    week_0_length = (7 - first_weekday) % 7\n    if week_of_year == 0:\n        return 1 + day_of_week - first_weekday\n    else:\n        days_to_week = week_0_length + (7 * (week_of_year - 1))\n        return 1 + days_to_week + day_of_week\n\n\ndef _strptime(data_string, format=\"%a %b %d %H:%M:%S %Y\"):\n    \"\"\"Return a time struct based on the input string and the format string.\"\"\"\n    global _TimeRE_cache, _regex_cache\n    with _cache_lock:\n        if _getlang() != _TimeRE_cache.locale_time.lang:\n            _TimeRE_cache = TimeRE()\n            _regex_cache.clear()\n        if len(_regex_cache) > _CACHE_MAX_SIZE:\n            _regex_cache.clear()\n        locale_time = _TimeRE_cache.locale_time\n        format_regex = _regex_cache.get(format)\n        if not format_regex:\n            try:\n                format_regex = _TimeRE_cache.compile(format)\n            # KeyError raised when a bad format is found; can be specified as\n            # \\\\, in which case it was a stray % but with a space after it\n            except KeyError, err:\n                bad_directive = err.args[0]\n                if bad_directive == \"\\\\\":\n                    bad_directive = \"%\"\n                del err\n                raise ValueError(\"'%s' is a bad directive in format '%s'\" %\n                                    (bad_directive, format))\n            # IndexError only occurs when the format string is \"%\"\n            except IndexError:\n                raise ValueError(\"stray %% in format '%s'\" % format)\n            _regex_cache[format] = format_regex\n    found = format_regex.match(data_string)\n    if not found:\n        raise ValueError(\"time data %r does not match format %r\" %\n                         (data_string, format))\n    if len(data_string) != found.end():\n        raise ValueError(\"unconverted data remains: %s\" %\n                          data_string[found.end():])\n\n    year = None\n    month = day = 1\n    hour = minute = second = fraction = 0\n    tz = -1\n    # Default to -1 to signify that values not known; not critical to have,\n    # though\n    week_of_year = -1\n    week_of_year_start = -1\n    # weekday and julian defaulted to -1 so as to signal need to calculate\n    # values\n    weekday = julian = -1\n    found_dict = found.groupdict()\n    for group_key in found_dict.iterkeys():\n        # Directives not explicitly handled below:\n        #   c, x, X\n        #      handled by making out of other directives\n        #   U, W\n        #      worthless without day of the week\n        if group_key == 'y':\n            year = int(found_dict['y'])\n            # Open Group specification for strptime() states that a %y\n            #value in the range of [00, 68] is in the century 2000, while\n            #[69,99] is in the century 1900\n            if year <= 68:\n                year += 2000\n            else:\n                year += 1900\n        elif group_key == 'Y':\n            year = int(found_dict['Y'])\n        elif group_key == 'm':\n            month = int(found_dict['m'])\n        elif group_key == 'B':\n            month = locale_time.f_month.index(found_dict['B'].lower())\n        elif group_key == 'b':\n            month = locale_time.a_month.index(found_dict['b'].lower())\n        elif group_key == 'd':\n            day = int(found_dict['d'])\n        elif group_key == 'H':\n            hour = int(found_dict['H'])\n        elif group_key == 'I':\n            hour = int(found_dict['I'])\n            ampm = found_dict.get('p', '').lower()\n            # If there was no AM/PM indicator, we'll treat this like AM\n            if ampm in ('', locale_time.am_pm[0]):\n                # We're in AM so the hour is correct unless we're\n                # looking at 12 midnight.\n                # 12 midnight == 12 AM == hour 0\n                if hour == 12:\n                    hour = 0\n            elif ampm == locale_time.am_pm[1]:\n                # We're in PM so we need to add 12 to the hour unless\n                # we're looking at 12 noon.\n                # 12 noon == 12 PM == hour 12\n                if hour != 12:\n                    hour += 12\n        elif group_key == 'M':\n            minute = int(found_dict['M'])\n        elif group_key == 'S':\n            second = int(found_dict['S'])\n        elif group_key == 'f':\n            s = found_dict['f']\n            # Pad to always return microseconds.\n            s += \"0\" * (6 - len(s))\n            fraction = int(s)\n        elif group_key == 'A':\n            weekday = locale_time.f_weekday.index(found_dict['A'].lower())\n        elif group_key == 'a':\n            weekday = locale_time.a_weekday.index(found_dict['a'].lower())\n        elif group_key == 'w':\n            weekday = int(found_dict['w'])\n            if weekday == 0:\n                weekday = 6\n            else:\n                weekday -= 1\n        elif group_key == 'j':\n            julian = int(found_dict['j'])\n        elif group_key in ('U', 'W'):\n            week_of_year = int(found_dict[group_key])\n            if group_key == 'U':\n                # U starts week on Sunday.\n                week_of_year_start = 6\n            else:\n                # W starts week on Monday.\n                week_of_year_start = 0\n        elif group_key == 'Z':\n            # Since -1 is default value only need to worry about setting tz if\n            # it can be something other than -1.\n            found_zone = found_dict['Z'].lower()\n            for value, tz_values in enumerate(locale_time.timezone):\n                if found_zone in tz_values:\n                    # Deal with bad locale setup where timezone names are the\n                    # same and yet time.daylight is true; too ambiguous to\n                    # be able to tell what timezone has daylight savings\n                    if (time.tzname[0] == time.tzname[1] and\n                       time.daylight and found_zone not in (\"utc\", \"gmt\")):\n                        break\n                    else:\n                        tz = value\n                        break\n    leap_year_fix = False\n    if year is None and month == 2 and day == 29:\n        year = 1904  # 1904 is first leap year of 20th century\n        leap_year_fix = True\n    elif year is None:\n        year = 1900\n    # If we know the week of the year and what day of that week, we can figure\n    # out the Julian day of the year.\n    if julian == -1 and week_of_year != -1 and weekday != -1:\n        week_starts_Mon = True if week_of_year_start == 0 else False\n        julian = _calc_julian_from_U_or_W(year, week_of_year, weekday,\n                                            week_starts_Mon)\n    # Cannot pre-calculate datetime_date() since can change in Julian\n    # calculation and thus could have different value for the day of the week\n    # calculation.\n    if julian == -1:\n        # Need to add 1 to result since first day of the year is 1, not 0.\n        julian = datetime_date(year, month, day).toordinal() - \\\n                  datetime_date(year, 1, 1).toordinal() + 1\n    else:  # Assume that if they bothered to include Julian day it will\n           # be accurate.\n        datetime_result = datetime_date.fromordinal((julian - 1) + datetime_date(year, 1, 1).toordinal())\n        year = datetime_result.year\n        month = datetime_result.month\n        day = datetime_result.day\n    if weekday == -1:\n        weekday = datetime_date(year, month, day).weekday()\n    if leap_year_fix:\n        # the caller didn't supply a year but asked for Feb 29th. We couldn't\n        # use the default of 1900 for computations. We set it back to ensure\n        # that February 29th is smaller than March 1st.\n        year = 1900\n\n    return (time.struct_time((year, month, day,\n                              hour, minute, second,\n                              weekday, julian, tz)), fraction)\n\ndef _strptime_time(data_string, format=\"%a %b %d %H:%M:%S %Y\"):\n    return _strptime(data_string, format)[0]\n",
		"file_name": "_strptime.py"
	},
	{
		"content": "\"\"\"Calendar printing functions\n\nNote when comparing these calendars to the ones printed by cal(1): By\ndefault, these calendars have Monday as the first day of the week, and\nSunday as the last (the European convention). Use setfirstweekday() to\nset the first day of the week (0=Monday, 6=Sunday).\"\"\"\n\nimport sys\nimport datetime\nimport locale as _locale\n\n__all__ = [\"IllegalMonthError\", \"IllegalWeekdayError\", \"setfirstweekday\",\n           \"firstweekday\", \"isleap\", \"leapdays\", \"weekday\", \"monthrange\",\n           \"monthcalendar\", \"prmonth\", \"month\", \"prcal\", \"calendar\",\n           \"timegm\", \"month_name\", \"month_abbr\", \"day_name\", \"day_abbr\"]\n\n# Exception raised for bad input (with string parameter for details)\nerror = ValueError\n\n# Exceptions raised for bad input\nclass IllegalMonthError(ValueError):\n    def __init__(self, month):\n        self.month = month\n    def __str__(self):\n        return \"bad month number %r; must be 1-12\" % self.month\n\n\nclass IllegalWeekdayError(ValueError):\n    def __init__(self, weekday):\n        self.weekday = weekday\n    def __str__(self):\n        return \"bad weekday number %r; must be 0 (Monday) to 6 (Sunday)\" % self.weekday\n\n\n# Constants for months referenced later\nJanuary = 1\nFebruary = 2\n\n# Number of days per month (except for February in leap years)\nmdays = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n\n# This module used to have hard-coded lists of day and month names, as\n# English strings.  The classes following emulate a read-only version of\n# that, but supply localized names.  Note that the values are computed\n# fresh on each call, in case the user changes locale between calls.\n\nclass _localized_month:\n\n    _months = [datetime.date(2001, i+1, 1).strftime for i in range(12)]\n    _months.insert(0, lambda x: \"\")\n\n    def __init__(self, format):\n        self.format = format\n\n    def __getitem__(self, i):\n        funcs = self._months[i]\n        if isinstance(i, slice):\n            return [f(self.format) for f in funcs]\n        else:\n            return funcs(self.format)\n\n    def __len__(self):\n        return 13\n\n\nclass _localized_day:\n\n    # January 1, 2001, was a Monday.\n    _days = [datetime.date(2001, 1, i+1).strftime for i in range(7)]\n\n    def __init__(self, format):\n        self.format = format\n\n    def __getitem__(self, i):\n        funcs = self._days[i]\n        if isinstance(i, slice):\n            return [f(self.format) for f in funcs]\n        else:\n            return funcs(self.format)\n\n    def __len__(self):\n        return 7\n\n\n# Full and abbreviated names of weekdays\nday_name = _localized_day('%A')\nday_abbr = _localized_day('%a')\n\n# Full and abbreviated names of months (1-based arrays!!!)\nmonth_name = _localized_month('%B')\nmonth_abbr = _localized_month('%b')\n\n# Constants for weekdays\n(MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY, SUNDAY) = range(7)\n\n\ndef isleap(year):\n    \"\"\"Return True for leap years, False for non-leap years.\"\"\"\n    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n\n\ndef leapdays(y1, y2):\n    \"\"\"Return number of leap years in range [y1, y2).\n       Assume y1 <= y2.\"\"\"\n    y1 -= 1\n    y2 -= 1\n    return (y2//4 - y1//4) - (y2//100 - y1//100) + (y2//400 - y1//400)\n\n\ndef weekday(year, month, day):\n    \"\"\"Return weekday (0-6 ~ Mon-Sun) for year (1970-...), month (1-12),\n       day (1-31).\"\"\"\n    return datetime.date(year, month, day).weekday()\n\n\ndef monthrange(year, month):\n    \"\"\"Return weekday (0-6 ~ Mon-Sun) and number of days (28-31) for\n       year, month.\"\"\"\n    if not 1 <= month <= 12:\n        raise IllegalMonthError(month)\n    day1 = weekday(year, month, 1)\n    ndays = mdays[month] + (month == February and isleap(year))\n    return day1, ndays\n\n\nclass Calendar(object):\n    \"\"\"\n    Base calendar class. This class doesn't do any formatting. It simply\n    provides data to subclasses.\n    \"\"\"\n\n    def __init__(self, firstweekday=0):\n        self.firstweekday = firstweekday # 0 = Monday, 6 = Sunday\n\n    def getfirstweekday(self):\n        return self._firstweekday % 7\n\n    def setfirstweekday(self, firstweekday):\n        self._firstweekday = firstweekday\n\n    firstweekday = property(getfirstweekday, setfirstweekday)\n\n    def iterweekdays(self):\n        \"\"\"\n        Return a iterator for one week of weekday numbers starting with the\n        configured first one.\n        \"\"\"\n        for i in range(self.firstweekday, self.firstweekday + 7):\n            yield i%7\n\n    def itermonthdates(self, year, month):\n        \"\"\"\n        Return an iterator for one month. The iterator will yield datetime.date\n        values and will always iterate through complete weeks, so it will yield\n        dates outside the specified month.\n        \"\"\"\n        date = datetime.date(year, month, 1)\n        # Go back to the beginning of the week\n        days = (date.weekday() - self.firstweekday) % 7\n        date -= datetime.timedelta(days=days)\n        oneday = datetime.timedelta(days=1)\n        while True:\n            yield date\n            try:\n                date += oneday\n            except OverflowError:\n                # Adding one day could fail after datetime.MAXYEAR\n                break\n            if date.month != month and date.weekday() == self.firstweekday:\n                break\n\n    def itermonthdays2(self, year, month):\n        \"\"\"\n        Like itermonthdates(), but will yield (day number, weekday number)\n        tuples. For days outside the specified month the day number is 0.\n        \"\"\"\n        for date in self.itermonthdates(year, month):\n            if date.month != month:\n                yield (0, date.weekday())\n            else:\n                yield (date.day, date.weekday())\n\n    def itermonthdays(self, year, month):\n        \"\"\"\n        Like itermonthdates(), but will yield day numbers. For days outside\n        the specified month the day number is 0.\n        \"\"\"\n        for date in self.itermonthdates(year, month):\n            if date.month != month:\n                yield 0\n            else:\n                yield date.day\n\n    def monthdatescalendar(self, year, month):\n        \"\"\"\n        Return a matrix (list of lists) representing a month's calendar.\n        Each row represents a week; week entries are datetime.date values.\n        \"\"\"\n        dates = list(self.itermonthdates(year, month))\n        return [ dates[i:i+7] for i in range(0, len(dates), 7) ]\n\n    def monthdays2calendar(self, year, month):\n        \"\"\"\n        Return a matrix representing a month's calendar.\n        Each row represents a week; week entries are\n        (day number, weekday number) tuples. Day numbers outside this month\n        are zero.\n        \"\"\"\n        days = list(self.itermonthdays2(year, month))\n        return [ days[i:i+7] for i in range(0, len(days), 7) ]\n\n    def monthdayscalendar(self, year, month):\n        \"\"\"\n        Return a matrix representing a month's calendar.\n        Each row represents a week; days outside this month are zero.\n        \"\"\"\n        days = list(self.itermonthdays(year, month))\n        return [ days[i:i+7] for i in range(0, len(days), 7) ]\n\n    def yeardatescalendar(self, year, width=3):\n        \"\"\"\n        Return the data for the specified year ready for formatting. The return\n        value is a list of month rows. Each month row contains up to width months.\n        Each month contains between 4 and 6 weeks and each week contains 1-7\n        days. Days are datetime.date objects.\n        \"\"\"\n        months = [\n            self.monthdatescalendar(year, i)\n            for i in range(January, January+12)\n        ]\n        return [months[i:i+width] for i in range(0, len(months), width) ]\n\n    def yeardays2calendar(self, year, width=3):\n        \"\"\"\n        Return the data for the specified year ready for formatting (similar to\n        yeardatescalendar()). Entries in the week lists are\n        (day number, weekday number) tuples. Day numbers outside this month are\n        zero.\n        \"\"\"\n        months = [\n            self.monthdays2calendar(year, i)\n            for i in range(January, January+12)\n        ]\n        return [months[i:i+width] for i in range(0, len(months), width) ]\n\n    def yeardayscalendar(self, year, width=3):\n        \"\"\"\n        Return the data for the specified year ready for formatting (similar to\n        yeardatescalendar()). Entries in the week lists are day numbers.\n        Day numbers outside this month are zero.\n        \"\"\"\n        months = [\n            self.monthdayscalendar(year, i)\n            for i in range(January, January+12)\n        ]\n        return [months[i:i+width] for i in range(0, len(months), width) ]\n\n\nclass TextCalendar(Calendar):\n    \"\"\"\n    Subclass of Calendar that outputs a calendar as a simple plain text\n    similar to the UNIX program cal.\n    \"\"\"\n\n    def prweek(self, theweek, width):\n        \"\"\"\n        Print a single week (no newline).\n        \"\"\"\n        print self.formatweek(theweek, width),\n\n    def formatday(self, day, weekday, width):\n        \"\"\"\n        Returns a formatted day.\n        \"\"\"\n        if day == 0:\n            s = ''\n        else:\n            s = '%2i' % day             # right-align single-digit days\n        return s.center(width)\n\n    def formatweek(self, theweek, width):\n        \"\"\"\n        Returns a single week in a string (no newline).\n        \"\"\"\n        return ' '.join(self.formatday(d, wd, width) for (d, wd) in theweek)\n\n    def formatweekday(self, day, width):\n        \"\"\"\n        Returns a formatted week day name.\n        \"\"\"\n        if width >= 9:\n            names = day_name\n        else:\n            names = day_abbr\n        return names[day][:width].center(width)\n\n    def formatweekheader(self, width):\n        \"\"\"\n        Return a header for a week.\n        \"\"\"\n        return ' '.join(self.formatweekday(i, width) for i in self.iterweekdays())\n\n    def formatmonthname(self, theyear, themonth, width, withyear=True):\n        \"\"\"\n        Return a formatted month name.\n        \"\"\"\n        s = month_name[themonth]\n        if withyear:\n            s = \"%s %r\" % (s, theyear)\n        return s.center(width)\n\n    def prmonth(self, theyear, themonth, w=0, l=0):\n        \"\"\"\n        Print a month's calendar.\n        \"\"\"\n        print self.formatmonth(theyear, themonth, w, l),\n\n    def formatmonth(self, theyear, themonth, w=0, l=0):\n        \"\"\"\n        Return a month's calendar string (multi-line).\n        \"\"\"\n        w = max(2, w)\n        l = max(1, l)\n        s = self.formatmonthname(theyear, themonth, 7 * (w + 1) - 1)\n        s = s.rstrip()\n        s += '\\n' * l\n        s += self.formatweekheader(w).rstrip()\n        s += '\\n' * l\n        for week in self.monthdays2calendar(theyear, themonth):\n            s += self.formatweek(week, w).rstrip()\n            s += '\\n' * l\n        return s\n\n    def formatyear(self, theyear, w=2, l=1, c=6, m=3):\n        \"\"\"\n        Returns a year's calendar as a multi-line string.\n        \"\"\"\n        w = max(2, w)\n        l = max(1, l)\n        c = max(2, c)\n        colwidth = (w + 1) * 7 - 1\n        v = []\n        a = v.append\n        a(repr(theyear).center(colwidth*m+c*(m-1)).rstrip())\n        a('\\n'*l)\n        header = self.formatweekheader(w)\n        for (i, row) in enumerate(self.yeardays2calendar(theyear, m)):\n            # months in this row\n            months = range(m*i+1, min(m*(i+1)+1, 13))\n            a('\\n'*l)\n            names = (self.formatmonthname(theyear, k, colwidth, False)\n                     for k in months)\n            a(formatstring(names, colwidth, c).rstrip())\n            a('\\n'*l)\n            headers = (header for k in months)\n            a(formatstring(headers, colwidth, c).rstrip())\n            a('\\n'*l)\n            # max number of weeks for this row\n            height = max(len(cal) for cal in row)\n            for j in range(height):\n                weeks = []\n                for cal in row:\n                    if j >= len(cal):\n                        weeks.append('')\n                    else:\n                        weeks.append(self.formatweek(cal[j], w))\n                a(formatstring(weeks, colwidth, c).rstrip())\n                a('\\n' * l)\n        return ''.join(v)\n\n    def pryear(self, theyear, w=0, l=0, c=6, m=3):\n        \"\"\"Print a year's calendar.\"\"\"\n        print self.formatyear(theyear, w, l, c, m)\n\n\nclass HTMLCalendar(Calendar):\n    \"\"\"\n    This calendar returns complete HTML pages.\n    \"\"\"\n\n    # CSS classes for the day <td>s\n    cssclasses = [\"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\"]\n\n    def formatday(self, day, weekday):\n        \"\"\"\n        Return a day as a table cell.\n        \"\"\"\n        if day == 0:\n            return '<td class=\"noday\">&nbsp;</td>' # day outside month\n        else:\n            return '<td class=\"%s\">%d</td>' % (self.cssclasses[weekday], day)\n\n    def formatweek(self, theweek):\n        \"\"\"\n        Return a complete week as a table row.\n        \"\"\"\n        s = ''.join(self.formatday(d, wd) for (d, wd) in theweek)\n        return '<tr>%s</tr>' % s\n\n    def formatweekday(self, day):\n        \"\"\"\n        Return a weekday name as a table header.\n        \"\"\"\n        return '<th class=\"%s\">%s</th>' % (self.cssclasses[day], day_abbr[day])\n\n    def formatweekheader(self):\n        \"\"\"\n        Return a header for a week as a table row.\n        \"\"\"\n        s = ''.join(self.formatweekday(i) for i in self.iterweekdays())\n        return '<tr>%s</tr>' % s\n\n    def formatmonthname(self, theyear, themonth, withyear=True):\n        \"\"\"\n        Return a month name as a table row.\n        \"\"\"\n        if withyear:\n            s = '%s %s' % (month_name[themonth], theyear)\n        else:\n            s = '%s' % month_name[themonth]\n        return '<tr><th colspan=\"7\" class=\"month\">%s</th></tr>' % s\n\n    def formatmonth(self, theyear, themonth, withyear=True):\n        \"\"\"\n        Return a formatted month as a table.\n        \"\"\"\n        v = []\n        a = v.append\n        a('<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" class=\"month\">')\n        a('\\n')\n        a(self.formatmonthname(theyear, themonth, withyear=withyear))\n        a('\\n')\n        a(self.formatweekheader())\n        a('\\n')\n        for week in self.monthdays2calendar(theyear, themonth):\n            a(self.formatweek(week))\n            a('\\n')\n        a('</table>')\n        a('\\n')\n        return ''.join(v)\n\n    def formatyear(self, theyear, width=3):\n        \"\"\"\n        Return a formatted year as a table of tables.\n        \"\"\"\n        v = []\n        a = v.append\n        width = max(width, 1)\n        a('<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" class=\"year\">')\n        a('\\n')\n        a('<tr><th colspan=\"%d\" class=\"year\">%s</th></tr>' % (width, theyear))\n        for i in range(January, January+12, width):\n            # months in this row\n            months = range(i, min(i+width, 13))\n            a('<tr>')\n            for m in months:\n                a('<td>')\n                a(self.formatmonth(theyear, m, withyear=False))\n                a('</td>')\n            a('</tr>')\n        a('</table>')\n        return ''.join(v)\n\n    def formatyearpage(self, theyear, width=3, css='calendar.css', encoding=None):\n        \"\"\"\n        Return a formatted year as a complete HTML page.\n        \"\"\"\n        if encoding is None:\n            encoding = sys.getdefaultencoding()\n        v = []\n        a = v.append\n        a('<?xml version=\"1.0\" encoding=\"%s\"?>\\n' % encoding)\n        a('<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\\n')\n        a('<html>\\n')\n        a('<head>\\n')\n        a('<meta http-equiv=\"Content-Type\" content=\"text/html; charset=%s\" />\\n' % encoding)\n        if css is not None:\n            a('<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />\\n' % css)\n        a('<title>Calendar for %d</title>\\n' % theyear)\n        a('</head>\\n')\n        a('<body>\\n')\n        a(self.formatyear(theyear, width))\n        a('</body>\\n')\n        a('</html>\\n')\n        return ''.join(v).encode(encoding, \"xmlcharrefreplace\")\n\n\nclass TimeEncoding:\n    def __init__(self, locale):\n        self.locale = locale\n\n    def __enter__(self):\n        self.oldlocale = _locale.getlocale(_locale.LC_TIME)\n        _locale.setlocale(_locale.LC_TIME, self.locale)\n        return _locale.getlocale(_locale.LC_TIME)[1]\n\n    def __exit__(self, *args):\n        _locale.setlocale(_locale.LC_TIME, self.oldlocale)\n\n\nclass LocaleTextCalendar(TextCalendar):\n    \"\"\"\n    This class can be passed a locale name in the constructor and will return\n    month and weekday names in the specified locale. If this locale includes\n    an encoding all strings containing month and weekday names will be returned\n    as unicode.\n    \"\"\"\n\n    def __init__(self, firstweekday=0, locale=None):\n        TextCalendar.__init__(self, firstweekday)\n        if locale is None:\n            locale = _locale.getdefaultlocale()\n        self.locale = locale\n\n    def formatweekday(self, day, width):\n        with TimeEncoding(self.locale) as encoding:\n            if width >= 9:\n                names = day_name\n            else:\n                names = day_abbr\n            name = names[day]\n            if encoding is not None:\n                name = name.decode(encoding)\n            return name[:width].center(width)\n\n    def formatmonthname(self, theyear, themonth, width, withyear=True):\n        with TimeEncoding(self.locale) as encoding:\n            s = month_name[themonth]\n            if encoding is not None:\n                s = s.decode(encoding)\n            if withyear:\n                s = \"%s %r\" % (s, theyear)\n            return s.center(width)\n\n\nclass LocaleHTMLCalendar(HTMLCalendar):\n    \"\"\"\n    This class can be passed a locale name in the constructor and will return\n    month and weekday names in the specified locale. If this locale includes\n    an encoding all strings containing month and weekday names will be returned\n    as unicode.\n    \"\"\"\n    def __init__(self, firstweekday=0, locale=None):\n        HTMLCalendar.__init__(self, firstweekday)\n        if locale is None:\n            locale = _locale.getdefaultlocale()\n        self.locale = locale\n\n    def formatweekday(self, day):\n        with TimeEncoding(self.locale) as encoding:\n            s = day_abbr[day]\n            if encoding is not None:\n                s = s.decode(encoding)\n            return '<th class=\"%s\">%s</th>' % (self.cssclasses[day], s)\n\n    def formatmonthname(self, theyear, themonth, withyear=True):\n        with TimeEncoding(self.locale) as encoding:\n            s = month_name[themonth]\n            if encoding is not None:\n                s = s.decode(encoding)\n            if withyear:\n                s = '%s %s' % (s, theyear)\n            return '<tr><th colspan=\"7\" class=\"month\">%s</th></tr>' % s\n\n\n# Support for old module level interface\nc = TextCalendar()\n\nfirstweekday = c.getfirstweekday\n\ndef setfirstweekday(firstweekday):\n    try:\n        firstweekday.__index__\n    except AttributeError:\n        raise IllegalWeekdayError(firstweekday)\n    if not MONDAY <= firstweekday <= SUNDAY:\n        raise IllegalWeekdayError(firstweekday)\n    c.firstweekday = firstweekday\n\nmonthcalendar = c.monthdayscalendar\nprweek = c.prweek\nweek = c.formatweek\nweekheader = c.formatweekheader\nprmonth = c.prmonth\nmonth = c.formatmonth\ncalendar = c.formatyear\nprcal = c.pryear\n\n\n# Spacing of month columns for multi-column year calendar\n_colwidth = 7*3 - 1         # Amount printed by prweek()\n_spacing = 6                # Number of spaces between columns\n\n\ndef format(cols, colwidth=_colwidth, spacing=_spacing):\n    \"\"\"Prints multi-column formatting for year calendars\"\"\"\n    print formatstring(cols, colwidth, spacing)\n\n\ndef formatstring(cols, colwidth=_colwidth, spacing=_spacing):\n    \"\"\"Returns a string formatted from n strings, centered within n columns.\"\"\"\n    spacing *= ' '\n    return spacing.join(c.center(colwidth) for c in cols)\n\n\nEPOCH = 1970\n_EPOCH_ORD = datetime.date(EPOCH, 1, 1).toordinal()\n\n\ndef timegm(tuple):\n    \"\"\"Unrelated but handy function to calculate Unix timestamp from GMT.\"\"\"\n    year, month, day, hour, minute, second = tuple[:6]\n    days = datetime.date(year, month, 1).toordinal() - _EPOCH_ORD + day - 1\n    hours = days*24 + hour\n    minutes = hours*60 + minute\n    seconds = minutes*60 + second\n    return seconds\n\n\ndef main(args):\n    import optparse\n    parser = optparse.OptionParser(usage=\"usage: %prog [options] [year [month]]\")\n    parser.add_option(\n        \"-w\", \"--width\",\n        dest=\"width\", type=\"int\", default=2,\n        help=\"width of date column (default 2, text only)\"\n    )\n    parser.add_option(\n        \"-l\", \"--lines\",\n        dest=\"lines\", type=\"int\", default=1,\n        help=\"number of lines for each week (default 1, text only)\"\n    )\n    parser.add_option(\n        \"-s\", \"--spacing\",\n        dest=\"spacing\", type=\"int\", default=6,\n        help=\"spacing between months (default 6, text only)\"\n    )\n    parser.add_option(\n        \"-m\", \"--months\",\n        dest=\"months\", type=\"int\", default=3,\n        help=\"months per row (default 3, text only)\"\n    )\n    parser.add_option(\n        \"-c\", \"--css\",\n        dest=\"css\", default=\"calendar.css\",\n        help=\"CSS to use for page (html only)\"\n    )\n    parser.add_option(\n        \"-L\", \"--locale\",\n        dest=\"locale\", default=None,\n        help=\"locale to be used from month and weekday names\"\n    )\n    parser.add_option(\n        \"-e\", \"--encoding\",\n        dest=\"encoding\", default=None,\n        help=\"Encoding to use for output\"\n    )\n    parser.add_option(\n        \"-t\", \"--type\",\n        dest=\"type\", default=\"text\",\n        choices=(\"text\", \"html\"),\n        help=\"output type (text or html)\"\n    )\n\n    (options, args) = parser.parse_args(args)\n\n    if options.locale and not options.encoding:\n        parser.error(\"if --locale is specified --encoding is required\")\n        sys.exit(1)\n\n    locale = options.locale, options.encoding\n\n    if options.type == \"html\":\n        if options.locale:\n            cal = LocaleHTMLCalendar(locale=locale)\n        else:\n            cal = HTMLCalendar()\n        encoding = options.encoding\n        if encoding is None:\n            encoding = sys.getdefaultencoding()\n        optdict = dict(encoding=encoding, css=options.css)\n        if len(args) == 1:\n            print cal.formatyearpage(datetime.date.today().year, **optdict)\n        elif len(args) == 2:\n            print cal.formatyearpage(int(args[1]), **optdict)\n        else:\n            parser.error(\"incorrect number of arguments\")\n            sys.exit(1)\n    else:\n        if options.locale:\n            cal = LocaleTextCalendar(locale=locale)\n        else:\n            cal = TextCalendar()\n        optdict = dict(w=options.width, l=options.lines)\n        if len(args) != 3:\n            optdict[\"c\"] = options.spacing\n            optdict[\"m\"] = options.months\n        if len(args) == 1:\n            result = cal.formatyear(datetime.date.today().year, **optdict)\n        elif len(args) == 2:\n            result = cal.formatyear(int(args[1]), **optdict)\n        elif len(args) == 3:\n            result = cal.formatmonth(int(args[1]), int(args[2]), **optdict)\n        else:\n            parser.error(\"incorrect number of arguments\")\n            sys.exit(1)\n        if options.encoding:\n            result = result.encode(options.encoding)\n        print result\n\n\nif __name__ == \"__main__\":\n    main(sys.argv)\n",
		"file_name": "calendar.py"
	},
	{
		"content": "\"\"\" Locale support.\n\n    The module provides low-level access to the C lib's locale APIs\n    and adds high level number formatting APIs as well as a locale\n    aliasing engine to complement these.\n\n    The aliasing engine includes support for many commonly used locale\n    names and maps them to values suitable for passing to the C lib's\n    setlocale() function. It also includes default encodings for all\n    supported locale names.\n\n\"\"\"\n\nimport sys\nimport encodings\nimport encodings.aliases\nimport re\nimport operator\nimport functools\n\ntry:\n    _unicode = unicode\nexcept NameError:\n    # If Python is built without Unicode support, the unicode type\n    # will not exist. Fake one.\n    class _unicode(object):\n        pass\n\n# Try importing the _locale module.\n#\n# If this fails, fall back on a basic 'C' locale emulation.\n\n# Yuck:  LC_MESSAGES is non-standard:  can't tell whether it exists before\n# trying the import.  So __all__ is also fiddled at the end of the file.\n__all__ = [\"getlocale\", \"getdefaultlocale\", \"getpreferredencoding\", \"Error\",\n           \"setlocale\", \"resetlocale\", \"localeconv\", \"strcoll\", \"strxfrm\",\n           \"str\", \"atof\", \"atoi\", \"format\", \"format_string\", \"currency\",\n           \"normalize\", \"LC_CTYPE\", \"LC_COLLATE\", \"LC_TIME\", \"LC_MONETARY\",\n           \"LC_NUMERIC\", \"LC_ALL\", \"CHAR_MAX\"]\n\ntry:\n\n    from _locale import *\n\nexcept ImportError:\n\n    # Locale emulation\n\n    CHAR_MAX = 127\n    LC_ALL = 6\n    LC_COLLATE = 3\n    LC_CTYPE = 0\n    LC_MESSAGES = 5\n    LC_MONETARY = 4\n    LC_NUMERIC = 1\n    LC_TIME = 2\n    Error = ValueError\n\n    def localeconv():\n        \"\"\" localeconv() -> dict.\n            Returns numeric and monetary locale-specific parameters.\n        \"\"\"\n        # 'C' locale default values\n        return {'grouping': [127],\n                'currency_symbol': '',\n                'n_sign_posn': 127,\n                'p_cs_precedes': 127,\n                'n_cs_precedes': 127,\n                'mon_grouping': [],\n                'n_sep_by_space': 127,\n                'decimal_point': '.',\n                'negative_sign': '',\n                'positive_sign': '',\n                'p_sep_by_space': 127,\n                'int_curr_symbol': '',\n                'p_sign_posn': 127,\n                'thousands_sep': '',\n                'mon_thousands_sep': '',\n                'frac_digits': 127,\n                'mon_decimal_point': '',\n                'int_frac_digits': 127}\n\n    def setlocale(category, value=None):\n        \"\"\" setlocale(integer,string=None) -> string.\n            Activates/queries locale processing.\n        \"\"\"\n        if value not in (None, '', 'C'):\n            raise Error, '_locale emulation only supports \"C\" locale'\n        return 'C'\n\n    def strcoll(a,b):\n        \"\"\" strcoll(string,string) -> int.\n            Compares two strings according to the locale.\n        \"\"\"\n        return cmp(a,b)\n\n    def strxfrm(s):\n        \"\"\" strxfrm(string) -> string.\n            Returns a string that behaves for cmp locale-aware.\n        \"\"\"\n        return s\n\n\n_localeconv = localeconv\n\n# With this dict, you can override some items of localeconv's return value.\n# This is useful for testing purposes.\n_override_localeconv = {}\n\n@functools.wraps(_localeconv)\ndef localeconv():\n    d = _localeconv()\n    if _override_localeconv:\n        d.update(_override_localeconv)\n    return d\n\n\n### Number formatting APIs\n\n# Author: Martin von Loewis\n# improved by Georg Brandl\n\n# Iterate over grouping intervals\ndef _grouping_intervals(grouping):\n    last_interval = None\n    for interval in grouping:\n        # if grouping is -1, we are done\n        if interval == CHAR_MAX:\n            return\n        # 0: re-use last group ad infinitum\n        if interval == 0:\n            if last_interval is None:\n                raise ValueError(\"invalid grouping\")\n            while True:\n                yield last_interval\n        yield interval\n        last_interval = interval\n\n#perform the grouping from right to left\ndef _group(s, monetary=False):\n    conv = localeconv()\n    thousands_sep = conv[monetary and 'mon_thousands_sep' or 'thousands_sep']\n    grouping = conv[monetary and 'mon_grouping' or 'grouping']\n    if not grouping:\n        return (s, 0)\n    if s[-1] == ' ':\n        stripped = s.rstrip()\n        right_spaces = s[len(stripped):]\n        s = stripped\n    else:\n        right_spaces = ''\n    left_spaces = ''\n    groups = []\n    for interval in _grouping_intervals(grouping):\n        if not s or s[-1] not in \"0123456789\":\n            # only non-digit characters remain (sign, spaces)\n            left_spaces = s\n            s = ''\n            break\n        groups.append(s[-interval:])\n        s = s[:-interval]\n    if s:\n        groups.append(s)\n    groups.reverse()\n    return (\n        left_spaces + thousands_sep.join(groups) + right_spaces,\n        len(thousands_sep) * (len(groups) - 1)\n    )\n\n# Strip a given amount of excess padding from the given string\ndef _strip_padding(s, amount):\n    lpos = 0\n    while amount and s[lpos] == ' ':\n        lpos += 1\n        amount -= 1\n    rpos = len(s) - 1\n    while amount and s[rpos] == ' ':\n        rpos -= 1\n        amount -= 1\n    return s[lpos:rpos+1]\n\n_percent_re = re.compile(r'%(?:\\((?P<key>.*?)\\))?'\n                         r'(?P<modifiers>[-#0-9 +*.hlL]*?)[eEfFgGdiouxXcrs%]')\n\ndef format(percent, value, grouping=False, monetary=False, *additional):\n    \"\"\"Returns the locale-aware substitution of a %? specifier\n    (percent).\n\n    additional is for format strings which contain one or more\n    '*' modifiers.\"\"\"\n    # this is only for one-percent-specifier strings and this should be checked\n    match = _percent_re.match(percent)\n    if not match or len(match.group())!= len(percent):\n        raise ValueError((\"format() must be given exactly one %%char \"\n                         \"format specifier, %s not valid\") % repr(percent))\n    return _format(percent, value, grouping, monetary, *additional)\n\ndef _format(percent, value, grouping=False, monetary=False, *additional):\n    if additional:\n        formatted = percent % ((value,) + additional)\n    else:\n        formatted = percent % value\n    # floats and decimal ints need special action!\n    if percent[-1] in 'eEfFgG':\n        seps = 0\n        parts = formatted.split('.')\n        if grouping:\n            parts[0], seps = _group(parts[0], monetary=monetary)\n        decimal_point = localeconv()[monetary and 'mon_decimal_point'\n                                              or 'decimal_point']\n        formatted = decimal_point.join(parts)\n        if seps:\n            formatted = _strip_padding(formatted, seps)\n    elif percent[-1] in 'diu':\n        seps = 0\n        if grouping:\n            formatted, seps = _group(formatted, monetary=monetary)\n        if seps:\n            formatted = _strip_padding(formatted, seps)\n    return formatted\n\ndef format_string(f, val, grouping=False):\n    \"\"\"Formats a string in the same way that the % formatting would use,\n    but takes the current locale into account.\n    Grouping is applied if the third parameter is true.\"\"\"\n    percents = list(_percent_re.finditer(f))\n    new_f = _percent_re.sub('%s', f)\n\n    if operator.isMappingType(val):\n        new_val = []\n        for perc in percents:\n            if perc.group()[-1]=='%':\n                new_val.append('%')\n            else:\n                new_val.append(format(perc.group(), val, grouping))\n    else:\n        if not isinstance(val, tuple):\n            val = (val,)\n        new_val = []\n        i = 0\n        for perc in percents:\n            if perc.group()[-1]=='%':\n                new_val.append('%')\n            else:\n                starcount = perc.group('modifiers').count('*')\n                new_val.append(_format(perc.group(),\n                                      val[i],\n                                      grouping,\n                                      False,\n                                      *val[i+1:i+1+starcount]))\n                i += (1 + starcount)\n    val = tuple(new_val)\n\n    return new_f % val\n\ndef currency(val, symbol=True, grouping=False, international=False):\n    \"\"\"Formats val according to the currency settings\n    in the current locale.\"\"\"\n    conv = localeconv()\n\n    # check for illegal values\n    digits = conv[international and 'int_frac_digits' or 'frac_digits']\n    if digits == 127:\n        raise ValueError(\"Currency formatting is not possible using \"\n                         \"the 'C' locale.\")\n\n    s = format('%%.%if' % digits, abs(val), grouping, monetary=True)\n    # '<' and '>' are markers if the sign must be inserted between symbol and value\n    s = '<' + s + '>'\n\n    if symbol:\n        smb = conv[international and 'int_curr_symbol' or 'currency_symbol']\n        precedes = conv[val<0 and 'n_cs_precedes' or 'p_cs_precedes']\n        separated = conv[val<0 and 'n_sep_by_space' or 'p_sep_by_space']\n\n        if precedes:\n            s = smb + (separated and ' ' or '') + s\n        else:\n            s = s + (separated and ' ' or '') + smb\n\n    sign_pos = conv[val<0 and 'n_sign_posn' or 'p_sign_posn']\n    sign = conv[val<0 and 'negative_sign' or 'positive_sign']\n\n    if sign_pos == 0:\n        s = '(' + s + ')'\n    elif sign_pos == 1:\n        s = sign + s\n    elif sign_pos == 2:\n        s = s + sign\n    elif sign_pos == 3:\n        s = s.replace('<', sign)\n    elif sign_pos == 4:\n        s = s.replace('>', sign)\n    else:\n        # the default if nothing specified;\n        # this should be the most fitting sign position\n        s = sign + s\n\n    return s.replace('<', '').replace('>', '')\n\ndef str(val):\n    \"\"\"Convert float to integer, taking the locale into account.\"\"\"\n    return format(\"%.12g\", val)\n\ndef atof(string, func=float):\n    \"Parses a string as a float according to the locale settings.\"\n    #First, get rid of the grouping\n    ts = localeconv()['thousands_sep']\n    if ts:\n        string = string.replace(ts, '')\n    #next, replace the decimal point with a dot\n    dd = localeconv()['decimal_point']\n    if dd:\n        string = string.replace(dd, '.')\n    #finally, parse the string\n    return func(string)\n\ndef atoi(str):\n    \"Converts a string to an integer according to the locale settings.\"\n    return atof(str, int)\n\ndef _test():\n    setlocale(LC_ALL, \"\")\n    #do grouping\n    s1 = format(\"%d\", 123456789,1)\n    print s1, \"is\", atoi(s1)\n    #standard formatting\n    s1 = str(3.14)\n    print s1, \"is\", atof(s1)\n\n### Locale name aliasing engine\n\n# Author: Marc-Andre Lemburg, mal@lemburg.com\n# Various tweaks by Fredrik Lundh <fredrik@pythonware.com>\n\n# store away the low-level version of setlocale (it's\n# overridden below)\n_setlocale = setlocale\n\n# Avoid relying on the locale-dependent .lower() method\n# (see issue #1813).\n_ascii_lower_map = ''.join(\n    chr(x + 32 if x >= ord('A') and x <= ord('Z') else x)\n    for x in range(256)\n)\n\ndef _replace_encoding(code, encoding):\n    if '.' in code:\n        langname = code[:code.index('.')]\n    else:\n        langname = code\n    # Convert the encoding to a C lib compatible encoding string\n    norm_encoding = encodings.normalize_encoding(encoding)\n    #print('norm encoding: %r' % norm_encoding)\n    norm_encoding = encodings.aliases.aliases.get(norm_encoding,\n                                                  norm_encoding)\n    #print('aliased encoding: %r' % norm_encoding)\n    encoding = locale_encoding_alias.get(norm_encoding,\n                                         norm_encoding)\n    #print('found encoding %r' % encoding)\n    return langname + '.' + encoding\n\ndef normalize(localename):\n\n    \"\"\" Returns a normalized locale code for the given locale\n        name.\n\n        The returned locale code is formatted for use with\n        setlocale().\n\n        If normalization fails, the original name is returned\n        unchanged.\n\n        If the given encoding is not known, the function defaults to\n        the default encoding for the locale code just like setlocale()\n        does.\n\n    \"\"\"\n    # Normalize the locale name and extract the encoding and modifier\n    if isinstance(localename, _unicode):\n        localename = localename.encode('ascii')\n    code = localename.translate(_ascii_lower_map)\n    if ':' in code:\n        # ':' is sometimes used as encoding delimiter.\n        code = code.replace(':', '.')\n    if '@' in code:\n        code, modifier = code.split('@', 1)\n    else:\n        modifier = ''\n    if '.' in code:\n        langname, encoding = code.split('.')[:2]\n    else:\n        langname = code\n        encoding = ''\n\n    # First lookup: fullname (possibly with encoding and modifier)\n    lang_enc = langname\n    if encoding:\n        norm_encoding = encoding.replace('-', '')\n        norm_encoding = norm_encoding.replace('_', '')\n        lang_enc += '.' + norm_encoding\n    lookup_name = lang_enc\n    if modifier:\n        lookup_name += '@' + modifier\n    code = locale_alias.get(lookup_name, None)\n    if code is not None:\n        return code\n    #print('first lookup failed')\n\n    if modifier:\n        # Second try: fullname without modifier (possibly with encoding)\n        code = locale_alias.get(lang_enc, None)\n        if code is not None:\n            #print('lookup without modifier succeeded')\n            if '@' not in code:\n                return code + '@' + modifier\n            if code.split('@', 1)[1].translate(_ascii_lower_map) == modifier:\n                return code\n        #print('second lookup failed')\n\n    if encoding:\n        # Third try: langname (without encoding, possibly with modifier)\n        lookup_name = langname\n        if modifier:\n            lookup_name += '@' + modifier\n        code = locale_alias.get(lookup_name, None)\n        if code is not None:\n            #print('lookup without encoding succeeded')\n            if '@' not in code:\n                return _replace_encoding(code, encoding)\n            code, modifier = code.split('@', 1)\n            return _replace_encoding(code, encoding) + '@' + modifier\n\n        if modifier:\n            # Fourth try: langname (without encoding and modifier)\n            code = locale_alias.get(langname, None)\n            if code is not None:\n                #print('lookup without modifier and encoding succeeded')\n                if '@' not in code:\n                    return _replace_encoding(code, encoding) + '@' + modifier\n                code, defmod = code.split('@', 1)\n                if defmod.translate(_ascii_lower_map) == modifier:\n                    return _replace_encoding(code, encoding) + '@' + defmod\n\n    return localename\n\ndef _parse_localename(localename):\n\n    \"\"\" Parses the locale code for localename and returns the\n        result as tuple (language code, encoding).\n\n        The localename is normalized and passed through the locale\n        alias engine. A ValueError is raised in case the locale name\n        cannot be parsed.\n\n        The language code corresponds to RFC 1766.  code and encoding\n        can be None in case the values cannot be determined or are\n        unknown to this implementation.\n\n    \"\"\"\n    code = normalize(localename)\n    if '@' in code:\n        # Deal with locale modifiers\n        code, modifier = code.split('@', 1)\n        if modifier == 'euro' and '.' not in code:\n            # Assume Latin-9 for @euro locales. This is bogus,\n            # since some systems may use other encodings for these\n            # locales. Also, we ignore other modifiers.\n            return code, 'iso-8859-15'\n\n    if '.' in code:\n        return tuple(code.split('.')[:2])\n    elif code == 'C':\n        return None, None\n    raise ValueError, 'unknown locale: %s' % localename\n\ndef _build_localename(localetuple):\n\n    \"\"\" Builds a locale code from the given tuple (language code,\n        encoding).\n\n        No aliasing or normalizing takes place.\n\n    \"\"\"\n    language, encoding = localetuple\n    if language is None:\n        language = 'C'\n    if encoding is None:\n        return language\n    else:\n        return language + '.' + encoding\n\ndef getdefaultlocale(envvars=('LC_ALL', 'LC_CTYPE', 'LANG', 'LANGUAGE')):\n\n    \"\"\" Tries to determine the default locale settings and returns\n        them as tuple (language code, encoding).\n\n        According to POSIX, a program which has not called\n        setlocale(LC_ALL, \"\") runs using the portable 'C' locale.\n        Calling setlocale(LC_ALL, \"\") lets it use the default locale as\n        defined by the LANG variable. Since we don't want to interfere\n        with the current locale setting we thus emulate the behavior\n        in the way described above.\n\n        To maintain compatibility with other platforms, not only the\n        LANG variable is tested, but a list of variables given as\n        envvars parameter. The first found to be defined will be\n        used. envvars defaults to the search path used in GNU gettext;\n        it must always contain the variable name 'LANG'.\n\n        Except for the code 'C', the language code corresponds to RFC\n        1766.  code and encoding can be None in case the values cannot\n        be determined.\n\n    \"\"\"\n\n    try:\n        # check if it's supported by the _locale module\n        import _locale\n        code, encoding = _locale._getdefaultlocale()\n    except (ImportError, AttributeError):\n        pass\n    else:\n        # make sure the code/encoding values are valid\n        if sys.platform == \"win32\" and code and code[:2] == \"0x\":\n            # map windows language identifier to language name\n            code = windows_locale.get(int(code, 0))\n        # ...add other platform-specific processing here, if\n        # necessary...\n        return code, encoding\n\n    # fall back on POSIX behaviour\n    import os\n    lookup = os.environ.get\n    for variable in envvars:\n        localename = lookup(variable,None)\n        if localename:\n            if variable == 'LANGUAGE':\n                localename = localename.split(':')[0]\n            break\n    else:\n        localename = 'C'\n    return _parse_localename(localename)\n\n\ndef getlocale(category=LC_CTYPE):\n\n    \"\"\" Returns the current setting for the given locale category as\n        tuple (language code, encoding).\n\n        category may be one of the LC_* value except LC_ALL. It\n        defaults to LC_CTYPE.\n\n        Except for the code 'C', the language code corresponds to RFC\n        1766.  code and encoding can be None in case the values cannot\n        be determined.\n\n    \"\"\"\n    localename = _setlocale(category)\n    if category == LC_ALL and ';' in localename:\n        raise TypeError, 'category LC_ALL is not supported'\n    return _parse_localename(localename)\n\ndef setlocale(category, locale=None):\n\n    \"\"\" Set the locale for the given category.  The locale can be\n        a string, an iterable of two strings (language code and encoding),\n        or None.\n\n        Iterables are converted to strings using the locale aliasing\n        engine.  Locale strings are passed directly to the C lib.\n\n        category may be given as one of the LC_* values.\n\n    \"\"\"\n    if locale and type(locale) is not type(\"\"):\n        # convert to string\n        locale = normalize(_build_localename(locale))\n    return _setlocale(category, locale)\n\ndef resetlocale(category=LC_ALL):\n\n    \"\"\" Sets the locale for category to the default setting.\n\n        The default setting is determined by calling\n        getdefaultlocale(). category defaults to LC_ALL.\n\n    \"\"\"\n    _setlocale(category, _build_localename(getdefaultlocale()))\n\nif sys.platform.startswith(\"win\"):\n    # On Win32, this will return the ANSI code page\n    def getpreferredencoding(do_setlocale = True):\n        \"\"\"Return the charset that the user is likely using.\"\"\"\n        import _locale\n        return _locale._getdefaultlocale()[1]\nelse:\n    # On Unix, if CODESET is available, use that.\n    try:\n        CODESET\n    except NameError:\n        # Fall back to parsing environment variables :-(\n        def getpreferredencoding(do_setlocale = True):\n            \"\"\"Return the charset that the user is likely using,\n            by looking at environment variables.\"\"\"\n            return getdefaultlocale()[1]\n    else:\n        def getpreferredencoding(do_setlocale = True):\n            \"\"\"Return the charset that the user is likely using,\n            according to the system configuration.\"\"\"\n            if do_setlocale:\n                oldloc = setlocale(LC_CTYPE)\n                try:\n                    setlocale(LC_CTYPE, \"\")\n                except Error:\n                    pass\n                result = nl_langinfo(CODESET)\n                setlocale(LC_CTYPE, oldloc)\n                return result\n            else:\n                return nl_langinfo(CODESET)\n\n\n### Database\n#\n# The following data was extracted from the locale.alias file which\n# comes with X11 and then hand edited removing the explicit encoding\n# definitions and adding some more aliases. The file is usually\n# available as /usr/lib/X11/locale/locale.alias.\n#\n\n#\n# The local_encoding_alias table maps lowercase encoding alias names\n# to C locale encoding names (case-sensitive). Note that normalize()\n# first looks up the encoding in the encodings.aliases dictionary and\n# then applies this mapping to find the correct C lib name for the\n# encoding.\n#\nlocale_encoding_alias = {\n\n    # Mappings for non-standard encoding names used in locale names\n    '437':                          'C',\n    'c':                            'C',\n    'en':                           'ISO8859-1',\n    'jis':                          'JIS7',\n    'jis7':                         'JIS7',\n    'ajec':                         'eucJP',\n\n    # Mappings from Python codec names to C lib encoding names\n    'ascii':                        'ISO8859-1',\n    'latin_1':                      'ISO8859-1',\n    'iso8859_1':                    'ISO8859-1',\n    'iso8859_10':                   'ISO8859-10',\n    'iso8859_11':                   'ISO8859-11',\n    'iso8859_13':                   'ISO8859-13',\n    'iso8859_14':                   'ISO8859-14',\n    'iso8859_15':                   'ISO8859-15',\n    'iso8859_16':                   'ISO8859-16',\n    'iso8859_2':                    'ISO8859-2',\n    'iso8859_3':                    'ISO8859-3',\n    'iso8859_4':                    'ISO8859-4',\n    'iso8859_5':                    'ISO8859-5',\n    'iso8859_6':                    'ISO8859-6',\n    'iso8859_7':                    'ISO8859-7',\n    'iso8859_8':                    'ISO8859-8',\n    'iso8859_9':                    'ISO8859-9',\n    'iso2022_jp':                   'JIS7',\n    'shift_jis':                    'SJIS',\n    'tactis':                       'TACTIS',\n    'euc_jp':                       'eucJP',\n    'euc_kr':                       'eucKR',\n    'utf_8':                        'UTF-8',\n    'koi8_r':                       'KOI8-R',\n    'koi8_u':                       'KOI8-U',\n    # XXX This list is still incomplete. If you know more\n    # mappings, please file a bug report. Thanks.\n}\n\n#\n# The locale_alias table maps lowercase alias names to C locale names\n# (case-sensitive). Encodings are always separated from the locale\n# name using a dot ('.'); they should only be given in case the\n# language name is needed to interpret the given encoding alias\n# correctly (CJK codes often have this need).\n#\n# Note that the normalize() function which uses this tables\n# removes '_' and '-' characters from the encoding part of the\n# locale name before doing the lookup. This saves a lot of\n# space in the table.\n#\n# MAL 2004-12-10:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.4\n# and older):\n#\n#    updated 'bg' -> 'bg_BG.ISO8859-5' to 'bg_BG.CP1251'\n#    updated 'bg_bg' -> 'bg_BG.ISO8859-5' to 'bg_BG.CP1251'\n#    updated 'bulgarian' -> 'bg_BG.ISO8859-5' to 'bg_BG.CP1251'\n#    updated 'cz' -> 'cz_CZ.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'cz_cz' -> 'cz_CZ.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'czech' -> 'cs_CS.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'dutch' -> 'nl_BE.ISO8859-1' to 'nl_NL.ISO8859-1'\n#    updated 'et' -> 'et_EE.ISO8859-4' to 'et_EE.ISO8859-15'\n#    updated 'et_ee' -> 'et_EE.ISO8859-4' to 'et_EE.ISO8859-15'\n#    updated 'fi' -> 'fi_FI.ISO8859-1' to 'fi_FI.ISO8859-15'\n#    updated 'fi_fi' -> 'fi_FI.ISO8859-1' to 'fi_FI.ISO8859-15'\n#    updated 'iw' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'iw_il' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'japanese' -> 'ja_JP.SJIS' to 'ja_JP.eucJP'\n#    updated 'lt' -> 'lt_LT.ISO8859-4' to 'lt_LT.ISO8859-13'\n#    updated 'lv' -> 'lv_LV.ISO8859-4' to 'lv_LV.ISO8859-13'\n#    updated 'sl' -> 'sl_CS.ISO8859-2' to 'sl_SI.ISO8859-2'\n#    updated 'slovene' -> 'sl_CS.ISO8859-2' to 'sl_SI.ISO8859-2'\n#    updated 'th_th' -> 'th_TH.TACTIS' to 'th_TH.ISO8859-11'\n#    updated 'zh_cn' -> 'zh_CN.eucCN' to 'zh_CN.gb2312'\n#    updated 'zh_cn.big5' -> 'zh_TW.eucTW' to 'zh_TW.big5'\n#    updated 'zh_tw' -> 'zh_TW.eucTW' to 'zh_TW.big5'\n#\n# MAL 2008-05-30:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.5\n# and older):\n#\n#    updated 'cs_cs.iso88592' -> 'cs_CZ.ISO8859-2' to 'cs_CS.ISO8859-2'\n#    updated 'serbocroatian' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sh' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sh_hr.iso88592' -> 'sh_HR.ISO8859-2' to 'hr_HR.ISO8859-2'\n#    updated 'sh_sp' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sh_yu' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sp' -> 'sp_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sp_yu' -> 'sp_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr@cyrillic' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_sp' -> 'sr_SP.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sr_yu' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_yu.cp1251@cyrillic' -> 'sr_YU.CP1251' to 'sr_CS.CP1251'\n#    updated 'sr_yu.iso88592' -> 'sr_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sr_yu.iso88595' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_yu.iso88595@cyrillic' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_yu.microsoftcp1251@cyrillic' -> 'sr_YU.CP1251' to 'sr_CS.CP1251'\n#    updated 'sr_yu.utf8@cyrillic' -> 'sr_YU.UTF-8' to 'sr_CS.UTF-8'\n#    updated 'sr_yu@cyrillic' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#\n# AP 2010-04-12:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.6.5\n# and older):\n#\n#    updated 'ru' -> 'ru_RU.ISO8859-5' to 'ru_RU.UTF-8'\n#    updated 'ru_ru' -> 'ru_RU.ISO8859-5' to 'ru_RU.UTF-8'\n#    updated 'serbocroatian' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sh' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sh_yu' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sr' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8'\n#    updated 'sr@cyrillic' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8'\n#    updated 'sr@latn' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_cs.utf8@latn' -> 'sr_CS.UTF-8' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_cs@latn' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_yu' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_yu.utf8@cyrillic' -> 'sr_CS.UTF-8' to 'sr_RS.UTF-8'\n#    updated 'sr_yu@cyrillic' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8'\n#\n# SS 2013-12-20:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.7.6\n# and older):\n#\n#    updated 'a3' -> 'a3_AZ.KOI8-C' to 'az_AZ.KOI8-C'\n#    updated 'a3_az' -> 'a3_AZ.KOI8-C' to 'az_AZ.KOI8-C'\n#    updated 'a3_az.koi8c' -> 'a3_AZ.KOI8-C' to 'az_AZ.KOI8-C'\n#    updated 'cs_cs.iso88592' -> 'cs_CS.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'hebrew' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'hebrew.iso88598' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'sd' -> 'sd_IN@devanagari.UTF-8' to 'sd_IN.UTF-8'\n#    updated 'sr@latn' -> 'sr_RS.UTF-8@latin' to 'sr_CS.UTF-8@latin'\n#    updated 'sr_cs' -> 'sr_RS.UTF-8' to 'sr_CS.UTF-8'\n#    updated 'sr_cs.utf8@latn' -> 'sr_RS.UTF-8@latin' to 'sr_CS.UTF-8@latin'\n#    updated 'sr_cs@latn' -> 'sr_RS.UTF-8@latin' to 'sr_CS.UTF-8@latin'\n\nlocale_alias = {\n    'a3':                                   'az_AZ.KOI8-C',\n    'a3_az':                                'az_AZ.KOI8-C',\n    'a3_az.koi8c':                          'az_AZ.KOI8-C',\n    'a3_az.koic':                           'az_AZ.KOI8-C',\n    'af':                                   'af_ZA.ISO8859-1',\n    'af_za':                                'af_ZA.ISO8859-1',\n    'af_za.iso88591':                       'af_ZA.ISO8859-1',\n    'am':                                   'am_ET.UTF-8',\n    'am_et':                                'am_ET.UTF-8',\n    'american':                             'en_US.ISO8859-1',\n    'american.iso88591':                    'en_US.ISO8859-1',\n    'ar':                                   'ar_AA.ISO8859-6',\n    'ar_aa':                                'ar_AA.ISO8859-6',\n    'ar_aa.iso88596':                       'ar_AA.ISO8859-6',\n    'ar_ae':                                'ar_AE.ISO8859-6',\n    'ar_ae.iso88596':                       'ar_AE.ISO8859-6',\n    'ar_bh':                                'ar_BH.ISO8859-6',\n    'ar_bh.iso88596':                       'ar_BH.ISO8859-6',\n    'ar_dz':                                'ar_DZ.ISO8859-6',\n    'ar_dz.iso88596':                       'ar_DZ.ISO8859-6',\n    'ar_eg':                                'ar_EG.ISO8859-6',\n    'ar_eg.iso88596':                       'ar_EG.ISO8859-6',\n    'ar_in':                                'ar_IN.UTF-8',\n    'ar_iq':                                'ar_IQ.ISO8859-6',\n    'ar_iq.iso88596':                       'ar_IQ.ISO8859-6',\n    'ar_jo':                                'ar_JO.ISO8859-6',\n    'ar_jo.iso88596':                       'ar_JO.ISO8859-6',\n    'ar_kw':                                'ar_KW.ISO8859-6',\n    'ar_kw.iso88596':                       'ar_KW.ISO8859-6',\n    'ar_lb':                                'ar_LB.ISO8859-6',\n    'ar_lb.iso88596':                       'ar_LB.ISO8859-6',\n    'ar_ly':                                'ar_LY.ISO8859-6',\n    'ar_ly.iso88596':                       'ar_LY.ISO8859-6',\n    'ar_ma':                                'ar_MA.ISO8859-6',\n    'ar_ma.iso88596':                       'ar_MA.ISO8859-6',\n    'ar_om':                                'ar_OM.ISO8859-6',\n    'ar_om.iso88596':                       'ar_OM.ISO8859-6',\n    'ar_qa':                                'ar_QA.ISO8859-6',\n    'ar_qa.iso88596':                       'ar_QA.ISO8859-6',\n    'ar_sa':                                'ar_SA.ISO8859-6',\n    'ar_sa.iso88596':                       'ar_SA.ISO8859-6',\n    'ar_sd':                                'ar_SD.ISO8859-6',\n    'ar_sd.iso88596':                       'ar_SD.ISO8859-6',\n    'ar_sy':                                'ar_SY.ISO8859-6',\n    'ar_sy.iso88596':                       'ar_SY.ISO8859-6',\n    'ar_tn':                                'ar_TN.ISO8859-6',\n    'ar_tn.iso88596':                       'ar_TN.ISO8859-6',\n    'ar_ye':                                'ar_YE.ISO8859-6',\n    'ar_ye.iso88596':                       'ar_YE.ISO8859-6',\n    'arabic':                               'ar_AA.ISO8859-6',\n    'arabic.iso88596':                      'ar_AA.ISO8859-6',\n    'as':                                   'as_IN.UTF-8',\n    'as_in':                                'as_IN.UTF-8',\n    'az':                                   'az_AZ.ISO8859-9E',\n    'az_az':                                'az_AZ.ISO8859-9E',\n    'az_az.iso88599e':                      'az_AZ.ISO8859-9E',\n    'be':                                   'be_BY.CP1251',\n    'be@latin':                             'be_BY.UTF-8@latin',\n    'be_by':                                'be_BY.CP1251',\n    'be_by.cp1251':                         'be_BY.CP1251',\n    'be_by.microsoftcp1251':                'be_BY.CP1251',\n    'be_by.utf8@latin':                     'be_BY.UTF-8@latin',\n    'be_by@latin':                          'be_BY.UTF-8@latin',\n    'bg':                                   'bg_BG.CP1251',\n    'bg_bg':                                'bg_BG.CP1251',\n    'bg_bg.cp1251':                         'bg_BG.CP1251',\n    'bg_bg.iso88595':                       'bg_BG.ISO8859-5',\n    'bg_bg.koi8r':                          'bg_BG.KOI8-R',\n    'bg_bg.microsoftcp1251':                'bg_BG.CP1251',\n    'bn_in':                                'bn_IN.UTF-8',\n    'bo_in':                                'bo_IN.UTF-8',\n    'bokmal':                               'nb_NO.ISO8859-1',\n    'bokm\\xe5l':                            'nb_NO.ISO8859-1',\n    'br':                                   'br_FR.ISO8859-1',\n    'br_fr':                                'br_FR.ISO8859-1',\n    'br_fr.iso88591':                       'br_FR.ISO8859-1',\n    'br_fr.iso885914':                      'br_FR.ISO8859-14',\n    'br_fr.iso885915':                      'br_FR.ISO8859-15',\n    'br_fr.iso885915@euro':                 'br_FR.ISO8859-15',\n    'br_fr.utf8@euro':                      'br_FR.UTF-8',\n    'br_fr@euro':                           'br_FR.ISO8859-15',\n    'bs':                                   'bs_BA.ISO8859-2',\n    'bs_ba':                                'bs_BA.ISO8859-2',\n    'bs_ba.iso88592':                       'bs_BA.ISO8859-2',\n    'bulgarian':                            'bg_BG.CP1251',\n    'c':                                    'C',\n    'c-french':                             'fr_CA.ISO8859-1',\n    'c-french.iso88591':                    'fr_CA.ISO8859-1',\n    'c.ascii':                              'C',\n    'c.en':                                 'C',\n    'c.iso88591':                           'en_US.ISO8859-1',\n    'c_c':                                  'C',\n    'c_c.c':                                'C',\n    'ca':                                   'ca_ES.ISO8859-1',\n    'ca_ad':                                'ca_AD.ISO8859-1',\n    'ca_ad.iso88591':                       'ca_AD.ISO8859-1',\n    'ca_ad.iso885915':                      'ca_AD.ISO8859-15',\n    'ca_ad.iso885915@euro':                 'ca_AD.ISO8859-15',\n    'ca_ad.utf8@euro':                      'ca_AD.UTF-8',\n    'ca_ad@euro':                           'ca_AD.ISO8859-15',\n    'ca_es':                                'ca_ES.ISO8859-1',\n    'ca_es.iso88591':                       'ca_ES.ISO8859-1',\n    'ca_es.iso885915':                      'ca_ES.ISO8859-15',\n    'ca_es.iso885915@euro':                 'ca_ES.ISO8859-15',\n    'ca_es.utf8@euro':                      'ca_ES.UTF-8',\n    'ca_es@euro':                           'ca_ES.ISO8859-15',\n    'ca_fr':                                'ca_FR.ISO8859-1',\n    'ca_fr.iso88591':                       'ca_FR.ISO8859-1',\n    'ca_fr.iso885915':                      'ca_FR.ISO8859-15',\n    'ca_fr.iso885915@euro':                 'ca_FR.ISO8859-15',\n    'ca_fr.utf8@euro':                      'ca_FR.UTF-8',\n    'ca_fr@euro':                           'ca_FR.ISO8859-15',\n    'ca_it':                                'ca_IT.ISO8859-1',\n    'ca_it.iso88591':                       'ca_IT.ISO8859-1',\n    'ca_it.iso885915':                      'ca_IT.ISO8859-15',\n    'ca_it.iso885915@euro':                 'ca_IT.ISO8859-15',\n    'ca_it.utf8@euro':                      'ca_IT.UTF-8',\n    'ca_it@euro':                           'ca_IT.ISO8859-15',\n    'catalan':                              'ca_ES.ISO8859-1',\n    'cextend':                              'en_US.ISO8859-1',\n    'cextend.en':                           'en_US.ISO8859-1',\n    'chinese-s':                            'zh_CN.eucCN',\n    'chinese-t':                            'zh_TW.eucTW',\n    'croatian':                             'hr_HR.ISO8859-2',\n    'cs':                                   'cs_CZ.ISO8859-2',\n    'cs_cs':                                'cs_CZ.ISO8859-2',\n    'cs_cs.iso88592':                       'cs_CZ.ISO8859-2',\n    'cs_cz':                                'cs_CZ.ISO8859-2',\n    'cs_cz.iso88592':                       'cs_CZ.ISO8859-2',\n    'cy':                                   'cy_GB.ISO8859-1',\n    'cy_gb':                                'cy_GB.ISO8859-1',\n    'cy_gb.iso88591':                       'cy_GB.ISO8859-1',\n    'cy_gb.iso885914':                      'cy_GB.ISO8859-14',\n    'cy_gb.iso885915':                      'cy_GB.ISO8859-15',\n    'cy_gb@euro':                           'cy_GB.ISO8859-15',\n    'cz':                                   'cs_CZ.ISO8859-2',\n    'cz_cz':                                'cs_CZ.ISO8859-2',\n    'czech':                                'cs_CZ.ISO8859-2',\n    'da':                                   'da_DK.ISO8859-1',\n    'da.iso885915':                         'da_DK.ISO8859-15',\n    'da_dk':                                'da_DK.ISO8859-1',\n    'da_dk.88591':                          'da_DK.ISO8859-1',\n    'da_dk.885915':                         'da_DK.ISO8859-15',\n    'da_dk.iso88591':                       'da_DK.ISO8859-1',\n    'da_dk.iso885915':                      'da_DK.ISO8859-15',\n    'da_dk@euro':                           'da_DK.ISO8859-15',\n    'danish':                               'da_DK.ISO8859-1',\n    'danish.iso88591':                      'da_DK.ISO8859-1',\n    'dansk':                                'da_DK.ISO8859-1',\n    'de':                                   'de_DE.ISO8859-1',\n    'de.iso885915':                         'de_DE.ISO8859-15',\n    'de_at':                                'de_AT.ISO8859-1',\n    'de_at.iso88591':                       'de_AT.ISO8859-1',\n    'de_at.iso885915':                      'de_AT.ISO8859-15',\n    'de_at.iso885915@euro':                 'de_AT.ISO8859-15',\n    'de_at.utf8@euro':                      'de_AT.UTF-8',\n    'de_at@euro':                           'de_AT.ISO8859-15',\n    'de_be':                                'de_BE.ISO8859-1',\n    'de_be.iso88591':                       'de_BE.ISO8859-1',\n    'de_be.iso885915':                      'de_BE.ISO8859-15',\n    'de_be.iso885915@euro':                 'de_BE.ISO8859-15',\n    'de_be.utf8@euro':                      'de_BE.UTF-8',\n    'de_be@euro':                           'de_BE.ISO8859-15',\n    'de_ch':                                'de_CH.ISO8859-1',\n    'de_ch.iso88591':                       'de_CH.ISO8859-1',\n    'de_ch.iso885915':                      'de_CH.ISO8859-15',\n    'de_ch@euro':                           'de_CH.ISO8859-15',\n    'de_de':                                'de_DE.ISO8859-1',\n    'de_de.88591':                          'de_DE.ISO8859-1',\n    'de_de.885915':                         'de_DE.ISO8859-15',\n    'de_de.885915@euro':                    'de_DE.ISO8859-15',\n    'de_de.iso88591':                       'de_DE.ISO8859-1',\n    'de_de.iso885915':                      'de_DE.ISO8859-15',\n    'de_de.iso885915@euro':                 'de_DE.ISO8859-15',\n    'de_de.utf8@euro':                      'de_DE.UTF-8',\n    'de_de@euro':                           'de_DE.ISO8859-15',\n    'de_lu':                                'de_LU.ISO8859-1',\n    'de_lu.iso88591':                       'de_LU.ISO8859-1',\n    'de_lu.iso885915':                      'de_LU.ISO8859-15',\n    'de_lu.iso885915@euro':                 'de_LU.ISO8859-15',\n    'de_lu.utf8@euro':                      'de_LU.UTF-8',\n    'de_lu@euro':                           'de_LU.ISO8859-15',\n    'deutsch':                              'de_DE.ISO8859-1',\n    'dutch':                                'nl_NL.ISO8859-1',\n    'dutch.iso88591':                       'nl_BE.ISO8859-1',\n    'ee':                                   'ee_EE.ISO8859-4',\n    'ee_ee':                                'ee_EE.ISO8859-4',\n    'ee_ee.iso88594':                       'ee_EE.ISO8859-4',\n    'eesti':                                'et_EE.ISO8859-1',\n    'el':                                   'el_GR.ISO8859-7',\n    'el_gr':                                'el_GR.ISO8859-7',\n    'el_gr.iso88597':                       'el_GR.ISO8859-7',\n    'el_gr@euro':                           'el_GR.ISO8859-15',\n    'en':                                   'en_US.ISO8859-1',\n    'en.iso88591':                          'en_US.ISO8859-1',\n    'en_au':                                'en_AU.ISO8859-1',\n    'en_au.iso88591':                       'en_AU.ISO8859-1',\n    'en_be':                                'en_BE.ISO8859-1',\n    'en_be@euro':                           'en_BE.ISO8859-15',\n    'en_bw':                                'en_BW.ISO8859-1',\n    'en_bw.iso88591':                       'en_BW.ISO8859-1',\n    'en_ca':                                'en_CA.ISO8859-1',\n    'en_ca.iso88591':                       'en_CA.ISO8859-1',\n    'en_gb':                                'en_GB.ISO8859-1',\n    'en_gb.88591':                          'en_GB.ISO8859-1',\n    'en_gb.iso88591':                       'en_GB.ISO8859-1',\n    'en_gb.iso885915':                      'en_GB.ISO8859-15',\n    'en_gb@euro':                           'en_GB.ISO8859-15',\n    'en_hk':                                'en_HK.ISO8859-1',\n    'en_hk.iso88591':                       'en_HK.ISO8859-1',\n    'en_ie':                                'en_IE.ISO8859-1',\n    'en_ie.iso88591':                       'en_IE.ISO8859-1',\n    'en_ie.iso885915':                      'en_IE.ISO8859-15',\n    'en_ie.iso885915@euro':                 'en_IE.ISO8859-15',\n    'en_ie.utf8@euro':                      'en_IE.UTF-8',\n    'en_ie@euro':                           'en_IE.ISO8859-15',\n    'en_in':                                'en_IN.ISO8859-1',\n    'en_nz':                                'en_NZ.ISO8859-1',\n    'en_nz.iso88591':                       'en_NZ.ISO8859-1',\n    'en_ph':                                'en_PH.ISO8859-1',\n    'en_ph.iso88591':                       'en_PH.ISO8859-1',\n    'en_sg':                                'en_SG.ISO8859-1',\n    'en_sg.iso88591':                       'en_SG.ISO8859-1',\n    'en_uk':                                'en_GB.ISO8859-1',\n    'en_us':                                'en_US.ISO8859-1',\n    'en_us.88591':                          'en_US.ISO8859-1',\n    'en_us.885915':                         'en_US.ISO8859-15',\n    'en_us.iso88591':                       'en_US.ISO8859-1',\n    'en_us.iso885915':                      'en_US.ISO8859-15',\n    'en_us.iso885915@euro':                 'en_US.ISO8859-15',\n    'en_us@euro':                           'en_US.ISO8859-15',\n    'en_us@euro@euro':                      'en_US.ISO8859-15',\n    'en_za':                                'en_ZA.ISO8859-1',\n    'en_za.88591':                          'en_ZA.ISO8859-1',\n    'en_za.iso88591':                       'en_ZA.ISO8859-1',\n    'en_za.iso885915':                      'en_ZA.ISO8859-15',\n    'en_za@euro':                           'en_ZA.ISO8859-15',\n    'en_zw':                                'en_ZW.ISO8859-1',\n    'en_zw.iso88591':                       'en_ZW.ISO8859-1',\n    'eng_gb':                               'en_GB.ISO8859-1',\n    'eng_gb.8859':                          'en_GB.ISO8859-1',\n    'english':                              'en_EN.ISO8859-1',\n    'english.iso88591':                     'en_EN.ISO8859-1',\n    'english_uk':                           'en_GB.ISO8859-1',\n    'english_uk.8859':                      'en_GB.ISO8859-1',\n    'english_united-states':                'en_US.ISO8859-1',\n    'english_united-states.437':            'C',\n    'english_us':                           'en_US.ISO8859-1',\n    'english_us.8859':                      'en_US.ISO8859-1',\n    'english_us.ascii':                     'en_US.ISO8859-1',\n    'eo':                                   'eo_XX.ISO8859-3',\n    'eo_eo':                                'eo_EO.ISO8859-3',\n    'eo_eo.iso88593':                       'eo_EO.ISO8859-3',\n    'eo_xx':                                'eo_XX.ISO8859-3',\n    'eo_xx.iso88593':                       'eo_XX.ISO8859-3',\n    'es':                                   'es_ES.ISO8859-1',\n    'es_ar':                                'es_AR.ISO8859-1',\n    'es_ar.iso88591':                       'es_AR.ISO8859-1',\n    'es_bo':                                'es_BO.ISO8859-1',\n    'es_bo.iso88591':                       'es_BO.ISO8859-1',\n    'es_cl':                                'es_CL.ISO8859-1',\n    'es_cl.iso88591':                       'es_CL.ISO8859-1',\n    'es_co':                                'es_CO.ISO8859-1',\n    'es_co.iso88591':                       'es_CO.ISO8859-1',\n    'es_cr':                                'es_CR.ISO8859-1',\n    'es_cr.iso88591':                       'es_CR.ISO8859-1',\n    'es_do':                                'es_DO.ISO8859-1',\n    'es_do.iso88591':                       'es_DO.ISO8859-1',\n    'es_ec':                                'es_EC.ISO8859-1',\n    'es_ec.iso88591':                       'es_EC.ISO8859-1',\n    'es_es':                                'es_ES.ISO8859-1',\n    'es_es.88591':                          'es_ES.ISO8859-1',\n    'es_es.iso88591':                       'es_ES.ISO8859-1',\n    'es_es.iso885915':                      'es_ES.ISO8859-15',\n    'es_es.iso885915@euro':                 'es_ES.ISO8859-15',\n    'es_es.utf8@euro':                      'es_ES.UTF-8',\n    'es_es@euro':                           'es_ES.ISO8859-15',\n    'es_gt':                                'es_GT.ISO8859-1',\n    'es_gt.iso88591':                       'es_GT.ISO8859-1',\n    'es_hn':                                'es_HN.ISO8859-1',\n    'es_hn.iso88591':                       'es_HN.ISO8859-1',\n    'es_mx':                                'es_MX.ISO8859-1',\n    'es_mx.iso88591':                       'es_MX.ISO8859-1',\n    'es_ni':                                'es_NI.ISO8859-1',\n    'es_ni.iso88591':                       'es_NI.ISO8859-1',\n    'es_pa':                                'es_PA.ISO8859-1',\n    'es_pa.iso88591':                       'es_PA.ISO8859-1',\n    'es_pa.iso885915':                      'es_PA.ISO8859-15',\n    'es_pa@euro':                           'es_PA.ISO8859-15',\n    'es_pe':                                'es_PE.ISO8859-1',\n    'es_pe.iso88591':                       'es_PE.ISO8859-1',\n    'es_pe.iso885915':                      'es_PE.ISO8859-15',\n    'es_pe@euro':                           'es_PE.ISO8859-15',\n    'es_pr':                                'es_PR.ISO8859-1',\n    'es_pr.iso88591':                       'es_PR.ISO8859-1',\n    'es_py':                                'es_PY.ISO8859-1',\n    'es_py.iso88591':                       'es_PY.ISO8859-1',\n    'es_py.iso885915':                      'es_PY.ISO8859-15',\n    'es_py@euro':                           'es_PY.ISO8859-15',\n    'es_sv':                                'es_SV.ISO8859-1',\n    'es_sv.iso88591':                       'es_SV.ISO8859-1',\n    'es_sv.iso885915':                      'es_SV.ISO8859-15',\n    'es_sv@euro':                           'es_SV.ISO8859-15',\n    'es_us':                                'es_US.ISO8859-1',\n    'es_us.iso88591':                       'es_US.ISO8859-1',\n    'es_uy':                                'es_UY.ISO8859-1',\n    'es_uy.iso88591':                       'es_UY.ISO8859-1',\n    'es_uy.iso885915':                      'es_UY.ISO8859-15',\n    'es_uy@euro':                           'es_UY.ISO8859-15',\n    'es_ve':                                'es_VE.ISO8859-1',\n    'es_ve.iso88591':                       'es_VE.ISO8859-1',\n    'es_ve.iso885915':                      'es_VE.ISO8859-15',\n    'es_ve@euro':                           'es_VE.ISO8859-15',\n    'estonian':                             'et_EE.ISO8859-1',\n    'et':                                   'et_EE.ISO8859-15',\n    'et_ee':                                'et_EE.ISO8859-15',\n    'et_ee.iso88591':                       'et_EE.ISO8859-1',\n    'et_ee.iso885913':                      'et_EE.ISO8859-13',\n    'et_ee.iso885915':                      'et_EE.ISO8859-15',\n    'et_ee.iso88594':                       'et_EE.ISO8859-4',\n    'et_ee@euro':                           'et_EE.ISO8859-15',\n    'eu':                                   'eu_ES.ISO8859-1',\n    'eu_es':                                'eu_ES.ISO8859-1',\n    'eu_es.iso88591':                       'eu_ES.ISO8859-1',\n    'eu_es.iso885915':                      'eu_ES.ISO8859-15',\n    'eu_es.iso885915@euro':                 'eu_ES.ISO8859-15',\n    'eu_es.utf8@euro':                      'eu_ES.UTF-8',\n    'eu_es@euro':                           'eu_ES.ISO8859-15',\n    'fa':                                   'fa_IR.UTF-8',\n    'fa_ir':                                'fa_IR.UTF-8',\n    'fa_ir.isiri3342':                      'fa_IR.ISIRI-3342',\n    'fi':                                   'fi_FI.ISO8859-15',\n    'fi.iso885915':                         'fi_FI.ISO8859-15',\n    'fi_fi':                                'fi_FI.ISO8859-15',\n    'fi_fi.88591':                          'fi_FI.ISO8859-1',\n    'fi_fi.iso88591':                       'fi_FI.ISO8859-1',\n    'fi_fi.iso885915':                      'fi_FI.ISO8859-15',\n    'fi_fi.iso885915@euro':                 'fi_FI.ISO8859-15',\n    'fi_fi.utf8@euro':                      'fi_FI.UTF-8',\n    'fi_fi@euro':                           'fi_FI.ISO8859-15',\n    'finnish':                              'fi_FI.ISO8859-1',\n    'finnish.iso88591':                     'fi_FI.ISO8859-1',\n    'fo':                                   'fo_FO.ISO8859-1',\n    'fo_fo':                                'fo_FO.ISO8859-1',\n    'fo_fo.iso88591':                       'fo_FO.ISO8859-1',\n    'fo_fo.iso885915':                      'fo_FO.ISO8859-15',\n    'fo_fo@euro':                           'fo_FO.ISO8859-15',\n    'fr':                                   'fr_FR.ISO8859-1',\n    'fr.iso885915':                         'fr_FR.ISO8859-15',\n    'fr_be':                                'fr_BE.ISO8859-1',\n    'fr_be.88591':                          'fr_BE.ISO8859-1',\n    'fr_be.iso88591':                       'fr_BE.ISO8859-1',\n    'fr_be.iso885915':                      'fr_BE.ISO8859-15',\n    'fr_be.iso885915@euro':                 'fr_BE.ISO8859-15',\n    'fr_be.utf8@euro':                      'fr_BE.UTF-8',\n    'fr_be@euro':                           'fr_BE.ISO8859-15',\n    'fr_ca':                                'fr_CA.ISO8859-1',\n    'fr_ca.88591':                          'fr_CA.ISO8859-1',\n    'fr_ca.iso88591':                       'fr_CA.ISO8859-1',\n    'fr_ca.iso885915':                      'fr_CA.ISO8859-15',\n    'fr_ca@euro':                           'fr_CA.ISO8859-15',\n    'fr_ch':                                'fr_CH.ISO8859-1',\n    'fr_ch.88591':                          'fr_CH.ISO8859-1',\n    'fr_ch.iso88591':                       'fr_CH.ISO8859-1',\n    'fr_ch.iso885915':                      'fr_CH.ISO8859-15',\n    'fr_ch@euro':                           'fr_CH.ISO8859-15',\n    'fr_fr':                                'fr_FR.ISO8859-1',\n    'fr_fr.88591':                          'fr_FR.ISO8859-1',\n    'fr_fr.iso88591':                       'fr_FR.ISO8859-1',\n    'fr_fr.iso885915':                      'fr_FR.ISO8859-15',\n    'fr_fr.iso885915@euro':                 'fr_FR.ISO8859-15',\n    'fr_fr.utf8@euro':                      'fr_FR.UTF-8',\n    'fr_fr@euro':                           'fr_FR.ISO8859-15',\n    'fr_lu':                                'fr_LU.ISO8859-1',\n    'fr_lu.88591':                          'fr_LU.ISO8859-1',\n    'fr_lu.iso88591':                       'fr_LU.ISO8859-1',\n    'fr_lu.iso885915':                      'fr_LU.ISO8859-15',\n    'fr_lu.iso885915@euro':                 'fr_LU.ISO8859-15',\n    'fr_lu.utf8@euro':                      'fr_LU.UTF-8',\n    'fr_lu@euro':                           'fr_LU.ISO8859-15',\n    'fran\\xe7ais':                          'fr_FR.ISO8859-1',\n    'fre_fr':                               'fr_FR.ISO8859-1',\n    'fre_fr.8859':                          'fr_FR.ISO8859-1',\n    'french':                               'fr_FR.ISO8859-1',\n    'french.iso88591':                      'fr_CH.ISO8859-1',\n    'french_france':                        'fr_FR.ISO8859-1',\n    'french_france.8859':                   'fr_FR.ISO8859-1',\n    'ga':                                   'ga_IE.ISO8859-1',\n    'ga_ie':                                'ga_IE.ISO8859-1',\n    'ga_ie.iso88591':                       'ga_IE.ISO8859-1',\n    'ga_ie.iso885914':                      'ga_IE.ISO8859-14',\n    'ga_ie.iso885915':                      'ga_IE.ISO8859-15',\n    'ga_ie.iso885915@euro':                 'ga_IE.ISO8859-15',\n    'ga_ie.utf8@euro':                      'ga_IE.UTF-8',\n    'ga_ie@euro':                           'ga_IE.ISO8859-15',\n    'galego':                               'gl_ES.ISO8859-1',\n    'galician':                             'gl_ES.ISO8859-1',\n    'gd':                                   'gd_GB.ISO8859-1',\n    'gd_gb':                                'gd_GB.ISO8859-1',\n    'gd_gb.iso88591':                       'gd_GB.ISO8859-1',\n    'gd_gb.iso885914':                      'gd_GB.ISO8859-14',\n    'gd_gb.iso885915':                      'gd_GB.ISO8859-15',\n    'gd_gb@euro':                           'gd_GB.ISO8859-15',\n    'ger_de':                               'de_DE.ISO8859-1',\n    'ger_de.8859':                          'de_DE.ISO8859-1',\n    'german':                               'de_DE.ISO8859-1',\n    'german.iso88591':                      'de_CH.ISO8859-1',\n    'german_germany':                       'de_DE.ISO8859-1',\n    'german_germany.8859':                  'de_DE.ISO8859-1',\n    'gl':                                   'gl_ES.ISO8859-1',\n    'gl_es':                                'gl_ES.ISO8859-1',\n    'gl_es.iso88591':                       'gl_ES.ISO8859-1',\n    'gl_es.iso885915':                      'gl_ES.ISO8859-15',\n    'gl_es.iso885915@euro':                 'gl_ES.ISO8859-15',\n    'gl_es.utf8@euro':                      'gl_ES.UTF-8',\n    'gl_es@euro':                           'gl_ES.ISO8859-15',\n    'greek':                                'el_GR.ISO8859-7',\n    'greek.iso88597':                       'el_GR.ISO8859-7',\n    'gu_in':                                'gu_IN.UTF-8',\n    'gv':                                   'gv_GB.ISO8859-1',\n    'gv_gb':                                'gv_GB.ISO8859-1',\n    'gv_gb.iso88591':                       'gv_GB.ISO8859-1',\n    'gv_gb.iso885914':                      'gv_GB.ISO8859-14',\n    'gv_gb.iso885915':                      'gv_GB.ISO8859-15',\n    'gv_gb@euro':                           'gv_GB.ISO8859-15',\n    'he':                                   'he_IL.ISO8859-8',\n    'he_il':                                'he_IL.ISO8859-8',\n    'he_il.cp1255':                         'he_IL.CP1255',\n    'he_il.iso88598':                       'he_IL.ISO8859-8',\n    'he_il.microsoftcp1255':                'he_IL.CP1255',\n    'hebrew':                               'he_IL.ISO8859-8',\n    'hebrew.iso88598':                      'he_IL.ISO8859-8',\n    'hi':                                   'hi_IN.ISCII-DEV',\n    'hi_in':                                'hi_IN.ISCII-DEV',\n    'hi_in.isciidev':                       'hi_IN.ISCII-DEV',\n    'hne':                                  'hne_IN.UTF-8',\n    'hne_in':                               'hne_IN.UTF-8',\n    'hr':                                   'hr_HR.ISO8859-2',\n    'hr_hr':                                'hr_HR.ISO8859-2',\n    'hr_hr.iso88592':                       'hr_HR.ISO8859-2',\n    'hrvatski':                             'hr_HR.ISO8859-2',\n    'hu':                                   'hu_HU.ISO8859-2',\n    'hu_hu':                                'hu_HU.ISO8859-2',\n    'hu_hu.iso88592':                       'hu_HU.ISO8859-2',\n    'hungarian':                            'hu_HU.ISO8859-2',\n    'icelandic':                            'is_IS.ISO8859-1',\n    'icelandic.iso88591':                   'is_IS.ISO8859-1',\n    'id':                                   'id_ID.ISO8859-1',\n    'id_id':                                'id_ID.ISO8859-1',\n    'in':                                   'id_ID.ISO8859-1',\n    'in_id':                                'id_ID.ISO8859-1',\n    'is':                                   'is_IS.ISO8859-1',\n    'is_is':                                'is_IS.ISO8859-1',\n    'is_is.iso88591':                       'is_IS.ISO8859-1',\n    'is_is.iso885915':                      'is_IS.ISO8859-15',\n    'is_is@euro':                           'is_IS.ISO8859-15',\n    'iso-8859-1':                           'en_US.ISO8859-1',\n    'iso-8859-15':                          'en_US.ISO8859-15',\n    'iso8859-1':                            'en_US.ISO8859-1',\n    'iso8859-15':                           'en_US.ISO8859-15',\n    'iso_8859_1':                           'en_US.ISO8859-1',\n    'iso_8859_15':                          'en_US.ISO8859-15',\n    'it':                                   'it_IT.ISO8859-1',\n    'it.iso885915':                         'it_IT.ISO8859-15',\n    'it_ch':                                'it_CH.ISO8859-1',\n    'it_ch.iso88591':                       'it_CH.ISO8859-1',\n    'it_ch.iso885915':                      'it_CH.ISO8859-15',\n    'it_ch@euro':                           'it_CH.ISO8859-15',\n    'it_it':                                'it_IT.ISO8859-1',\n    'it_it.88591':                          'it_IT.ISO8859-1',\n    'it_it.iso88591':                       'it_IT.ISO8859-1',\n    'it_it.iso885915':                      'it_IT.ISO8859-15',\n    'it_it.iso885915@euro':                 'it_IT.ISO8859-15',\n    'it_it.utf8@euro':                      'it_IT.UTF-8',\n    'it_it@euro':                           'it_IT.ISO8859-15',\n    'italian':                              'it_IT.ISO8859-1',\n    'italian.iso88591':                     'it_IT.ISO8859-1',\n    'iu':                                   'iu_CA.NUNACOM-8',\n    'iu_ca':                                'iu_CA.NUNACOM-8',\n    'iu_ca.nunacom8':                       'iu_CA.NUNACOM-8',\n    'iw':                                   'he_IL.ISO8859-8',\n    'iw_il':                                'he_IL.ISO8859-8',\n    'iw_il.iso88598':                       'he_IL.ISO8859-8',\n    'ja':                                   'ja_JP.eucJP',\n    'ja.jis':                               'ja_JP.JIS7',\n    'ja.sjis':                              'ja_JP.SJIS',\n    'ja_jp':                                'ja_JP.eucJP',\n    'ja_jp.ajec':                           'ja_JP.eucJP',\n    'ja_jp.euc':                            'ja_JP.eucJP',\n    'ja_jp.eucjp':                          'ja_JP.eucJP',\n    'ja_jp.iso-2022-jp':                    'ja_JP.JIS7',\n    'ja_jp.iso2022jp':                      'ja_JP.JIS7',\n    'ja_jp.jis':                            'ja_JP.JIS7',\n    'ja_jp.jis7':                           'ja_JP.JIS7',\n    'ja_jp.mscode':                         'ja_JP.SJIS',\n    'ja_jp.pck':                            'ja_JP.SJIS',\n    'ja_jp.sjis':                           'ja_JP.SJIS',\n    'ja_jp.ujis':                           'ja_JP.eucJP',\n    'japan':                                'ja_JP.eucJP',\n    'japanese':                             'ja_JP.eucJP',\n    'japanese-euc':                         'ja_JP.eucJP',\n    'japanese.euc':                         'ja_JP.eucJP',\n    'japanese.sjis':                        'ja_JP.SJIS',\n    'jp_jp':                                'ja_JP.eucJP',\n    'ka':                                   'ka_GE.GEORGIAN-ACADEMY',\n    'ka_ge':                                'ka_GE.GEORGIAN-ACADEMY',\n    'ka_ge.georgianacademy':                'ka_GE.GEORGIAN-ACADEMY',\n    'ka_ge.georgianps':                     'ka_GE.GEORGIAN-PS',\n    'ka_ge.georgianrs':                     'ka_GE.GEORGIAN-ACADEMY',\n    'kl':                                   'kl_GL.ISO8859-1',\n    'kl_gl':                                'kl_GL.ISO8859-1',\n    'kl_gl.iso88591':                       'kl_GL.ISO8859-1',\n    'kl_gl.iso885915':                      'kl_GL.ISO8859-15',\n    'kl_gl@euro':                           'kl_GL.ISO8859-15',\n    'km_kh':                                'km_KH.UTF-8',\n    'kn':                                   'kn_IN.UTF-8',\n    'kn_in':                                'kn_IN.UTF-8',\n    'ko':                                   'ko_KR.eucKR',\n    'ko_kr':                                'ko_KR.eucKR',\n    'ko_kr.euc':                            'ko_KR.eucKR',\n    'ko_kr.euckr':                          'ko_KR.eucKR',\n    'korean':                               'ko_KR.eucKR',\n    'korean.euc':                           'ko_KR.eucKR',\n    'ks':                                   'ks_IN.UTF-8',\n    'ks_in':                                'ks_IN.UTF-8',\n    'ks_in@devanagari':                     'ks_IN.UTF-8@devanagari',\n    'kw':                                   'kw_GB.ISO8859-1',\n    'kw_gb':                                'kw_GB.ISO8859-1',\n    'kw_gb.iso88591':                       'kw_GB.ISO8859-1',\n    'kw_gb.iso885914':                      'kw_GB.ISO8859-14',\n    'kw_gb.iso885915':                      'kw_GB.ISO8859-15',\n    'kw_gb@euro':                           'kw_GB.ISO8859-15',\n    'ky':                                   'ky_KG.UTF-8',\n    'ky_kg':                                'ky_KG.UTF-8',\n    'lithuanian':                           'lt_LT.ISO8859-13',\n    'lo':                                   'lo_LA.MULELAO-1',\n    'lo_la':                                'lo_LA.MULELAO-1',\n    'lo_la.cp1133':                         'lo_LA.IBM-CP1133',\n    'lo_la.ibmcp1133':                      'lo_LA.IBM-CP1133',\n    'lo_la.mulelao1':                       'lo_LA.MULELAO-1',\n    'lt':                                   'lt_LT.ISO8859-13',\n    'lt_lt':                                'lt_LT.ISO8859-13',\n    'lt_lt.iso885913':                      'lt_LT.ISO8859-13',\n    'lt_lt.iso88594':                       'lt_LT.ISO8859-4',\n    'lv':                                   'lv_LV.ISO8859-13',\n    'lv_lv':                                'lv_LV.ISO8859-13',\n    'lv_lv.iso885913':                      'lv_LV.ISO8859-13',\n    'lv_lv.iso88594':                       'lv_LV.ISO8859-4',\n    'mai':                                  'mai_IN.UTF-8',\n    'mai_in':                               'mai_IN.UTF-8',\n    'mi':                                   'mi_NZ.ISO8859-1',\n    'mi_nz':                                'mi_NZ.ISO8859-1',\n    'mi_nz.iso88591':                       'mi_NZ.ISO8859-1',\n    'mk':                                   'mk_MK.ISO8859-5',\n    'mk_mk':                                'mk_MK.ISO8859-5',\n    'mk_mk.cp1251':                         'mk_MK.CP1251',\n    'mk_mk.iso88595':                       'mk_MK.ISO8859-5',\n    'mk_mk.microsoftcp1251':                'mk_MK.CP1251',\n    'ml':                                   'ml_IN.UTF-8',\n    'ml_in':                                'ml_IN.UTF-8',\n    'mr':                                   'mr_IN.UTF-8',\n    'mr_in':                                'mr_IN.UTF-8',\n    'ms':                                   'ms_MY.ISO8859-1',\n    'ms_my':                                'ms_MY.ISO8859-1',\n    'ms_my.iso88591':                       'ms_MY.ISO8859-1',\n    'mt':                                   'mt_MT.ISO8859-3',\n    'mt_mt':                                'mt_MT.ISO8859-3',\n    'mt_mt.iso88593':                       'mt_MT.ISO8859-3',\n    'nb':                                   'nb_NO.ISO8859-1',\n    'nb_no':                                'nb_NO.ISO8859-1',\n    'nb_no.88591':                          'nb_NO.ISO8859-1',\n    'nb_no.iso88591':                       'nb_NO.ISO8859-1',\n    'nb_no.iso885915':                      'nb_NO.ISO8859-15',\n    'nb_no@euro':                           'nb_NO.ISO8859-15',\n    'ne_np':                                'ne_NP.UTF-8',\n    'nl':                                   'nl_NL.ISO8859-1',\n    'nl.iso885915':                         'nl_NL.ISO8859-15',\n    'nl_be':                                'nl_BE.ISO8859-1',\n    'nl_be.88591':                          'nl_BE.ISO8859-1',\n    'nl_be.iso88591':                       'nl_BE.ISO8859-1',\n    'nl_be.iso885915':                      'nl_BE.ISO8859-15',\n    'nl_be.iso885915@euro':                 'nl_BE.ISO8859-15',\n    'nl_be.utf8@euro':                      'nl_BE.UTF-8',\n    'nl_be@euro':                           'nl_BE.ISO8859-15',\n    'nl_nl':                                'nl_NL.ISO8859-1',\n    'nl_nl.88591':                          'nl_NL.ISO8859-1',\n    'nl_nl.iso88591':                       'nl_NL.ISO8859-1',\n    'nl_nl.iso885915':                      'nl_NL.ISO8859-15',\n    'nl_nl.iso885915@euro':                 'nl_NL.ISO8859-15',\n    'nl_nl.utf8@euro':                      'nl_NL.UTF-8',\n    'nl_nl@euro':                           'nl_NL.ISO8859-15',\n    'nn':                                   'nn_NO.ISO8859-1',\n    'nn_no':                                'nn_NO.ISO8859-1',\n    'nn_no.88591':                          'nn_NO.ISO8859-1',\n    'nn_no.iso88591':                       'nn_NO.ISO8859-1',\n    'nn_no.iso885915':                      'nn_NO.ISO8859-15',\n    'nn_no@euro':                           'nn_NO.ISO8859-15',\n    'no':                                   'no_NO.ISO8859-1',\n    'no@nynorsk':                           'ny_NO.ISO8859-1',\n    'no_no':                                'no_NO.ISO8859-1',\n    'no_no.88591':                          'no_NO.ISO8859-1',\n    'no_no.iso88591':                       'no_NO.ISO8859-1',\n    'no_no.iso885915':                      'no_NO.ISO8859-15',\n    'no_no.iso88591@bokmal':                'no_NO.ISO8859-1',\n    'no_no.iso88591@nynorsk':               'no_NO.ISO8859-1',\n    'no_no@euro':                           'no_NO.ISO8859-15',\n    'norwegian':                            'no_NO.ISO8859-1',\n    'norwegian.iso88591':                   'no_NO.ISO8859-1',\n    'nr':                                   'nr_ZA.ISO8859-1',\n    'nr_za':                                'nr_ZA.ISO8859-1',\n    'nr_za.iso88591':                       'nr_ZA.ISO8859-1',\n    'nso':                                  'nso_ZA.ISO8859-15',\n    'nso_za':                               'nso_ZA.ISO8859-15',\n    'nso_za.iso885915':                     'nso_ZA.ISO8859-15',\n    'ny':                                   'ny_NO.ISO8859-1',\n    'ny_no':                                'ny_NO.ISO8859-1',\n    'ny_no.88591':                          'ny_NO.ISO8859-1',\n    'ny_no.iso88591':                       'ny_NO.ISO8859-1',\n    'ny_no.iso885915':                      'ny_NO.ISO8859-15',\n    'ny_no@euro':                           'ny_NO.ISO8859-15',\n    'nynorsk':                              'nn_NO.ISO8859-1',\n    'oc':                                   'oc_FR.ISO8859-1',\n    'oc_fr':                                'oc_FR.ISO8859-1',\n    'oc_fr.iso88591':                       'oc_FR.ISO8859-1',\n    'oc_fr.iso885915':                      'oc_FR.ISO8859-15',\n    'oc_fr@euro':                           'oc_FR.ISO8859-15',\n    'or':                                   'or_IN.UTF-8',\n    'or_in':                                'or_IN.UTF-8',\n    'pa':                                   'pa_IN.UTF-8',\n    'pa_in':                                'pa_IN.UTF-8',\n    'pd':                                   'pd_US.ISO8859-1',\n    'pd_de':                                'pd_DE.ISO8859-1',\n    'pd_de.iso88591':                       'pd_DE.ISO8859-1',\n    'pd_de.iso885915':                      'pd_DE.ISO8859-15',\n    'pd_de@euro':                           'pd_DE.ISO8859-15',\n    'pd_us':                                'pd_US.ISO8859-1',\n    'pd_us.iso88591':                       'pd_US.ISO8859-1',\n    'pd_us.iso885915':                      'pd_US.ISO8859-15',\n    'pd_us@euro':                           'pd_US.ISO8859-15',\n    'ph':                                   'ph_PH.ISO8859-1',\n    'ph_ph':                                'ph_PH.ISO8859-1',\n    'ph_ph.iso88591':                       'ph_PH.ISO8859-1',\n    'pl':                                   'pl_PL.ISO8859-2',\n    'pl_pl':                                'pl_PL.ISO8859-2',\n    'pl_pl.iso88592':                       'pl_PL.ISO8859-2',\n    'polish':                               'pl_PL.ISO8859-2',\n    'portuguese':                           'pt_PT.ISO8859-1',\n    'portuguese.iso88591':                  'pt_PT.ISO8859-1',\n    'portuguese_brazil':                    'pt_BR.ISO8859-1',\n    'portuguese_brazil.8859':               'pt_BR.ISO8859-1',\n    'posix':                                'C',\n    'posix-utf2':                           'C',\n    'pp':                                   'pp_AN.ISO8859-1',\n    'pp_an':                                'pp_AN.ISO8859-1',\n    'pp_an.iso88591':                       'pp_AN.ISO8859-1',\n    'pt':                                   'pt_PT.ISO8859-1',\n    'pt.iso885915':                         'pt_PT.ISO8859-15',\n    'pt_br':                                'pt_BR.ISO8859-1',\n    'pt_br.88591':                          'pt_BR.ISO8859-1',\n    'pt_br.iso88591':                       'pt_BR.ISO8859-1',\n    'pt_br.iso885915':                      'pt_BR.ISO8859-15',\n    'pt_br@euro':                           'pt_BR.ISO8859-15',\n    'pt_pt':                                'pt_PT.ISO8859-1',\n    'pt_pt.88591':                          'pt_PT.ISO8859-1',\n    'pt_pt.iso88591':                       'pt_PT.ISO8859-1',\n    'pt_pt.iso885915':                      'pt_PT.ISO8859-15',\n    'pt_pt.iso885915@euro':                 'pt_PT.ISO8859-15',\n    'pt_pt.utf8@euro':                      'pt_PT.UTF-8',\n    'pt_pt@euro':                           'pt_PT.ISO8859-15',\n    'ro':                                   'ro_RO.ISO8859-2',\n    'ro_ro':                                'ro_RO.ISO8859-2',\n    'ro_ro.iso88592':                       'ro_RO.ISO8859-2',\n    'romanian':                             'ro_RO.ISO8859-2',\n    'ru':                                   'ru_RU.UTF-8',\n    'ru.koi8r':                             'ru_RU.KOI8-R',\n    'ru_ru':                                'ru_RU.UTF-8',\n    'ru_ru.cp1251':                         'ru_RU.CP1251',\n    'ru_ru.iso88595':                       'ru_RU.ISO8859-5',\n    'ru_ru.koi8r':                          'ru_RU.KOI8-R',\n    'ru_ru.microsoftcp1251':                'ru_RU.CP1251',\n    'ru_ua':                                'ru_UA.KOI8-U',\n    'ru_ua.cp1251':                         'ru_UA.CP1251',\n    'ru_ua.koi8u':                          'ru_UA.KOI8-U',\n    'ru_ua.microsoftcp1251':                'ru_UA.CP1251',\n    'rumanian':                             'ro_RO.ISO8859-2',\n    'russian':                              'ru_RU.ISO8859-5',\n    'rw':                                   'rw_RW.ISO8859-1',\n    'rw_rw':                                'rw_RW.ISO8859-1',\n    'rw_rw.iso88591':                       'rw_RW.ISO8859-1',\n    'sd':                                   'sd_IN.UTF-8',\n    'sd@devanagari':                        'sd_IN.UTF-8@devanagari',\n    'sd_in':                                'sd_IN.UTF-8',\n    'sd_in@devanagari':                     'sd_IN.UTF-8@devanagari',\n    'se_no':                                'se_NO.UTF-8',\n    'serbocroatian':                        'sr_RS.UTF-8@latin',\n    'sh':                                   'sr_RS.UTF-8@latin',\n    'sh_ba.iso88592@bosnia':                'sr_CS.ISO8859-2',\n    'sh_hr':                                'sh_HR.ISO8859-2',\n    'sh_hr.iso88592':                       'hr_HR.ISO8859-2',\n    'sh_sp':                                'sr_CS.ISO8859-2',\n    'sh_yu':                                'sr_RS.UTF-8@latin',\n    'si':                                   'si_LK.UTF-8',\n    'si_lk':                                'si_LK.UTF-8',\n    'sinhala':                              'si_LK.UTF-8',\n    'sk':                                   'sk_SK.ISO8859-2',\n    'sk_sk':                                'sk_SK.ISO8859-2',\n    'sk_sk.iso88592':                       'sk_SK.ISO8859-2',\n    'sl':                                   'sl_SI.ISO8859-2',\n    'sl_cs':                                'sl_CS.ISO8859-2',\n    'sl_si':                                'sl_SI.ISO8859-2',\n    'sl_si.iso88592':                       'sl_SI.ISO8859-2',\n    'slovak':                               'sk_SK.ISO8859-2',\n    'slovene':                              'sl_SI.ISO8859-2',\n    'slovenian':                            'sl_SI.ISO8859-2',\n    'sp':                                   'sr_CS.ISO8859-5',\n    'sp_yu':                                'sr_CS.ISO8859-5',\n    'spanish':                              'es_ES.ISO8859-1',\n    'spanish.iso88591':                     'es_ES.ISO8859-1',\n    'spanish_spain':                        'es_ES.ISO8859-1',\n    'spanish_spain.8859':                   'es_ES.ISO8859-1',\n    'sq':                                   'sq_AL.ISO8859-2',\n    'sq_al':                                'sq_AL.ISO8859-2',\n    'sq_al.iso88592':                       'sq_AL.ISO8859-2',\n    'sr':                                   'sr_RS.UTF-8',\n    'sr@cyrillic':                          'sr_RS.UTF-8',\n    'sr@latin':                             'sr_RS.UTF-8@latin',\n    'sr@latn':                              'sr_CS.UTF-8@latin',\n    'sr_cs':                                'sr_CS.UTF-8',\n    'sr_cs.iso88592':                       'sr_CS.ISO8859-2',\n    'sr_cs.iso88592@latn':                  'sr_CS.ISO8859-2',\n    'sr_cs.iso88595':                       'sr_CS.ISO8859-5',\n    'sr_cs.utf8@latn':                      'sr_CS.UTF-8@latin',\n    'sr_cs@latn':                           'sr_CS.UTF-8@latin',\n    'sr_me':                                'sr_ME.UTF-8',\n    'sr_rs':                                'sr_RS.UTF-8',\n    'sr_rs.utf8@latn':                      'sr_RS.UTF-8@latin',\n    'sr_rs@latin':                          'sr_RS.UTF-8@latin',\n    'sr_rs@latn':                           'sr_RS.UTF-8@latin',\n    'sr_sp':                                'sr_CS.ISO8859-2',\n    'sr_yu':                                'sr_RS.UTF-8@latin',\n    'sr_yu.cp1251@cyrillic':                'sr_CS.CP1251',\n    'sr_yu.iso88592':                       'sr_CS.ISO8859-2',\n    'sr_yu.iso88595':                       'sr_CS.ISO8859-5',\n    'sr_yu.iso88595@cyrillic':              'sr_CS.ISO8859-5',\n    'sr_yu.microsoftcp1251@cyrillic':       'sr_CS.CP1251',\n    'sr_yu.utf8@cyrillic':                  'sr_RS.UTF-8',\n    'sr_yu@cyrillic':                       'sr_RS.UTF-8',\n    'ss':                                   'ss_ZA.ISO8859-1',\n    'ss_za':                                'ss_ZA.ISO8859-1',\n    'ss_za.iso88591':                       'ss_ZA.ISO8859-1',\n    'st':                                   'st_ZA.ISO8859-1',\n    'st_za':                                'st_ZA.ISO8859-1',\n    'st_za.iso88591':                       'st_ZA.ISO8859-1',\n    'sv':                                   'sv_SE.ISO8859-1',\n    'sv.iso885915':                         'sv_SE.ISO8859-15',\n    'sv_fi':                                'sv_FI.ISO8859-1',\n    'sv_fi.iso88591':                       'sv_FI.ISO8859-1',\n    'sv_fi.iso885915':                      'sv_FI.ISO8859-15',\n    'sv_fi.iso885915@euro':                 'sv_FI.ISO8859-15',\n    'sv_fi.utf8@euro':                      'sv_FI.UTF-8',\n    'sv_fi@euro':                           'sv_FI.ISO8859-15',\n    'sv_se':                                'sv_SE.ISO8859-1',\n    'sv_se.88591':                          'sv_SE.ISO8859-1',\n    'sv_se.iso88591':                       'sv_SE.ISO8859-1',\n    'sv_se.iso885915':                      'sv_SE.ISO8859-15',\n    'sv_se@euro':                           'sv_SE.ISO8859-15',\n    'swedish':                              'sv_SE.ISO8859-1',\n    'swedish.iso88591':                     'sv_SE.ISO8859-1',\n    'ta':                                   'ta_IN.TSCII-0',\n    'ta_in':                                'ta_IN.TSCII-0',\n    'ta_in.tscii':                          'ta_IN.TSCII-0',\n    'ta_in.tscii0':                         'ta_IN.TSCII-0',\n    'te':                                   'te_IN.UTF-8',\n    'tg':                                   'tg_TJ.KOI8-C',\n    'tg_tj':                                'tg_TJ.KOI8-C',\n    'tg_tj.koi8c':                          'tg_TJ.KOI8-C',\n    'th':                                   'th_TH.ISO8859-11',\n    'th_th':                                'th_TH.ISO8859-11',\n    'th_th.iso885911':                      'th_TH.ISO8859-11',\n    'th_th.tactis':                         'th_TH.TIS620',\n    'th_th.tis620':                         'th_TH.TIS620',\n    'thai':                                 'th_TH.ISO8859-11',\n    'tl':                                   'tl_PH.ISO8859-1',\n    'tl_ph':                                'tl_PH.ISO8859-1',\n    'tl_ph.iso88591':                       'tl_PH.ISO8859-1',\n    'tn':                                   'tn_ZA.ISO8859-15',\n    'tn_za':                                'tn_ZA.ISO8859-15',\n    'tn_za.iso885915':                      'tn_ZA.ISO8859-15',\n    'tr':                                   'tr_TR.ISO8859-9',\n    'tr_tr':                                'tr_TR.ISO8859-9',\n    'tr_tr.iso88599':                       'tr_TR.ISO8859-9',\n    'ts':                                   'ts_ZA.ISO8859-1',\n    'ts_za':                                'ts_ZA.ISO8859-1',\n    'ts_za.iso88591':                       'ts_ZA.ISO8859-1',\n    'tt':                                   'tt_RU.TATAR-CYR',\n    'tt_ru':                                'tt_RU.TATAR-CYR',\n    'tt_ru.koi8c':                          'tt_RU.KOI8-C',\n    'tt_ru.tatarcyr':                       'tt_RU.TATAR-CYR',\n    'turkish':                              'tr_TR.ISO8859-9',\n    'turkish.iso88599':                     'tr_TR.ISO8859-9',\n    'uk':                                   'uk_UA.KOI8-U',\n    'uk_ua':                                'uk_UA.KOI8-U',\n    'uk_ua.cp1251':                         'uk_UA.CP1251',\n    'uk_ua.iso88595':                       'uk_UA.ISO8859-5',\n    'uk_ua.koi8u':                          'uk_UA.KOI8-U',\n    'uk_ua.microsoftcp1251':                'uk_UA.CP1251',\n    'univ':                                 'en_US.utf',\n    'universal':                            'en_US.utf',\n    'universal.utf8@ucs4':                  'en_US.UTF-8',\n    'ur':                                   'ur_PK.CP1256',\n    'ur_in':                                'ur_IN.UTF-8',\n    'ur_pk':                                'ur_PK.CP1256',\n    'ur_pk.cp1256':                         'ur_PK.CP1256',\n    'ur_pk.microsoftcp1256':                'ur_PK.CP1256',\n    'uz':                                   'uz_UZ.UTF-8',\n    'uz_uz':                                'uz_UZ.UTF-8',\n    'uz_uz.iso88591':                       'uz_UZ.ISO8859-1',\n    'uz_uz.utf8@cyrillic':                  'uz_UZ.UTF-8',\n    'uz_uz@cyrillic':                       'uz_UZ.UTF-8',\n    've':                                   've_ZA.UTF-8',\n    've_za':                                've_ZA.UTF-8',\n    'vi':                                   'vi_VN.TCVN',\n    'vi_vn':                                'vi_VN.TCVN',\n    'vi_vn.tcvn':                           'vi_VN.TCVN',\n    'vi_vn.tcvn5712':                       'vi_VN.TCVN',\n    'vi_vn.viscii':                         'vi_VN.VISCII',\n    'vi_vn.viscii111':                      'vi_VN.VISCII',\n    'wa':                                   'wa_BE.ISO8859-1',\n    'wa_be':                                'wa_BE.ISO8859-1',\n    'wa_be.iso88591':                       'wa_BE.ISO8859-1',\n    'wa_be.iso885915':                      'wa_BE.ISO8859-15',\n    'wa_be.iso885915@euro':                 'wa_BE.ISO8859-15',\n    'wa_be@euro':                           'wa_BE.ISO8859-15',\n    'xh':                                   'xh_ZA.ISO8859-1',\n    'xh_za':                                'xh_ZA.ISO8859-1',\n    'xh_za.iso88591':                       'xh_ZA.ISO8859-1',\n    'yi':                                   'yi_US.CP1255',\n    'yi_us':                                'yi_US.CP1255',\n    'yi_us.cp1255':                         'yi_US.CP1255',\n    'yi_us.microsoftcp1255':                'yi_US.CP1255',\n    'zh':                                   'zh_CN.eucCN',\n    'zh_cn':                                'zh_CN.gb2312',\n    'zh_cn.big5':                           'zh_TW.big5',\n    'zh_cn.euc':                            'zh_CN.eucCN',\n    'zh_cn.gb18030':                        'zh_CN.gb18030',\n    'zh_cn.gb2312':                         'zh_CN.gb2312',\n    'zh_cn.gbk':                            'zh_CN.gbk',\n    'zh_hk':                                'zh_HK.big5hkscs',\n    'zh_hk.big5':                           'zh_HK.big5',\n    'zh_hk.big5hk':                         'zh_HK.big5hkscs',\n    'zh_hk.big5hkscs':                      'zh_HK.big5hkscs',\n    'zh_tw':                                'zh_TW.big5',\n    'zh_tw.big5':                           'zh_TW.big5',\n    'zh_tw.euc':                            'zh_TW.eucTW',\n    'zh_tw.euctw':                          'zh_TW.eucTW',\n    'zu':                                   'zu_ZA.ISO8859-1',\n    'zu_za':                                'zu_ZA.ISO8859-1',\n    'zu_za.iso88591':                       'zu_ZA.ISO8859-1',\n}\n\n#\n# This maps Windows language identifiers to locale strings.\n#\n# This list has been updated from\n# http://msdn.microsoft.com/library/default.asp?url=/library/en-us/intl/nls_238z.asp\n# to include every locale up to Windows Vista.\n#\n# NOTE: this mapping is incomplete.  If your language is missing, please\n# submit a bug report to the Python bug tracker at http://bugs.python.org/\n# Make sure you include the missing language identifier and the suggested\n# locale code.\n#\n\nwindows_locale = {\n    0x0436: \"af_ZA\", # Afrikaans\n    0x041c: \"sq_AL\", # Albanian\n    0x0484: \"gsw_FR\",# Alsatian - France\n    0x045e: \"am_ET\", # Amharic - Ethiopia\n    0x0401: \"ar_SA\", # Arabic - Saudi Arabia\n    0x0801: \"ar_IQ\", # Arabic - Iraq\n    0x0c01: \"ar_EG\", # Arabic - Egypt\n    0x1001: \"ar_LY\", # Arabic - Libya\n    0x1401: \"ar_DZ\", # Arabic - Algeria\n    0x1801: \"ar_MA\", # Arabic - Morocco\n    0x1c01: \"ar_TN\", # Arabic - Tunisia\n    0x2001: \"ar_OM\", # Arabic - Oman\n    0x2401: \"ar_YE\", # Arabic - Yemen\n    0x2801: \"ar_SY\", # Arabic - Syria\n    0x2c01: \"ar_JO\", # Arabic - Jordan\n    0x3001: \"ar_LB\", # Arabic - Lebanon\n    0x3401: \"ar_KW\", # Arabic - Kuwait\n    0x3801: \"ar_AE\", # Arabic - United Arab Emirates\n    0x3c01: \"ar_BH\", # Arabic - Bahrain\n    0x4001: \"ar_QA\", # Arabic - Qatar\n    0x042b: \"hy_AM\", # Armenian\n    0x044d: \"as_IN\", # Assamese - India\n    0x042c: \"az_AZ\", # Azeri - Latin\n    0x082c: \"az_AZ\", # Azeri - Cyrillic\n    0x046d: \"ba_RU\", # Bashkir\n    0x042d: \"eu_ES\", # Basque - Russia\n    0x0423: \"be_BY\", # Belarusian\n    0x0445: \"bn_IN\", # Begali\n    0x201a: \"bs_BA\", # Bosnian - Cyrillic\n    0x141a: \"bs_BA\", # Bosnian - Latin\n    0x047e: \"br_FR\", # Breton - France\n    0x0402: \"bg_BG\", # Bulgarian\n#    0x0455: \"my_MM\", # Burmese - Not supported\n    0x0403: \"ca_ES\", # Catalan\n    0x0004: \"zh_CHS\",# Chinese - Simplified\n    0x0404: \"zh_TW\", # Chinese - Taiwan\n    0x0804: \"zh_CN\", # Chinese - PRC\n    0x0c04: \"zh_HK\", # Chinese - Hong Kong S.A.R.\n    0x1004: \"zh_SG\", # Chinese - Singapore\n    0x1404: \"zh_MO\", # Chinese - Macao S.A.R.\n    0x7c04: \"zh_CHT\",# Chinese - Traditional\n    0x0483: \"co_FR\", # Corsican - France\n    0x041a: \"hr_HR\", # Croatian\n    0x101a: \"hr_BA\", # Croatian - Bosnia\n    0x0405: \"cs_CZ\", # Czech\n    0x0406: \"da_DK\", # Danish\n    0x048c: \"gbz_AF\",# Dari - Afghanistan\n    0x0465: \"div_MV\",# Divehi - Maldives\n    0x0413: \"nl_NL\", # Dutch - The Netherlands\n    0x0813: \"nl_BE\", # Dutch - Belgium\n    0x0409: \"en_US\", # English - United States\n    0x0809: \"en_GB\", # English - United Kingdom\n    0x0c09: \"en_AU\", # English - Australia\n    0x1009: \"en_CA\", # English - Canada\n    0x1409: \"en_NZ\", # English - New Zealand\n    0x1809: \"en_IE\", # English - Ireland\n    0x1c09: \"en_ZA\", # English - South Africa\n    0x2009: \"en_JA\", # English - Jamaica\n    0x2409: \"en_CB\", # English - Carribbean\n    0x2809: \"en_BZ\", # English - Belize\n    0x2c09: \"en_TT\", # English - Trinidad\n    0x3009: \"en_ZW\", # English - Zimbabwe\n    0x3409: \"en_PH\", # English - Philippines\n    0x4009: \"en_IN\", # English - India\n    0x4409: \"en_MY\", # English - Malaysia\n    0x4809: \"en_IN\", # English - Singapore\n    0x0425: \"et_EE\", # Estonian\n    0x0438: \"fo_FO\", # Faroese\n    0x0464: \"fil_PH\",# Filipino\n    0x040b: \"fi_FI\", # Finnish\n    0x040c: \"fr_FR\", # French - France\n    0x080c: \"fr_BE\", # French - Belgium\n    0x0c0c: \"fr_CA\", # French - Canada\n    0x100c: \"fr_CH\", # French - Switzerland\n    0x140c: \"fr_LU\", # French - Luxembourg\n    0x180c: \"fr_MC\", # French - Monaco\n    0x0462: \"fy_NL\", # Frisian - Netherlands\n    0x0456: \"gl_ES\", # Galician\n    0x0437: \"ka_GE\", # Georgian\n    0x0407: \"de_DE\", # German - Germany\n    0x0807: \"de_CH\", # German - Switzerland\n    0x0c07: \"de_AT\", # German - Austria\n    0x1007: \"de_LU\", # German - Luxembourg\n    0x1407: \"de_LI\", # German - Liechtenstein\n    0x0408: \"el_GR\", # Greek\n    0x046f: \"kl_GL\", # Greenlandic - Greenland\n    0x0447: \"gu_IN\", # Gujarati\n    0x0468: \"ha_NG\", # Hausa - Latin\n    0x040d: \"he_IL\", # Hebrew\n    0x0439: \"hi_IN\", # Hindi\n    0x040e: \"hu_HU\", # Hungarian\n    0x040f: \"is_IS\", # Icelandic\n    0x0421: \"id_ID\", # Indonesian\n    0x045d: \"iu_CA\", # Inuktitut - Syllabics\n    0x085d: \"iu_CA\", # Inuktitut - Latin\n    0x083c: \"ga_IE\", # Irish - Ireland\n    0x0410: \"it_IT\", # Italian - Italy\n    0x0810: \"it_CH\", # Italian - Switzerland\n    0x0411: \"ja_JP\", # Japanese\n    0x044b: \"kn_IN\", # Kannada - India\n    0x043f: \"kk_KZ\", # Kazakh\n    0x0453: \"kh_KH\", # Khmer - Cambodia\n    0x0486: \"qut_GT\",# K'iche - Guatemala\n    0x0487: \"rw_RW\", # Kinyarwanda - Rwanda\n    0x0457: \"kok_IN\",# Konkani\n    0x0412: \"ko_KR\", # Korean\n    0x0440: \"ky_KG\", # Kyrgyz\n    0x0454: \"lo_LA\", # Lao - Lao PDR\n    0x0426: \"lv_LV\", # Latvian\n    0x0427: \"lt_LT\", # Lithuanian\n    0x082e: \"dsb_DE\",# Lower Sorbian - Germany\n    0x046e: \"lb_LU\", # Luxembourgish\n    0x042f: \"mk_MK\", # FYROM Macedonian\n    0x043e: \"ms_MY\", # Malay - Malaysia\n    0x083e: \"ms_BN\", # Malay - Brunei Darussalam\n    0x044c: \"ml_IN\", # Malayalam - India\n    0x043a: \"mt_MT\", # Maltese\n    0x0481: \"mi_NZ\", # Maori\n    0x047a: \"arn_CL\",# Mapudungun\n    0x044e: \"mr_IN\", # Marathi\n    0x047c: \"moh_CA\",# Mohawk - Canada\n    0x0450: \"mn_MN\", # Mongolian - Cyrillic\n    0x0850: \"mn_CN\", # Mongolian - PRC\n    0x0461: \"ne_NP\", # Nepali\n    0x0414: \"nb_NO\", # Norwegian - Bokmal\n    0x0814: \"nn_NO\", # Norwegian - Nynorsk\n    0x0482: \"oc_FR\", # Occitan - France\n    0x0448: \"or_IN\", # Oriya - India\n    0x0463: \"ps_AF\", # Pashto - Afghanistan\n    0x0429: \"fa_IR\", # Persian\n    0x0415: \"pl_PL\", # Polish\n    0x0416: \"pt_BR\", # Portuguese - Brazil\n    0x0816: \"pt_PT\", # Portuguese - Portugal\n    0x0446: \"pa_IN\", # Punjabi\n    0x046b: \"quz_BO\",# Quechua (Bolivia)\n    0x086b: \"quz_EC\",# Quechua (Ecuador)\n    0x0c6b: \"quz_PE\",# Quechua (Peru)\n    0x0418: \"ro_RO\", # Romanian - Romania\n    0x0417: \"rm_CH\", # Romansh\n    0x0419: \"ru_RU\", # Russian\n    0x243b: \"smn_FI\",# Sami Finland\n    0x103b: \"smj_NO\",# Sami Norway\n    0x143b: \"smj_SE\",# Sami Sweden\n    0x043b: \"se_NO\", # Sami Northern Norway\n    0x083b: \"se_SE\", # Sami Northern Sweden\n    0x0c3b: \"se_FI\", # Sami Northern Finland\n    0x203b: \"sms_FI\",# Sami Skolt\n    0x183b: \"sma_NO\",# Sami Southern Norway\n    0x1c3b: \"sma_SE\",# Sami Southern Sweden\n    0x044f: \"sa_IN\", # Sanskrit\n    0x0c1a: \"sr_SP\", # Serbian - Cyrillic\n    0x1c1a: \"sr_BA\", # Serbian - Bosnia Cyrillic\n    0x081a: \"sr_SP\", # Serbian - Latin\n    0x181a: \"sr_BA\", # Serbian - Bosnia Latin\n    0x045b: \"si_LK\", # Sinhala - Sri Lanka\n    0x046c: \"ns_ZA\", # Northern Sotho\n    0x0432: \"tn_ZA\", # Setswana - Southern Africa\n    0x041b: \"sk_SK\", # Slovak\n    0x0424: \"sl_SI\", # Slovenian\n    0x040a: \"es_ES\", # Spanish - Spain\n    0x080a: \"es_MX\", # Spanish - Mexico\n    0x0c0a: \"es_ES\", # Spanish - Spain (Modern)\n    0x100a: \"es_GT\", # Spanish - Guatemala\n    0x140a: \"es_CR\", # Spanish - Costa Rica\n    0x180a: \"es_PA\", # Spanish - Panama\n    0x1c0a: \"es_DO\", # Spanish - Dominican Republic\n    0x200a: \"es_VE\", # Spanish - Venezuela\n    0x240a: \"es_CO\", # Spanish - Colombia\n    0x280a: \"es_PE\", # Spanish - Peru\n    0x2c0a: \"es_AR\", # Spanish - Argentina\n    0x300a: \"es_EC\", # Spanish - Ecuador\n    0x340a: \"es_CL\", # Spanish - Chile\n    0x380a: \"es_UR\", # Spanish - Uruguay\n    0x3c0a: \"es_PY\", # Spanish - Paraguay\n    0x400a: \"es_BO\", # Spanish - Bolivia\n    0x440a: \"es_SV\", # Spanish - El Salvador\n    0x480a: \"es_HN\", # Spanish - Honduras\n    0x4c0a: \"es_NI\", # Spanish - Nicaragua\n    0x500a: \"es_PR\", # Spanish - Puerto Rico\n    0x540a: \"es_US\", # Spanish - United States\n#    0x0430: \"\", # Sutu - Not supported\n    0x0441: \"sw_KE\", # Swahili\n    0x041d: \"sv_SE\", # Swedish - Sweden\n    0x081d: \"sv_FI\", # Swedish - Finland\n    0x045a: \"syr_SY\",# Syriac\n    0x0428: \"tg_TJ\", # Tajik - Cyrillic\n    0x085f: \"tmz_DZ\",# Tamazight - Latin\n    0x0449: \"ta_IN\", # Tamil\n    0x0444: \"tt_RU\", # Tatar\n    0x044a: \"te_IN\", # Telugu\n    0x041e: \"th_TH\", # Thai\n    0x0851: \"bo_BT\", # Tibetan - Bhutan\n    0x0451: \"bo_CN\", # Tibetan - PRC\n    0x041f: \"tr_TR\", # Turkish\n    0x0442: \"tk_TM\", # Turkmen - Cyrillic\n    0x0480: \"ug_CN\", # Uighur - Arabic\n    0x0422: \"uk_UA\", # Ukrainian\n    0x042e: \"wen_DE\",# Upper Sorbian - Germany\n    0x0420: \"ur_PK\", # Urdu\n    0x0820: \"ur_IN\", # Urdu - India\n    0x0443: \"uz_UZ\", # Uzbek - Latin\n    0x0843: \"uz_UZ\", # Uzbek - Cyrillic\n    0x042a: \"vi_VN\", # Vietnamese\n    0x0452: \"cy_GB\", # Welsh\n    0x0488: \"wo_SN\", # Wolof - Senegal\n    0x0434: \"xh_ZA\", # Xhosa - South Africa\n    0x0485: \"sah_RU\",# Yakut - Cyrillic\n    0x0478: \"ii_CN\", # Yi - PRC\n    0x046a: \"yo_NG\", # Yoruba - Nigeria\n    0x0435: \"zu_ZA\", # Zulu\n}\n\ndef _print_locale():\n\n    \"\"\" Test function.\n    \"\"\"\n    categories = {}\n    def _init_categories(categories=categories):\n        for k,v in globals().items():\n            if k[:3] == 'LC_':\n                categories[k] = v\n    _init_categories()\n    del categories['LC_ALL']\n\n    print 'Locale defaults as determined by getdefaultlocale():'\n    print '-'*72\n    lang, enc = getdefaultlocale()\n    print 'Language: ', lang or '(undefined)'\n    print 'Encoding: ', enc or '(undefined)'\n    print\n\n    print 'Locale settings on startup:'\n    print '-'*72\n    for name,category in categories.items():\n        print name, '...'\n        lang, enc = getlocale(category)\n        print '   Language: ', lang or '(undefined)'\n        print '   Encoding: ', enc or '(undefined)'\n        print\n\n    print\n    print 'Locale settings after calling resetlocale():'\n    print '-'*72\n    resetlocale()\n    for name,category in categories.items():\n        print name, '...'\n        lang, enc = getlocale(category)\n        print '   Language: ', lang or '(undefined)'\n        print '   Encoding: ', enc or '(undefined)'\n        print\n\n    try:\n        setlocale(LC_ALL, \"\")\n    except:\n        print 'NOTE:'\n        print 'setlocale(LC_ALL, \"\") does not support the default locale'\n        print 'given in the OS environment variables.'\n    else:\n        print\n        print 'Locale settings after calling setlocale(LC_ALL, \"\"):'\n        print '-'*72\n        for name,category in categories.items():\n            print name, '...'\n            lang, enc = getlocale(category)\n            print '   Language: ', lang or '(undefined)'\n            print '   Encoding: ', enc or '(undefined)'\n            print\n\n###\n\ntry:\n    LC_MESSAGES\nexcept NameError:\n    pass\nelse:\n    __all__.append(\"LC_MESSAGES\")\n\nif __name__=='__main__':\n    print 'Locale aliasing:'\n    print\n    _print_locale()\n    print\n    print 'Number formatting:'\n    print\n    _test()\n",
		"file_name": "locale.py"
	},
	{
		"content": "\"\"\"functools.py - Tools for working with functions and callable objects\n\"\"\"\n# Python module wrapper for _functools C module\n# to allow utilities written in Python to be added\n# to the functools module.\n# Written by Nick Coghlan <ncoghlan at gmail.com>\n#   Copyright (C) 2006 Python Software Foundation.\n# See C source code for _functools credits/copyright\n\nfrom _functools import partial, reduce\n\n# update_wrapper() and wraps() are tools to help write\n# wrapper functions that can handle naive introspection\n\nWRAPPER_ASSIGNMENTS = ('__module__', '__name__', '__doc__')\nWRAPPER_UPDATES = ('__dict__',)\ndef update_wrapper(wrapper,\n                   wrapped,\n                   assigned = WRAPPER_ASSIGNMENTS,\n                   updated = WRAPPER_UPDATES):\n    \"\"\"Update a wrapper function to look like the wrapped function\n\n       wrapper is the function to be updated\n       wrapped is the original function\n       assigned is a tuple naming the attributes assigned directly\n       from the wrapped function to the wrapper function (defaults to\n       functools.WRAPPER_ASSIGNMENTS)\n       updated is a tuple naming the attributes of the wrapper that\n       are updated with the corresponding attribute from the wrapped\n       function (defaults to functools.WRAPPER_UPDATES)\n    \"\"\"\n    for attr in assigned:\n        setattr(wrapper, attr, getattr(wrapped, attr))\n    for attr in updated:\n        getattr(wrapper, attr).update(getattr(wrapped, attr, {}))\n    # Return the wrapper so this can be used as a decorator via partial()\n    return wrapper\n\ndef wraps(wrapped,\n          assigned = WRAPPER_ASSIGNMENTS,\n          updated = WRAPPER_UPDATES):\n    \"\"\"Decorator factory to apply update_wrapper() to a wrapper function\n\n       Returns a decorator that invokes update_wrapper() with the decorated\n       function as the wrapper argument and the arguments to wraps() as the\n       remaining arguments. Default arguments are as for update_wrapper().\n       This is a convenience function to simplify applying partial() to\n       update_wrapper().\n    \"\"\"\n    return partial(update_wrapper, wrapped=wrapped,\n                   assigned=assigned, updated=updated)\n\ndef total_ordering(cls):\n    \"\"\"Class decorator that fills in missing ordering methods\"\"\"\n    convert = {\n        '__lt__': [('__gt__', lambda self, other: not (self < other or self == other)),\n                   ('__le__', lambda self, other: self < other or self == other),\n                   ('__ge__', lambda self, other: not self < other)],\n        '__le__': [('__ge__', lambda self, other: not self <= other or self == other),\n                   ('__lt__', lambda self, other: self <= other and not self == other),\n                   ('__gt__', lambda self, other: not self <= other)],\n        '__gt__': [('__lt__', lambda self, other: not (self > other or self == other)),\n                   ('__ge__', lambda self, other: self > other or self == other),\n                   ('__le__', lambda self, other: not self > other)],\n        '__ge__': [('__le__', lambda self, other: (not self >= other) or self == other),\n                   ('__gt__', lambda self, other: self >= other and not self == other),\n                   ('__lt__', lambda self, other: not self >= other)]\n    }\n    roots = set(dir(cls)) & set(convert)\n    if not roots:\n        raise ValueError('must define at least one ordering operation: < > <= >=')\n    root = max(roots)       # prefer __lt__ to __le__ to __gt__ to __ge__\n    for opname, opfunc in convert[root]:\n        if opname not in roots:\n            opfunc.__name__ = opname\n            opfunc.__doc__ = getattr(int, opname).__doc__\n            setattr(cls, opname, opfunc)\n    return cls\n\ndef cmp_to_key(mycmp):\n    \"\"\"Convert a cmp= function into a key= function\"\"\"\n    class K(object):\n        __slots__ = ['obj']\n        def __init__(self, obj, *args):\n            self.obj = obj\n        def __lt__(self, other):\n            return mycmp(self.obj, other.obj) < 0\n        def __gt__(self, other):\n            return mycmp(self.obj, other.obj) > 0\n        def __eq__(self, other):\n            return mycmp(self.obj, other.obj) == 0\n        def __le__(self, other):\n            return mycmp(self.obj, other.obj) <= 0\n        def __ge__(self, other):\n            return mycmp(self.obj, other.obj) >= 0\n        def __ne__(self, other):\n            return mycmp(self.obj, other.obj) != 0\n        def __hash__(self):\n            raise TypeError('hash not implemented')\n    return K\n",
		"file_name": "functools.py"
	},
	{
		"content": "\"\"\" Supplies the internal functions for functools.py in the standard library \"\"\"\n\n# reduce() has moved to _functools in Python 2.6+.\nreduce = reduce\n\nclass partial(object):\n    \"\"\"\n    partial(func, *args, **keywords) - new function with partial application\n    of the given arguments and keywords.\n    \"\"\"\n\n    def __init__(self, *args, **keywords):\n        if not args:\n            raise TypeError('__init__() takes at least 2 arguments (1 given)')\n        func, args = args[0], args[1:]\n        if not callable(func):\n            raise TypeError(\"the first argument must be callable\")\n        self._func = func\n        self._args = args\n        self._keywords = keywords or None\n\n    def __delattr__(self, key):\n        if key == '__dict__':\n            raise TypeError(\"a partial object's dictionary may not be deleted\")\n        object.__delattr__(self, key)\n\n    @property\n    def func(self):\n        return self._func\n\n    @property\n    def args(self):\n        return self._args\n\n    @property\n    def keywords(self):\n        return self._keywords\n\n    def __call__(self, *fargs, **fkeywords):\n        if self.keywords is not None:\n            fkeywords = dict(self.keywords, **fkeywords)\n        return self.func(*(self.args + fargs), **fkeywords)\n\n    def __reduce__(self):\n        d = dict((k, v) for k, v in self.__dict__.iteritems() if k not in\n                ('_func', '_args', '_keywords'))\n        if len(d) == 0:\n            d = None\n        return (type(self), (self.func,),\n                (self.func, self.args, self.keywords, d))\n\n    def __setstate__(self, state):\n        self._func, self._args, self._keywords, d = state\n        if d is not None:\n            self.__dict__.update(d)\n",
		"file_name": "_functools.py"
	},
	{
		"content": "\"\"\"A powerful, extensible, and easy-to-use option parser.\n\nBy Greg Ward <gward@python.net>\n\nOriginally distributed as Optik.\n\nFor support, use the optik-users@lists.sourceforge.net mailing list\n(http://lists.sourceforge.net/lists/listinfo/optik-users).\n\nSimple usage example:\n\n   from optparse import OptionParser\n\n   parser = OptionParser()\n   parser.add_option(\"-f\", \"--file\", dest=\"filename\",\n                     help=\"write report to FILE\", metavar=\"FILE\")\n   parser.add_option(\"-q\", \"--quiet\",\n                     action=\"store_false\", dest=\"verbose\", default=True,\n                     help=\"don't print status messages to stdout\")\n\n   (options, args) = parser.parse_args()\n\"\"\"\n\n__version__ = \"1.5.3\"\n\n__all__ = ['Option',\n           'make_option',\n           'SUPPRESS_HELP',\n           'SUPPRESS_USAGE',\n           'Values',\n           'OptionContainer',\n           'OptionGroup',\n           'OptionParser',\n           'HelpFormatter',\n           'IndentedHelpFormatter',\n           'TitledHelpFormatter',\n           'OptParseError',\n           'OptionError',\n           'OptionConflictError',\n           'OptionValueError',\n           'BadOptionError']\n\n__copyright__ = \"\"\"\nCopyright (c) 2001-2006 Gregory P. Ward.  All rights reserved.\nCopyright (c) 2002-2006 Python Software Foundation.  All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n  * Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n\n  * Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in the\n    documentation and/or other materials provided with the distribution.\n\n  * Neither the name of the author nor the names of its\n    contributors may be used to endorse or promote products derived from\n    this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\nIS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\nTO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\nPARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\"\"\"\n\nimport sys, os\nimport types\nimport textwrap\n\ndef _repr(self):\n    return \"<%s at 0x%x: %s>\" % (self.__class__.__name__, id(self), self)\n\n\n# This file was generated from:\n#   Id: option_parser.py 527 2006-07-23 15:21:30Z greg\n#   Id: option.py 522 2006-06-11 16:22:03Z gward\n#   Id: help.py 527 2006-07-23 15:21:30Z greg\n#   Id: errors.py 509 2006-04-20 00:58:24Z gward\n\ntry:\n    from gettext import gettext\nexcept ImportError:\n    def gettext(message):\n        return message\n_ = gettext\n\n\nclass OptParseError (Exception):\n    def __init__(self, msg):\n        self.msg = msg\n\n    def __str__(self):\n        return self.msg\n\n\nclass OptionError (OptParseError):\n    \"\"\"\n    Raised if an Option instance is created with invalid or\n    inconsistent arguments.\n    \"\"\"\n\n    def __init__(self, msg, option):\n        self.msg = msg\n        self.option_id = str(option)\n\n    def __str__(self):\n        if self.option_id:\n            return \"option %s: %s\" % (self.option_id, self.msg)\n        else:\n            return self.msg\n\nclass OptionConflictError (OptionError):\n    \"\"\"\n    Raised if conflicting options are added to an OptionParser.\n    \"\"\"\n\nclass OptionValueError (OptParseError):\n    \"\"\"\n    Raised if an invalid option value is encountered on the command\n    line.\n    \"\"\"\n\nclass BadOptionError (OptParseError):\n    \"\"\"\n    Raised if an invalid option is seen on the command line.\n    \"\"\"\n    def __init__(self, opt_str):\n        self.opt_str = opt_str\n\n    def __str__(self):\n        return _(\"no such option: %s\") % self.opt_str\n\nclass AmbiguousOptionError (BadOptionError):\n    \"\"\"\n    Raised if an ambiguous option is seen on the command line.\n    \"\"\"\n    def __init__(self, opt_str, possibilities):\n        BadOptionError.__init__(self, opt_str)\n        self.possibilities = possibilities\n\n    def __str__(self):\n        return (_(\"ambiguous option: %s (%s?)\")\n                % (self.opt_str, \", \".join(self.possibilities)))\n\n\nclass HelpFormatter:\n\n    \"\"\"\n    Abstract base class for formatting option help.  OptionParser\n    instances should use one of the HelpFormatter subclasses for\n    formatting help; by default IndentedHelpFormatter is used.\n\n    Instance attributes:\n      parser : OptionParser\n        the controlling OptionParser instance\n      indent_increment : int\n        the number of columns to indent per nesting level\n      max_help_position : int\n        the maximum starting column for option help text\n      help_position : int\n        the calculated starting column for option help text;\n        initially the same as the maximum\n      width : int\n        total number of columns for output (pass None to constructor for\n        this value to be taken from the $COLUMNS environment variable)\n      level : int\n        current indentation level\n      current_indent : int\n        current indentation level (in columns)\n      help_width : int\n        number of columns available for option help text (calculated)\n      default_tag : str\n        text to replace with each option's default value, \"%default\"\n        by default.  Set to false value to disable default value expansion.\n      option_strings : { Option : str }\n        maps Option instances to the snippet of help text explaining\n        the syntax of that option, e.g. \"-h, --help\" or\n        \"-fFILE, --file=FILE\"\n      _short_opt_fmt : str\n        format string controlling how short options with values are\n        printed in help text.  Must be either \"%s%s\" (\"-fFILE\") or\n        \"%s %s\" (\"-f FILE\"), because those are the two syntaxes that\n        Optik supports.\n      _long_opt_fmt : str\n        similar but for long options; must be either \"%s %s\" (\"--file FILE\")\n        or \"%s=%s\" (\"--file=FILE\").\n    \"\"\"\n\n    NO_DEFAULT_VALUE = \"none\"\n\n    def __init__(self,\n                 indent_increment,\n                 max_help_position,\n                 width,\n                 short_first):\n        self.parser = None\n        self.indent_increment = indent_increment\n        if width is None:\n            try:\n                width = int(os.environ['COLUMNS'])\n            except (KeyError, ValueError):\n                width = 80\n            width -= 2\n        self.width = width\n        self.help_position = self.max_help_position = \\\n                min(max_help_position, max(width - 20, indent_increment * 2))\n        self.current_indent = 0\n        self.level = 0\n        self.help_width = None          # computed later\n        self.short_first = short_first\n        self.default_tag = \"%default\"\n        self.option_strings = {}\n        self._short_opt_fmt = \"%s %s\"\n        self._long_opt_fmt = \"%s=%s\"\n\n    def set_parser(self, parser):\n        self.parser = parser\n\n    def set_short_opt_delimiter(self, delim):\n        if delim not in (\"\", \" \"):\n            raise ValueError(\n                \"invalid metavar delimiter for short options: %r\" % delim)\n        self._short_opt_fmt = \"%s\" + delim + \"%s\"\n\n    def set_long_opt_delimiter(self, delim):\n        if delim not in (\"=\", \" \"):\n            raise ValueError(\n                \"invalid metavar delimiter for long options: %r\" % delim)\n        self._long_opt_fmt = \"%s\" + delim + \"%s\"\n\n    def indent(self):\n        self.current_indent += self.indent_increment\n        self.level += 1\n\n    def dedent(self):\n        self.current_indent -= self.indent_increment\n        assert self.current_indent >= 0, \"Indent decreased below 0.\"\n        self.level -= 1\n\n    def format_usage(self, usage):\n        raise NotImplementedError, \"subclasses must implement\"\n\n    def format_heading(self, heading):\n        raise NotImplementedError, \"subclasses must implement\"\n\n    def _format_text(self, text):\n        \"\"\"\n        Format a paragraph of free-form text for inclusion in the\n        help output at the current indentation level.\n        \"\"\"\n        text_width = max(self.width - self.current_indent, 11)\n        indent = \" \"*self.current_indent\n        return textwrap.fill(text,\n                             text_width,\n                             initial_indent=indent,\n                             subsequent_indent=indent)\n\n    def format_description(self, description):\n        if description:\n            return self._format_text(description) + \"\\n\"\n        else:\n            return \"\"\n\n    def format_epilog(self, epilog):\n        if epilog:\n            return \"\\n\" + self._format_text(epilog) + \"\\n\"\n        else:\n            return \"\"\n\n\n    def expand_default(self, option):\n        if self.parser is None or not self.default_tag:\n            return option.help\n\n        default_value = self.parser.defaults.get(option.dest)\n        if default_value is NO_DEFAULT or default_value is None:\n            default_value = self.NO_DEFAULT_VALUE\n\n        return option.help.replace(self.default_tag, str(default_value))\n\n    def format_option(self, option):\n        # The help for each option consists of two parts:\n        #   * the opt strings and metavars\n        #     eg. (\"-x\", or \"-fFILENAME, --file=FILENAME\")\n        #   * the user-supplied help string\n        #     eg. (\"turn on expert mode\", \"read data from FILENAME\")\n        #\n        # If possible, we write both of these on the same line:\n        #   -x      turn on expert mode\n        #\n        # But if the opt string list is too long, we put the help\n        # string on a second line, indented to the same column it would\n        # start in if it fit on the first line.\n        #   -fFILENAME, --file=FILENAME\n        #           read data from FILENAME\n        result = []\n        opts = self.option_strings[option]\n        opt_width = self.help_position - self.current_indent - 2\n        if len(opts) > opt_width:\n            opts = \"%*s%s\\n\" % (self.current_indent, \"\", opts)\n            indent_first = self.help_position\n        else:                       # start help on same line as opts\n            opts = \"%*s%-*s  \" % (self.current_indent, \"\", opt_width, opts)\n            indent_first = 0\n        result.append(opts)\n        if option.help:\n            help_text = self.expand_default(option)\n            help_lines = textwrap.wrap(help_text, self.help_width)\n            result.append(\"%*s%s\\n\" % (indent_first, \"\", help_lines[0]))\n            result.extend([\"%*s%s\\n\" % (self.help_position, \"\", line)\n                           for line in help_lines[1:]])\n        elif opts[-1] != \"\\n\":\n            result.append(\"\\n\")\n        return \"\".join(result)\n\n    def store_option_strings(self, parser):\n        self.indent()\n        max_len = 0\n        for opt in parser.option_list:\n            strings = self.format_option_strings(opt)\n            self.option_strings[opt] = strings\n            max_len = max(max_len, len(strings) + self.current_indent)\n        self.indent()\n        for group in parser.option_groups:\n            for opt in group.option_list:\n                strings = self.format_option_strings(opt)\n                self.option_strings[opt] = strings\n                max_len = max(max_len, len(strings) + self.current_indent)\n        self.dedent()\n        self.dedent()\n        self.help_position = min(max_len + 2, self.max_help_position)\n        self.help_width = max(self.width - self.help_position, 11)\n\n    def format_option_strings(self, option):\n        \"\"\"Return a comma-separated list of option strings & metavariables.\"\"\"\n        if option.takes_value():\n            metavar = option.metavar or option.dest.upper()\n            short_opts = [self._short_opt_fmt % (sopt, metavar)\n                          for sopt in option._short_opts]\n            long_opts = [self._long_opt_fmt % (lopt, metavar)\n                         for lopt in option._long_opts]\n        else:\n            short_opts = option._short_opts\n            long_opts = option._long_opts\n\n        if self.short_first:\n            opts = short_opts + long_opts\n        else:\n            opts = long_opts + short_opts\n\n        return \", \".join(opts)\n\nclass IndentedHelpFormatter (HelpFormatter):\n    \"\"\"Format help with indented section bodies.\n    \"\"\"\n\n    def __init__(self,\n                 indent_increment=2,\n                 max_help_position=24,\n                 width=None,\n                 short_first=1):\n        HelpFormatter.__init__(\n            self, indent_increment, max_help_position, width, short_first)\n\n    def format_usage(self, usage):\n        return _(\"Usage: %s\\n\") % usage\n\n    def format_heading(self, heading):\n        return \"%*s%s:\\n\" % (self.current_indent, \"\", heading)\n\n\nclass TitledHelpFormatter (HelpFormatter):\n    \"\"\"Format help with underlined section headers.\n    \"\"\"\n\n    def __init__(self,\n                 indent_increment=0,\n                 max_help_position=24,\n                 width=None,\n                 short_first=0):\n        HelpFormatter.__init__ (\n            self, indent_increment, max_help_position, width, short_first)\n\n    def format_usage(self, usage):\n        return \"%s  %s\\n\" % (self.format_heading(_(\"Usage\")), usage)\n\n    def format_heading(self, heading):\n        return \"%s\\n%s\\n\" % (heading, \"=-\"[self.level] * len(heading))\n\n\ndef _parse_num(val, type):\n    if val[:2].lower() == \"0x\":         # hexadecimal\n        radix = 16\n    elif val[:2].lower() == \"0b\":       # binary\n        radix = 2\n        val = val[2:] or \"0\"            # have to remove \"0b\" prefix\n    elif val[:1] == \"0\":                # octal\n        radix = 8\n    else:                               # decimal\n        radix = 10\n\n    return type(val, radix)\n\ndef _parse_int(val):\n    return _parse_num(val, int)\n\ndef _parse_long(val):\n    return _parse_num(val, long)\n\n_builtin_cvt = { \"int\" : (_parse_int, _(\"integer\")),\n                 \"long\" : (_parse_long, _(\"long integer\")),\n                 \"float\" : (float, _(\"floating-point\")),\n                 \"complex\" : (complex, _(\"complex\")) }\n\ndef check_builtin(option, opt, value):\n    (cvt, what) = _builtin_cvt[option.type]\n    try:\n        return cvt(value)\n    except ValueError:\n        raise OptionValueError(\n            _(\"option %s: invalid %s value: %r\") % (opt, what, value))\n\ndef check_choice(option, opt, value):\n    if value in option.choices:\n        return value\n    else:\n        choices = \", \".join(map(repr, option.choices))\n        raise OptionValueError(\n            _(\"option %s: invalid choice: %r (choose from %s)\")\n            % (opt, value, choices))\n\n# Not supplying a default is different from a default of None,\n# so we need an explicit \"not supplied\" value.\nNO_DEFAULT = (\"NO\", \"DEFAULT\")\n\n\nclass Option:\n    \"\"\"\n    Instance attributes:\n      _short_opts : [string]\n      _long_opts : [string]\n\n      action : string\n      type : string\n      dest : string\n      default : any\n      nargs : int\n      const : any\n      choices : [string]\n      callback : function\n      callback_args : (any*)\n      callback_kwargs : { string : any }\n      help : string\n      metavar : string\n    \"\"\"\n\n    # The list of instance attributes that may be set through\n    # keyword args to the constructor.\n    ATTRS = ['action',\n             'type',\n             'dest',\n             'default',\n             'nargs',\n             'const',\n             'choices',\n             'callback',\n             'callback_args',\n             'callback_kwargs',\n             'help',\n             'metavar']\n\n    # The set of actions allowed by option parsers.  Explicitly listed\n    # here so the constructor can validate its arguments.\n    ACTIONS = (\"store\",\n               \"store_const\",\n               \"store_true\",\n               \"store_false\",\n               \"append\",\n               \"append_const\",\n               \"count\",\n               \"callback\",\n               \"help\",\n               \"version\")\n\n    # The set of actions that involve storing a value somewhere;\n    # also listed just for constructor argument validation.  (If\n    # the action is one of these, there must be a destination.)\n    STORE_ACTIONS = (\"store\",\n                     \"store_const\",\n                     \"store_true\",\n                     \"store_false\",\n                     \"append\",\n                     \"append_const\",\n                     \"count\")\n\n    # The set of actions for which it makes sense to supply a value\n    # type, ie. which may consume an argument from the command line.\n    TYPED_ACTIONS = (\"store\",\n                     \"append\",\n                     \"callback\")\n\n    # The set of actions which *require* a value type, ie. that\n    # always consume an argument from the command line.\n    ALWAYS_TYPED_ACTIONS = (\"store\",\n                            \"append\")\n\n    # The set of actions which take a 'const' attribute.\n    CONST_ACTIONS = (\"store_const\",\n                     \"append_const\")\n\n    # The set of known types for option parsers.  Again, listed here for\n    # constructor argument validation.\n    TYPES = (\"string\", \"int\", \"long\", \"float\", \"complex\", \"choice\")\n\n    # Dictionary of argument checking functions, which convert and\n    # validate option arguments according to the option type.\n    #\n    # Signature of checking functions is:\n    #   check(option : Option, opt : string, value : string) -> any\n    # where\n    #   option is the Option instance calling the checker\n    #   opt is the actual option seen on the command-line\n    #     (eg. \"-a\", \"--file\")\n    #   value is the option argument seen on the command-line\n    #\n    # The return value should be in the appropriate Python type\n    # for option.type -- eg. an integer if option.type == \"int\".\n    #\n    # If no checker is defined for a type, arguments will be\n    # unchecked and remain strings.\n    TYPE_CHECKER = { \"int\"    : check_builtin,\n                     \"long\"   : check_builtin,\n                     \"float\"  : check_builtin,\n                     \"complex\": check_builtin,\n                     \"choice\" : check_choice,\n                   }\n\n\n    # CHECK_METHODS is a list of unbound method objects; they are called\n    # by the constructor, in order, after all attributes are\n    # initialized.  The list is created and filled in later, after all\n    # the methods are actually defined.  (I just put it here because I\n    # like to define and document all class attributes in the same\n    # place.)  Subclasses that add another _check_*() method should\n    # define their own CHECK_METHODS list that adds their check method\n    # to those from this class.\n    CHECK_METHODS = None\n\n\n    # -- Constructor/initialization methods ----------------------------\n\n    def __init__(self, *opts, **attrs):\n        # Set _short_opts, _long_opts attrs from 'opts' tuple.\n        # Have to be set now, in case no option strings are supplied.\n        self._short_opts = []\n        self._long_opts = []\n        opts = self._check_opt_strings(opts)\n        self._set_opt_strings(opts)\n\n        # Set all other attrs (action, type, etc.) from 'attrs' dict\n        self._set_attrs(attrs)\n\n        # Check all the attributes we just set.  There are lots of\n        # complicated interdependencies, but luckily they can be farmed\n        # out to the _check_*() methods listed in CHECK_METHODS -- which\n        # could be handy for subclasses!  The one thing these all share\n        # is that they raise OptionError if they discover a problem.\n        for checker in self.CHECK_METHODS:\n            checker(self)\n\n    def _check_opt_strings(self, opts):\n        # Filter out None because early versions of Optik had exactly\n        # one short option and one long option, either of which\n        # could be None.\n        opts = filter(None, opts)\n        if not opts:\n            raise TypeError(\"at least one option string must be supplied\")\n        return opts\n\n    def _set_opt_strings(self, opts):\n        for opt in opts:\n            if len(opt) < 2:\n                raise OptionError(\n                    \"invalid option string %r: \"\n                    \"must be at least two characters long\" % opt, self)\n            elif len(opt) == 2:\n                if not (opt[0] == \"-\" and opt[1] != \"-\"):\n                    raise OptionError(\n                        \"invalid short option string %r: \"\n                        \"must be of the form -x, (x any non-dash char)\" % opt,\n                        self)\n                self._short_opts.append(opt)\n            else:\n                if not (opt[0:2] == \"--\" and opt[2] != \"-\"):\n                    raise OptionError(\n                        \"invalid long option string %r: \"\n                        \"must start with --, followed by non-dash\" % opt,\n                        self)\n                self._long_opts.append(opt)\n\n    def _set_attrs(self, attrs):\n        for attr in self.ATTRS:\n            if attr in attrs:\n                setattr(self, attr, attrs[attr])\n                del attrs[attr]\n            else:\n                if attr == 'default':\n                    setattr(self, attr, NO_DEFAULT)\n                else:\n                    setattr(self, attr, None)\n        if attrs:\n            attrs = attrs.keys()\n            attrs.sort()\n            raise OptionError(\n                \"invalid keyword arguments: %s\" % \", \".join(attrs),\n                self)\n\n\n    # -- Constructor validation methods --------------------------------\n\n    def _check_action(self):\n        if self.action is None:\n            self.action = \"store\"\n        elif self.action not in self.ACTIONS:\n            raise OptionError(\"invalid action: %r\" % self.action, self)\n\n    def _check_type(self):\n        if self.type is None:\n            if self.action in self.ALWAYS_TYPED_ACTIONS:\n                if self.choices is not None:\n                    # The \"choices\" attribute implies \"choice\" type.\n                    self.type = \"choice\"\n                else:\n                    # No type given?  \"string\" is the most sensible default.\n                    self.type = \"string\"\n        else:\n            # Allow type objects or builtin type conversion functions\n            # (int, str, etc.) as an alternative to their names.  (The\n            # complicated check of __builtin__ is only necessary for\n            # Python 2.1 and earlier, and is short-circuited by the\n            # first check on modern Pythons.)\n            import __builtin__\n            if ( type(self.type) is types.TypeType or\n                 (hasattr(self.type, \"__name__\") and\n                  getattr(__builtin__, self.type.__name__, None) is self.type) ):\n                self.type = self.type.__name__\n\n            if self.type == \"str\":\n                self.type = \"string\"\n\n            if self.type not in self.TYPES:\n                raise OptionError(\"invalid option type: %r\" % self.type, self)\n            if self.action not in self.TYPED_ACTIONS:\n                raise OptionError(\n                    \"must not supply a type for action %r\" % self.action, self)\n\n    def _check_choice(self):\n        if self.type == \"choice\":\n            if self.choices is None:\n                raise OptionError(\n                    \"must supply a list of choices for type 'choice'\", self)\n            elif type(self.choices) not in (types.TupleType, types.ListType):\n                raise OptionError(\n                    \"choices must be a list of strings ('%s' supplied)\"\n                    % str(type(self.choices)).split(\"'\")[1], self)\n        elif self.choices is not None:\n            raise OptionError(\n                \"must not supply choices for type %r\" % self.type, self)\n\n    def _check_dest(self):\n        # No destination given, and we need one for this action.  The\n        # self.type check is for callbacks that take a value.\n        takes_value = (self.action in self.STORE_ACTIONS or\n                       self.type is not None)\n        if self.dest is None and takes_value:\n\n            # Glean a destination from the first long option string,\n            # or from the first short option string if no long options.\n            if self._long_opts:\n                # eg. \"--foo-bar\" -> \"foo_bar\"\n                self.dest = self._long_opts[0][2:].replace('-', '_')\n            else:\n                self.dest = self._short_opts[0][1]\n\n    def _check_const(self):\n        if self.action not in self.CONST_ACTIONS and self.const is not None:\n            raise OptionError(\n                \"'const' must not be supplied for action %r\" % self.action,\n                self)\n\n    def _check_nargs(self):\n        if self.action in self.TYPED_ACTIONS:\n            if self.nargs is None:\n                self.nargs = 1\n        elif self.nargs is not None:\n            raise OptionError(\n                \"'nargs' must not be supplied for action %r\" % self.action,\n                self)\n\n    def _check_callback(self):\n        if self.action == \"callback\":\n            if not hasattr(self.callback, '__call__'):\n                raise OptionError(\n                    \"callback not callable: %r\" % self.callback, self)\n            if (self.callback_args is not None and\n                type(self.callback_args) is not types.TupleType):\n                raise OptionError(\n                    \"callback_args, if supplied, must be a tuple: not %r\"\n                    % self.callback_args, self)\n            if (self.callback_kwargs is not None and\n                type(self.callback_kwargs) is not types.DictType):\n                raise OptionError(\n                    \"callback_kwargs, if supplied, must be a dict: not %r\"\n                    % self.callback_kwargs, self)\n        else:\n            if self.callback is not None:\n                raise OptionError(\n                    \"callback supplied (%r) for non-callback option\"\n                    % self.callback, self)\n            if self.callback_args is not None:\n                raise OptionError(\n                    \"callback_args supplied for non-callback option\", self)\n            if self.callback_kwargs is not None:\n                raise OptionError(\n                    \"callback_kwargs supplied for non-callback option\", self)\n\n\n    CHECK_METHODS = [_check_action,\n                     _check_type,\n                     _check_choice,\n                     _check_dest,\n                     _check_const,\n                     _check_nargs,\n                     _check_callback]\n\n\n    # -- Miscellaneous methods -----------------------------------------\n\n    def __str__(self):\n        return \"/\".join(self._short_opts + self._long_opts)\n\n    __repr__ = _repr\n\n    def takes_value(self):\n        return self.type is not None\n\n    def get_opt_string(self):\n        if self._long_opts:\n            return self._long_opts[0]\n        else:\n            return self._short_opts[0]\n\n\n    # -- Processing methods --------------------------------------------\n\n    def check_value(self, opt, value):\n        checker = self.TYPE_CHECKER.get(self.type)\n        if checker is None:\n            return value\n        else:\n            return checker(self, opt, value)\n\n    def convert_value(self, opt, value):\n        if value is not None:\n            if self.nargs == 1:\n                return self.check_value(opt, value)\n            else:\n                return tuple([self.check_value(opt, v) for v in value])\n\n    def process(self, opt, value, values, parser):\n\n        # First, convert the value(s) to the right type.  Howl if any\n        # value(s) are bogus.\n        value = self.convert_value(opt, value)\n\n        # And then take whatever action is expected of us.\n        # This is a separate method to make life easier for\n        # subclasses to add new actions.\n        return self.take_action(\n            self.action, self.dest, opt, value, values, parser)\n\n    def take_action(self, action, dest, opt, value, values, parser):\n        if action == \"store\":\n            setattr(values, dest, value)\n        elif action == \"store_const\":\n            setattr(values, dest, self.const)\n        elif action == \"store_true\":\n            setattr(values, dest, True)\n        elif action == \"store_false\":\n            setattr(values, dest, False)\n        elif action == \"append\":\n            values.ensure_value(dest, []).append(value)\n        elif action == \"append_const\":\n            values.ensure_value(dest, []).append(self.const)\n        elif action == \"count\":\n            setattr(values, dest, values.ensure_value(dest, 0) + 1)\n        elif action == \"callback\":\n            args = self.callback_args or ()\n            kwargs = self.callback_kwargs or {}\n            self.callback(self, opt, value, parser, *args, **kwargs)\n        elif action == \"help\":\n            parser.print_help()\n            parser.exit()\n        elif action == \"version\":\n            parser.print_version()\n            parser.exit()\n        else:\n            raise ValueError(\"unknown action %r\" % self.action)\n\n        return 1\n\n# class Option\n\n\nSUPPRESS_HELP = \"SUPPRESS\"+\"HELP\"\nSUPPRESS_USAGE = \"SUPPRESS\"+\"USAGE\"\n\ntry:\n    basestring\nexcept NameError:\n    def isbasestring(x):\n        return isinstance(x, (types.StringType, types.UnicodeType))\nelse:\n    def isbasestring(x):\n        return isinstance(x, basestring)\n\nclass Values:\n\n    def __init__(self, defaults=None):\n        if defaults:\n            for (attr, val) in defaults.items():\n                setattr(self, attr, val)\n\n    def __str__(self):\n        return str(self.__dict__)\n\n    __repr__ = _repr\n\n    def __cmp__(self, other):\n        if isinstance(other, Values):\n            return cmp(self.__dict__, other.__dict__)\n        elif isinstance(other, types.DictType):\n            return cmp(self.__dict__, other)\n        else:\n            return -1\n\n    def _update_careful(self, dict):\n        \"\"\"\n        Update the option values from an arbitrary dictionary, but only\n        use keys from dict that already have a corresponding attribute\n        in self.  Any keys in dict without a corresponding attribute\n        are silently ignored.\n        \"\"\"\n        for attr in dir(self):\n            if attr in dict:\n                dval = dict[attr]\n                if dval is not None:\n                    setattr(self, attr, dval)\n\n    def _update_loose(self, dict):\n        \"\"\"\n        Update the option values from an arbitrary dictionary,\n        using all keys from the dictionary regardless of whether\n        they have a corresponding attribute in self or not.\n        \"\"\"\n        self.__dict__.update(dict)\n\n    def _update(self, dict, mode):\n        if mode == \"careful\":\n            self._update_careful(dict)\n        elif mode == \"loose\":\n            self._update_loose(dict)\n        else:\n            raise ValueError, \"invalid update mode: %r\" % mode\n\n    def read_module(self, modname, mode=\"careful\"):\n        __import__(modname)\n        mod = sys.modules[modname]\n        self._update(vars(mod), mode)\n\n    def read_file(self, filename, mode=\"careful\"):\n        vars = {}\n        execfile(filename, vars)\n        self._update(vars, mode)\n\n    def ensure_value(self, attr, value):\n        if not hasattr(self, attr) or getattr(self, attr) is None:\n            setattr(self, attr, value)\n        return getattr(self, attr)\n\n\nclass OptionContainer:\n\n    \"\"\"\n    Abstract base class.\n\n    Class attributes:\n      standard_option_list : [Option]\n        list of standard options that will be accepted by all instances\n        of this parser class (intended to be overridden by subclasses).\n\n    Instance attributes:\n      option_list : [Option]\n        the list of Option objects contained by this OptionContainer\n      _short_opt : { string : Option }\n        dictionary mapping short option strings, eg. \"-f\" or \"-X\",\n        to the Option instances that implement them.  If an Option\n        has multiple short option strings, it will appears in this\n        dictionary multiple times. [1]\n      _long_opt : { string : Option }\n        dictionary mapping long option strings, eg. \"--file\" or\n        \"--exclude\", to the Option instances that implement them.\n        Again, a given Option can occur multiple times in this\n        dictionary. [1]\n      defaults : { string : any }\n        dictionary mapping option destination names to default\n        values for each destination [1]\n\n    [1] These mappings are common to (shared by) all components of the\n        controlling OptionParser, where they are initially created.\n\n    \"\"\"\n\n    def __init__(self, option_class, conflict_handler, description):\n        # Initialize the option list and related data structures.\n        # This method must be provided by subclasses, and it must\n        # initialize at least the following instance attributes:\n        # option_list, _short_opt, _long_opt, defaults.\n        self._create_option_list()\n\n        self.option_class = option_class\n        self.set_conflict_handler(conflict_handler)\n        self.set_description(description)\n\n    def _create_option_mappings(self):\n        # For use by OptionParser constructor -- create the master\n        # option mappings used by this OptionParser and all\n        # OptionGroups that it owns.\n        self._short_opt = {}            # single letter -> Option instance\n        self._long_opt = {}             # long option -> Option instance\n        self.defaults = {}              # maps option dest -> default value\n\n\n    def _share_option_mappings(self, parser):\n        # For use by OptionGroup constructor -- use shared option\n        # mappings from the OptionParser that owns this OptionGroup.\n        self._short_opt = parser._short_opt\n        self._long_opt = parser._long_opt\n        self.defaults = parser.defaults\n\n    def set_conflict_handler(self, handler):\n        if handler not in (\"error\", \"resolve\"):\n            raise ValueError, \"invalid conflict_resolution value %r\" % handler\n        self.conflict_handler = handler\n\n    def set_description(self, description):\n        self.description = description\n\n    def get_description(self):\n        return self.description\n\n\n    def destroy(self):\n        \"\"\"see OptionParser.destroy().\"\"\"\n        del self._short_opt\n        del self._long_opt\n        del self.defaults\n\n\n    # -- Option-adding methods -----------------------------------------\n\n    def _check_conflict(self, option):\n        conflict_opts = []\n        for opt in option._short_opts:\n            if opt in self._short_opt:\n                conflict_opts.append((opt, self._short_opt[opt]))\n        for opt in option._long_opts:\n            if opt in self._long_opt:\n                conflict_opts.append((opt, self._long_opt[opt]))\n\n        if conflict_opts:\n            handler = self.conflict_handler\n            if handler == \"error\":\n                raise OptionConflictError(\n                    \"conflicting option string(s): %s\"\n                    % \", \".join([co[0] for co in conflict_opts]),\n                    option)\n            elif handler == \"resolve\":\n                for (opt, c_option) in conflict_opts:\n                    if opt.startswith(\"--\"):\n                        c_option._long_opts.remove(opt)\n                        del self._long_opt[opt]\n                    else:\n                        c_option._short_opts.remove(opt)\n                        del self._short_opt[opt]\n                    if not (c_option._short_opts or c_option._long_opts):\n                        c_option.container.option_list.remove(c_option)\n\n    def add_option(self, *args, **kwargs):\n        \"\"\"add_option(Option)\n           add_option(opt_str, ..., kwarg=val, ...)\n        \"\"\"\n        if type(args[0]) in types.StringTypes:\n            option = self.option_class(*args, **kwargs)\n        elif len(args) == 1 and not kwargs:\n            option = args[0]\n            if not isinstance(option, Option):\n                raise TypeError, \"not an Option instance: %r\" % option\n        else:\n            raise TypeError, \"invalid arguments\"\n\n        self._check_conflict(option)\n\n        self.option_list.append(option)\n        option.container = self\n        for opt in option._short_opts:\n            self._short_opt[opt] = option\n        for opt in option._long_opts:\n            self._long_opt[opt] = option\n\n        if option.dest is not None:     # option has a dest, we need a default\n            if option.default is not NO_DEFAULT:\n                self.defaults[option.dest] = option.default\n            elif option.dest not in self.defaults:\n                self.defaults[option.dest] = None\n\n        return option\n\n    def add_options(self, option_list):\n        for option in option_list:\n            self.add_option(option)\n\n    # -- Option query/removal methods ----------------------------------\n\n    def get_option(self, opt_str):\n        return (self._short_opt.get(opt_str) or\n                self._long_opt.get(opt_str))\n\n    def has_option(self, opt_str):\n        return (opt_str in self._short_opt or\n                opt_str in self._long_opt)\n\n    def remove_option(self, opt_str):\n        option = self._short_opt.get(opt_str)\n        if option is None:\n            option = self._long_opt.get(opt_str)\n        if option is None:\n            raise ValueError(\"no such option %r\" % opt_str)\n\n        for opt in option._short_opts:\n            del self._short_opt[opt]\n        for opt in option._long_opts:\n            del self._long_opt[opt]\n        option.container.option_list.remove(option)\n\n\n    # -- Help-formatting methods ---------------------------------------\n\n    def format_option_help(self, formatter):\n        if not self.option_list:\n            return \"\"\n        result = []\n        for option in self.option_list:\n            if not option.help is SUPPRESS_HELP:\n                result.append(formatter.format_option(option))\n        return \"\".join(result)\n\n    def format_description(self, formatter):\n        return formatter.format_description(self.get_description())\n\n    def format_help(self, formatter):\n        result = []\n        if self.description:\n            result.append(self.format_description(formatter))\n        if self.option_list:\n            result.append(self.format_option_help(formatter))\n        return \"\\n\".join(result)\n\n\nclass OptionGroup (OptionContainer):\n\n    def __init__(self, parser, title, description=None):\n        self.parser = parser\n        OptionContainer.__init__(\n            self, parser.option_class, parser.conflict_handler, description)\n        self.title = title\n\n    def _create_option_list(self):\n        self.option_list = []\n        self._share_option_mappings(self.parser)\n\n    def set_title(self, title):\n        self.title = title\n\n    def destroy(self):\n        \"\"\"see OptionParser.destroy().\"\"\"\n        OptionContainer.destroy(self)\n        del self.option_list\n\n    # -- Help-formatting methods ---------------------------------------\n\n    def format_help(self, formatter):\n        result = formatter.format_heading(self.title)\n        formatter.indent()\n        result += OptionContainer.format_help(self, formatter)\n        formatter.dedent()\n        return result\n\n\nclass OptionParser (OptionContainer):\n\n    \"\"\"\n    Class attributes:\n      standard_option_list : [Option]\n        list of standard options that will be accepted by all instances\n        of this parser class (intended to be overridden by subclasses).\n\n    Instance attributes:\n      usage : string\n        a usage string for your program.  Before it is displayed\n        to the user, \"%prog\" will be expanded to the name of\n        your program (self.prog or os.path.basename(sys.argv[0])).\n      prog : string\n        the name of the current program (to override\n        os.path.basename(sys.argv[0])).\n      description : string\n        A paragraph of text giving a brief overview of your program.\n        optparse reformats this paragraph to fit the current terminal\n        width and prints it when the user requests help (after usage,\n        but before the list of options).\n      epilog : string\n        paragraph of help text to print after option help\n\n      option_groups : [OptionGroup]\n        list of option groups in this parser (option groups are\n        irrelevant for parsing the command-line, but very useful\n        for generating help)\n\n      allow_interspersed_args : bool = true\n        if true, positional arguments may be interspersed with options.\n        Assuming -a and -b each take a single argument, the command-line\n          -ablah foo bar -bboo baz\n        will be interpreted the same as\n          -ablah -bboo -- foo bar baz\n        If this flag were false, that command line would be interpreted as\n          -ablah -- foo bar -bboo baz\n        -- ie. we stop processing options as soon as we see the first\n        non-option argument.  (This is the tradition followed by\n        Python's getopt module, Perl's Getopt::Std, and other argument-\n        parsing libraries, but it is generally annoying to users.)\n\n      process_default_values : bool = true\n        if true, option default values are processed similarly to option\n        values from the command line: that is, they are passed to the\n        type-checking function for the option's type (as long as the\n        default value is a string).  (This really only matters if you\n        have defined custom types; see SF bug #955889.)  Set it to false\n        to restore the behaviour of Optik 1.4.1 and earlier.\n\n      rargs : [string]\n        the argument list currently being parsed.  Only set when\n        parse_args() is active, and continually trimmed down as\n        we consume arguments.  Mainly there for the benefit of\n        callback options.\n      largs : [string]\n        the list of leftover arguments that we have skipped while\n        parsing options.  If allow_interspersed_args is false, this\n        list is always empty.\n      values : Values\n        the set of option values currently being accumulated.  Only\n        set when parse_args() is active.  Also mainly for callbacks.\n\n    Because of the 'rargs', 'largs', and 'values' attributes,\n    OptionParser is not thread-safe.  If, for some perverse reason, you\n    need to parse command-line arguments simultaneously in different\n    threads, use different OptionParser instances.\n\n    \"\"\"\n\n    standard_option_list = []\n\n    def __init__(self,\n                 usage=None,\n                 option_list=None,\n                 option_class=Option,\n                 version=None,\n                 conflict_handler=\"error\",\n                 description=None,\n                 formatter=None,\n                 add_help_option=True,\n                 prog=None,\n                 epilog=None):\n        OptionContainer.__init__(\n            self, option_class, conflict_handler, description)\n        self.set_usage(usage)\n        self.prog = prog\n        self.version = version\n        self.allow_interspersed_args = True\n        self.process_default_values = True\n        if formatter is None:\n            formatter = IndentedHelpFormatter()\n        self.formatter = formatter\n        self.formatter.set_parser(self)\n        self.epilog = epilog\n\n        # Populate the option list; initial sources are the\n        # standard_option_list class attribute, the 'option_list'\n        # argument, and (if applicable) the _add_version_option() and\n        # _add_help_option() methods.\n        self._populate_option_list(option_list,\n                                   add_help=add_help_option)\n\n        self._init_parsing_state()\n\n\n    def destroy(self):\n        \"\"\"\n        Declare that you are done with this OptionParser.  This cleans up\n        reference cycles so the OptionParser (and all objects referenced by\n        it) can be garbage-collected promptly.  After calling destroy(), the\n        OptionParser is unusable.\n        \"\"\"\n        OptionContainer.destroy(self)\n        for group in self.option_groups:\n            group.destroy()\n        del self.option_list\n        del self.option_groups\n        del self.formatter\n\n\n    # -- Private methods -----------------------------------------------\n    # (used by our or OptionContainer's constructor)\n\n    def _create_option_list(self):\n        self.option_list = []\n        self.option_groups = []\n        self._create_option_mappings()\n\n    def _add_help_option(self):\n        self.add_option(\"-h\", \"--help\",\n                        action=\"help\",\n                        help=_(\"show this help message and exit\"))\n\n    def _add_version_option(self):\n        self.add_option(\"--version\",\n                        action=\"version\",\n                        help=_(\"show program's version number and exit\"))\n\n    def _populate_option_list(self, option_list, add_help=True):\n        if self.standard_option_list:\n            self.add_options(self.standard_option_list)\n        if option_list:\n            self.add_options(option_list)\n        if self.version:\n            self._add_version_option()\n        if add_help:\n            self._add_help_option()\n\n    def _init_parsing_state(self):\n        # These are set in parse_args() for the convenience of callbacks.\n        self.rargs = None\n        self.largs = None\n        self.values = None\n\n\n    # -- Simple modifier methods ---------------------------------------\n\n    def set_usage(self, usage):\n        if usage is None:\n            self.usage = _(\"%prog [options]\")\n        elif usage is SUPPRESS_USAGE:\n            self.usage = None\n        # For backwards compatibility with Optik 1.3 and earlier.\n        elif usage.lower().startswith(\"usage: \"):\n            self.usage = usage[7:]\n        else:\n            self.usage = usage\n\n    def enable_interspersed_args(self):\n        \"\"\"Set parsing to not stop on the first non-option, allowing\n        interspersing switches with command arguments. This is the\n        default behavior. See also disable_interspersed_args() and the\n        class documentation description of the attribute\n        allow_interspersed_args.\"\"\"\n        self.allow_interspersed_args = True\n\n    def disable_interspersed_args(self):\n        \"\"\"Set parsing to stop on the first non-option. Use this if\n        you have a command processor which runs another command that\n        has options of its own and you want to make sure these options\n        don't get confused.\n        \"\"\"\n        self.allow_interspersed_args = False\n\n    def set_process_default_values(self, process):\n        self.process_default_values = process\n\n    def set_default(self, dest, value):\n        self.defaults[dest] = value\n\n    def set_defaults(self, **kwargs):\n        self.defaults.update(kwargs)\n\n    def _get_all_options(self):\n        options = self.option_list[:]\n        for group in self.option_groups:\n            options.extend(group.option_list)\n        return options\n\n    def get_default_values(self):\n        if not self.process_default_values:\n            # Old, pre-Optik 1.5 behaviour.\n            return Values(self.defaults)\n\n        defaults = self.defaults.copy()\n        for option in self._get_all_options():\n            default = defaults.get(option.dest)\n            if isbasestring(default):\n                opt_str = option.get_opt_string()\n                defaults[option.dest] = option.check_value(opt_str, default)\n\n        return Values(defaults)\n\n\n    # -- OptionGroup methods -------------------------------------------\n\n    def add_option_group(self, *args, **kwargs):\n        # XXX lots of overlap with OptionContainer.add_option()\n        if type(args[0]) is types.StringType:\n            group = OptionGroup(self, *args, **kwargs)\n        elif len(args) == 1 and not kwargs:\n            group = args[0]\n            if not isinstance(group, OptionGroup):\n                raise TypeError, \"not an OptionGroup instance: %r\" % group\n            if group.parser is not self:\n                raise ValueError, \"invalid OptionGroup (wrong parser)\"\n        else:\n            raise TypeError, \"invalid arguments\"\n\n        self.option_groups.append(group)\n        return group\n\n    def get_option_group(self, opt_str):\n        option = (self._short_opt.get(opt_str) or\n                  self._long_opt.get(opt_str))\n        if option and option.container is not self:\n            return option.container\n        return None\n\n\n    # -- Option-parsing methods ----------------------------------------\n\n    def _get_args(self, args):\n        if args is None:\n            return sys.argv[1:]\n        else:\n            return args[:]              # don't modify caller's list\n\n    def parse_args(self, args=None, values=None):\n        \"\"\"\n        parse_args(args : [string] = sys.argv[1:],\n                   values : Values = None)\n        -> (values : Values, args : [string])\n\n        Parse the command-line options found in 'args' (default:\n        sys.argv[1:]).  Any errors result in a call to 'error()', which\n        by default prints the usage message to stderr and calls\n        sys.exit() with an error message.  On success returns a pair\n        (values, args) where 'values' is an Values instance (with all\n        your option values) and 'args' is the list of arguments left\n        over after parsing options.\n        \"\"\"\n        rargs = self._get_args(args)\n        if values is None:\n            values = self.get_default_values()\n\n        # Store the halves of the argument list as attributes for the\n        # convenience of callbacks:\n        #   rargs\n        #     the rest of the command-line (the \"r\" stands for\n        #     \"remaining\" or \"right-hand\")\n        #   largs\n        #     the leftover arguments -- ie. what's left after removing\n        #     options and their arguments (the \"l\" stands for \"leftover\"\n        #     or \"left-hand\")\n        self.rargs = rargs\n        self.largs = largs = []\n        self.values = values\n\n        try:\n            stop = self._process_args(largs, rargs, values)\n        except (BadOptionError, OptionValueError), err:\n            self.error(str(err))\n\n        args = largs + rargs\n        return self.check_values(values, args)\n\n    def check_values(self, values, args):\n        \"\"\"\n        check_values(values : Values, args : [string])\n        -> (values : Values, args : [string])\n\n        Check that the supplied option values and leftover arguments are\n        valid.  Returns the option values and leftover arguments\n        (possibly adjusted, possibly completely new -- whatever you\n        like).  Default implementation just returns the passed-in\n        values; subclasses may override as desired.\n        \"\"\"\n        return (values, args)\n\n    def _process_args(self, largs, rargs, values):\n        \"\"\"_process_args(largs : [string],\n                         rargs : [string],\n                         values : Values)\n\n        Process command-line arguments and populate 'values', consuming\n        options and arguments from 'rargs'.  If 'allow_interspersed_args' is\n        false, stop at the first non-option argument.  If true, accumulate any\n        interspersed non-option arguments in 'largs'.\n        \"\"\"\n        while rargs:\n            arg = rargs[0]\n            # We handle bare \"--\" explicitly, and bare \"-\" is handled by the\n            # standard arg handler since the short arg case ensures that the\n            # len of the opt string is greater than 1.\n            if arg == \"--\":\n                del rargs[0]\n                return\n            elif arg[0:2] == \"--\":\n                # process a single long option (possibly with value(s))\n                self._process_long_opt(rargs, values)\n            elif arg[:1] == \"-\" and len(arg) > 1:\n                # process a cluster of short options (possibly with\n                # value(s) for the last one only)\n                self._process_short_opts(rargs, values)\n            elif self.allow_interspersed_args:\n                largs.append(arg)\n                del rargs[0]\n            else:\n                return                  # stop now, leave this arg in rargs\n\n        # Say this is the original argument list:\n        # [arg0, arg1, ..., arg(i-1), arg(i), arg(i+1), ..., arg(N-1)]\n        #                            ^\n        # (we are about to process arg(i)).\n        #\n        # Then rargs is [arg(i), ..., arg(N-1)] and largs is a *subset* of\n        # [arg0, ..., arg(i-1)] (any options and their arguments will have\n        # been removed from largs).\n        #\n        # The while loop will usually consume 1 or more arguments per pass.\n        # If it consumes 1 (eg. arg is an option that takes no arguments),\n        # then after _process_arg() is done the situation is:\n        #\n        #   largs = subset of [arg0, ..., arg(i)]\n        #   rargs = [arg(i+1), ..., arg(N-1)]\n        #\n        # If allow_interspersed_args is false, largs will always be\n        # *empty* -- still a subset of [arg0, ..., arg(i-1)], but\n        # not a very interesting subset!\n\n    def _match_long_opt(self, opt):\n        \"\"\"_match_long_opt(opt : string) -> string\n\n        Determine which long option string 'opt' matches, ie. which one\n        it is an unambiguous abbreviation for.  Raises BadOptionError if\n        'opt' doesn't unambiguously match any long option string.\n        \"\"\"\n        return _match_abbrev(opt, self._long_opt)\n\n    def _process_long_opt(self, rargs, values):\n        arg = rargs.pop(0)\n\n        # Value explicitly attached to arg?  Pretend it's the next\n        # argument.\n        if \"=\" in arg:\n            (opt, next_arg) = arg.split(\"=\", 1)\n            rargs.insert(0, next_arg)\n            had_explicit_value = True\n        else:\n            opt = arg\n            had_explicit_value = False\n\n        opt = self._match_long_opt(opt)\n        option = self._long_opt[opt]\n        if option.takes_value():\n            nargs = option.nargs\n            if len(rargs) < nargs:\n                if nargs == 1:\n                    self.error(_(\"%s option requires an argument\") % opt)\n                else:\n                    self.error(_(\"%s option requires %d arguments\")\n                               % (opt, nargs))\n            elif nargs == 1:\n                value = rargs.pop(0)\n            else:\n                value = tuple(rargs[0:nargs])\n                del rargs[0:nargs]\n\n        elif had_explicit_value:\n            self.error(_(\"%s option does not take a value\") % opt)\n\n        else:\n            value = None\n\n        option.process(opt, value, values, self)\n\n    def _process_short_opts(self, rargs, values):\n        arg = rargs.pop(0)\n        stop = False\n        i = 1\n        for ch in arg[1:]:\n            opt = \"-\" + ch\n            option = self._short_opt.get(opt)\n            i += 1                      # we have consumed a character\n\n            if not option:\n                raise BadOptionError(opt)\n            if option.takes_value():\n                # Any characters left in arg?  Pretend they're the\n                # next arg, and stop consuming characters of arg.\n                if i < len(arg):\n                    rargs.insert(0, arg[i:])\n                    stop = True\n\n                nargs = option.nargs\n                if len(rargs) < nargs:\n                    if nargs == 1:\n                        self.error(_(\"%s option requires an argument\") % opt)\n                    else:\n                        self.error(_(\"%s option requires %d arguments\")\n                                   % (opt, nargs))\n                elif nargs == 1:\n                    value = rargs.pop(0)\n                else:\n                    value = tuple(rargs[0:nargs])\n                    del rargs[0:nargs]\n\n            else:                       # option doesn't take a value\n                value = None\n\n            option.process(opt, value, values, self)\n\n            if stop:\n                break\n\n\n    # -- Feedback methods ----------------------------------------------\n\n    def get_prog_name(self):\n        if self.prog is None:\n            return os.path.basename(sys.argv[0])\n        else:\n            return self.prog\n\n    def expand_prog_name(self, s):\n        return s.replace(\"%prog\", self.get_prog_name())\n\n    def get_description(self):\n        return self.expand_prog_name(self.description)\n\n    def exit(self, status=0, msg=None):\n        if msg:\n            sys.stderr.write(msg)\n        sys.exit(status)\n\n    def error(self, msg):\n        \"\"\"error(msg : string)\n\n        Print a usage message incorporating 'msg' to stderr and exit.\n        If you override this in a subclass, it should not return -- it\n        should either exit or raise an exception.\n        \"\"\"\n        self.print_usage(sys.stderr)\n        self.exit(2, \"%s: error: %s\\n\" % (self.get_prog_name(), msg))\n\n    def get_usage(self):\n        if self.usage:\n            return self.formatter.format_usage(\n                self.expand_prog_name(self.usage))\n        else:\n            return \"\"\n\n    def print_usage(self, file=None):\n        \"\"\"print_usage(file : file = stdout)\n\n        Print the usage message for the current program (self.usage) to\n        'file' (default stdout).  Any occurrence of the string \"%prog\" in\n        self.usage is replaced with the name of the current program\n        (basename of sys.argv[0]).  Does nothing if self.usage is empty\n        or not defined.\n        \"\"\"\n        if self.usage:\n            print >>file, self.get_usage()\n\n    def get_version(self):\n        if self.version:\n            return self.expand_prog_name(self.version)\n        else:\n            return \"\"\n\n    def print_version(self, file=None):\n        \"\"\"print_version(file : file = stdout)\n\n        Print the version message for this program (self.version) to\n        'file' (default stdout).  As with print_usage(), any occurrence\n        of \"%prog\" in self.version is replaced by the current program's\n        name.  Does nothing if self.version is empty or undefined.\n        \"\"\"\n        if self.version:\n            print >>file, self.get_version()\n\n    def format_option_help(self, formatter=None):\n        if formatter is None:\n            formatter = self.formatter\n        formatter.store_option_strings(self)\n        result = []\n        result.append(formatter.format_heading(_(\"Options\")))\n        formatter.indent()\n        if self.option_list:\n            result.append(OptionContainer.format_option_help(self, formatter))\n            result.append(\"\\n\")\n        for group in self.option_groups:\n            result.append(group.format_help(formatter))\n            result.append(\"\\n\")\n        formatter.dedent()\n        # Drop the last \"\\n\", or the header if no options or option groups:\n        return \"\".join(result[:-1])\n\n    def format_epilog(self, formatter):\n        return formatter.format_epilog(self.epilog)\n\n    def format_help(self, formatter=None):\n        if formatter is None:\n            formatter = self.formatter\n        result = []\n        if self.usage:\n            result.append(self.get_usage() + \"\\n\")\n        if self.description:\n            result.append(self.format_description(formatter) + \"\\n\")\n        result.append(self.format_option_help(formatter))\n        result.append(self.format_epilog(formatter))\n        return \"\".join(result)\n\n    # used by test suite\n    def _get_encoding(self, file):\n        encoding = getattr(file, \"encoding\", None)\n        if not encoding:\n            encoding = sys.getdefaultencoding()\n        return encoding\n\n    def print_help(self, file=None):\n        \"\"\"print_help(file : file = stdout)\n\n        Print an extended help message, listing all options and any\n        help text provided with them, to 'file' (default stdout).\n        \"\"\"\n        if file is None:\n            file = sys.stdout\n        encoding = self._get_encoding(file)\n        file.write(self.format_help().encode(encoding, \"replace\"))\n\n# class OptionParser\n\n\ndef _match_abbrev(s, wordmap):\n    \"\"\"_match_abbrev(s : string, wordmap : {string : Option}) -> string\n\n    Return the string key in 'wordmap' for which 's' is an unambiguous\n    abbreviation.  If 's' is found to be ambiguous or doesn't match any of\n    'words', raise BadOptionError.\n    \"\"\"\n    # Is there an exact match?\n    if s in wordmap:\n        return s\n    else:\n        # Isolate all words with s as a prefix.\n        possibilities = [word for word in wordmap.keys()\n                         if word.startswith(s)]\n        # No exact match, so there had better be just one possibility.\n        if len(possibilities) == 1:\n            return possibilities[0]\n        elif not possibilities:\n            raise BadOptionError(s)\n        else:\n            # More than one possible completion: ambiguous prefix.\n            possibilities.sort()\n            raise AmbiguousOptionError(s, possibilities)\n\n\n# Some day, there might be many Option classes.  As of Optik 1.3, the\n# preferred way to instantiate Options is indirectly, via make_option(),\n# which will become a factory function when there are many Option\n# classes.\nmake_option = Option\n",
		"file_name": "optparse.py"
	},
	{
		"content": "\"\"\"Internationalization and localization support.\n\nThis module provides internationalization (I18N) and localization (L10N)\nsupport for your Python programs by providing an interface to the GNU gettext\nmessage catalog library.\n\nI18N refers to the operation by which a program is made aware of multiple\nlanguages.  L10N refers to the adaptation of your program, once\ninternationalized, to the local language and cultural habits.\n\n\"\"\"\n\n# This module represents the integration of work, contributions, feedback, and\n# suggestions from the following people:\n#\n# Martin von Loewis, who wrote the initial implementation of the underlying\n# C-based libintlmodule (later renamed _gettext), along with a skeletal\n# gettext.py implementation.\n#\n# Peter Funk, who wrote fintl.py, a fairly complete wrapper around intlmodule,\n# which also included a pure-Python implementation to read .mo files if\n# intlmodule wasn't available.\n#\n# James Henstridge, who also wrote a gettext.py module, which has some\n# interesting, but currently unsupported experimental features: the notion of\n# a Catalog class and instances, and the ability to add to a catalog file via\n# a Python API.\n#\n# Barry Warsaw integrated these modules, wrote the .install() API and code,\n# and conformed all C and Python code to Python's coding standards.\n#\n# Francois Pinard and Marc-Andre Lemburg also contributed valuably to this\n# module.\n#\n# J. David Ibanez implemented plural forms. Bruno Haible fixed some bugs.\n#\n# TODO:\n# - Lazy loading of .mo files.  Currently the entire catalog is loaded into\n#   memory, but that's probably bad for large translated programs.  Instead,\n#   the lexical sort of original strings in GNU .mo files should be exploited\n#   to do binary searches and lazy initializations.  Or you might want to use\n#   the undocumented double-hash algorithm for .mo files with hash tables, but\n#   you'll need to study the GNU gettext code to do this.\n#\n# - Support Solaris .mo file formats.  Unfortunately, we've been unable to\n#   find this format documented anywhere.\n\n\nimport locale, copy, os, re, struct, sys\nfrom errno import ENOENT\n\n\n__all__ = ['NullTranslations', 'GNUTranslations', 'Catalog',\n           'find', 'translation', 'install', 'textdomain', 'bindtextdomain',\n           'dgettext', 'dngettext', 'gettext', 'ngettext',\n           ]\n\n_default_localedir = os.path.join(sys.prefix, 'share', 'locale')\n\n\ndef test(condition, true, false):\n    \"\"\"\n    Implements the C expression:\n\n      condition ? true : false\n\n    Required to correctly interpret plural forms.\n    \"\"\"\n    if condition:\n        return true\n    else:\n        return false\n\n\ndef c2py(plural):\n    \"\"\"Gets a C expression as used in PO files for plural forms and returns a\n    Python lambda function that implements an equivalent expression.\n    \"\"\"\n    # Security check, allow only the \"n\" identifier\n    try:\n        from cStringIO import StringIO\n    except ImportError:\n        from StringIO import StringIO\n    import token, tokenize\n    tokens = tokenize.generate_tokens(StringIO(plural).readline)\n    try:\n        danger = [x for x in tokens if x[0] == token.NAME and x[1] != 'n']\n    except tokenize.TokenError:\n        raise ValueError, \\\n              'plural forms expression error, maybe unbalanced parenthesis'\n    else:\n        if danger:\n            raise ValueError, 'plural forms expression could be dangerous'\n\n    # Replace some C operators by their Python equivalents\n    plural = plural.replace('&&', ' and ')\n    plural = plural.replace('||', ' or ')\n\n    expr = re.compile(r'\\!([^=])')\n    plural = expr.sub(' not \\\\1', plural)\n\n    # Regular expression and replacement function used to transform\n    # \"a?b:c\" to \"test(a,b,c)\".\n    expr = re.compile(r'(.*?)\\?(.*?):(.*)')\n    def repl(x):\n        return \"test(%s, %s, %s)\" % (x.group(1), x.group(2),\n                                     expr.sub(repl, x.group(3)))\n\n    # Code to transform the plural expression, taking care of parentheses\n    stack = ['']\n    for c in plural:\n        if c == '(':\n            stack.append('')\n        elif c == ')':\n            if len(stack) == 1:\n                # Actually, we never reach this code, because unbalanced\n                # parentheses get caught in the security check at the\n                # beginning.\n                raise ValueError, 'unbalanced parenthesis in plural form'\n            s = expr.sub(repl, stack.pop())\n            stack[-1] += '(%s)' % s\n        else:\n            stack[-1] += c\n    plural = expr.sub(repl, stack.pop())\n\n    return eval('lambda n: int(%s)' % plural)\n\n\n\ndef _expand_lang(locale):\n    from locale import normalize\n    locale = normalize(locale)\n    COMPONENT_CODESET   = 1 << 0\n    COMPONENT_TERRITORY = 1 << 1\n    COMPONENT_MODIFIER  = 1 << 2\n    # split up the locale into its base components\n    mask = 0\n    pos = locale.find('@')\n    if pos >= 0:\n        modifier = locale[pos:]\n        locale = locale[:pos]\n        mask |= COMPONENT_MODIFIER\n    else:\n        modifier = ''\n    pos = locale.find('.')\n    if pos >= 0:\n        codeset = locale[pos:]\n        locale = locale[:pos]\n        mask |= COMPONENT_CODESET\n    else:\n        codeset = ''\n    pos = locale.find('_')\n    if pos >= 0:\n        territory = locale[pos:]\n        locale = locale[:pos]\n        mask |= COMPONENT_TERRITORY\n    else:\n        territory = ''\n    language = locale\n    ret = []\n    for i in range(mask+1):\n        if not (i & ~mask):  # if all components for this combo exist ...\n            val = language\n            if i & COMPONENT_TERRITORY: val += territory\n            if i & COMPONENT_CODESET:   val += codeset\n            if i & COMPONENT_MODIFIER:  val += modifier\n            ret.append(val)\n    ret.reverse()\n    return ret\n\n\n\nclass NullTranslations:\n    def __init__(self, fp=None):\n        self._info = {}\n        self._charset = None\n        self._output_charset = None\n        self._fallback = None\n        if fp is not None:\n            self._parse(fp)\n\n    def _parse(self, fp):\n        pass\n\n    def add_fallback(self, fallback):\n        if self._fallback:\n            self._fallback.add_fallback(fallback)\n        else:\n            self._fallback = fallback\n\n    def gettext(self, message):\n        if self._fallback:\n            return self._fallback.gettext(message)\n        return message\n\n    def lgettext(self, message):\n        if self._fallback:\n            return self._fallback.lgettext(message)\n        return message\n\n    def ngettext(self, msgid1, msgid2, n):\n        if self._fallback:\n            return self._fallback.ngettext(msgid1, msgid2, n)\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n\n    def lngettext(self, msgid1, msgid2, n):\n        if self._fallback:\n            return self._fallback.lngettext(msgid1, msgid2, n)\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n\n    def ugettext(self, message):\n        if self._fallback:\n            return self._fallback.ugettext(message)\n        return unicode(message)\n\n    def ungettext(self, msgid1, msgid2, n):\n        if self._fallback:\n            return self._fallback.ungettext(msgid1, msgid2, n)\n        if n == 1:\n            return unicode(msgid1)\n        else:\n            return unicode(msgid2)\n\n    def info(self):\n        return self._info\n\n    def charset(self):\n        return self._charset\n\n    def output_charset(self):\n        return self._output_charset\n\n    def set_output_charset(self, charset):\n        self._output_charset = charset\n\n    def install(self, unicode=False, names=None):\n        import __builtin__\n        __builtin__.__dict__['_'] = unicode and self.ugettext or self.gettext\n        if hasattr(names, \"__contains__\"):\n            if \"gettext\" in names:\n                __builtin__.__dict__['gettext'] = __builtin__.__dict__['_']\n            if \"ngettext\" in names:\n                __builtin__.__dict__['ngettext'] = (unicode and self.ungettext\n                                                             or self.ngettext)\n            if \"lgettext\" in names:\n                __builtin__.__dict__['lgettext'] = self.lgettext\n            if \"lngettext\" in names:\n                __builtin__.__dict__['lngettext'] = self.lngettext\n\n\nclass GNUTranslations(NullTranslations):\n    # Magic number of .mo files\n    LE_MAGIC = 0x950412deL\n    BE_MAGIC = 0xde120495L\n\n    def _parse(self, fp):\n        \"\"\"Override this method to support alternative .mo formats.\"\"\"\n        unpack = struct.unpack\n        filename = getattr(fp, 'name', '')\n        # Parse the .mo file header, which consists of 5 little endian 32\n        # bit words.\n        self._catalog = catalog = {}\n        self.plural = lambda n: int(n != 1) # germanic plural by default\n        buf = fp.read()\n        buflen = len(buf)\n        # Are we big endian or little endian?\n        magic = unpack('<I', buf[:4])[0]\n        if magic == self.LE_MAGIC:\n            version, msgcount, masteridx, transidx = unpack('<4I', buf[4:20])\n            ii = '<II'\n        elif magic == self.BE_MAGIC:\n            version, msgcount, masteridx, transidx = unpack('>4I', buf[4:20])\n            ii = '>II'\n        else:\n            raise IOError(0, 'Bad magic number', filename)\n        # Now put all messages from the .mo file buffer into the catalog\n        # dictionary.\n        for i in xrange(0, msgcount):\n            mlen, moff = unpack(ii, buf[masteridx:masteridx+8])\n            mend = moff + mlen\n            tlen, toff = unpack(ii, buf[transidx:transidx+8])\n            tend = toff + tlen\n            if mend < buflen and tend < buflen:\n                msg = buf[moff:mend]\n                tmsg = buf[toff:tend]\n            else:\n                raise IOError(0, 'File is corrupt', filename)\n            # See if we're looking at GNU .mo conventions for metadata\n            if mlen == 0:\n                # Catalog description\n                lastk = k = None\n                for item in tmsg.splitlines():\n                    item = item.strip()\n                    if not item:\n                        continue\n                    if ':' in item:\n                        k, v = item.split(':', 1)\n                        k = k.strip().lower()\n                        v = v.strip()\n                        self._info[k] = v\n                        lastk = k\n                    elif lastk:\n                        self._info[lastk] += '\\n' + item\n                    if k == 'content-type':\n                        self._charset = v.split('charset=')[1]\n                    elif k == 'plural-forms':\n                        v = v.split(';')\n                        plural = v[1].split('plural=')[1]\n                        self.plural = c2py(plural)\n            # Note: we unconditionally convert both msgids and msgstrs to\n            # Unicode using the character encoding specified in the charset\n            # parameter of the Content-Type header.  The gettext documentation\n            # strongly encourages msgids to be us-ascii, but some applications\n            # require alternative encodings (e.g. Zope's ZCML and ZPT).  For\n            # traditional gettext applications, the msgid conversion will\n            # cause no problems since us-ascii should always be a subset of\n            # the charset encoding.  We may want to fall back to 8-bit msgids\n            # if the Unicode conversion fails.\n            if '\\x00' in msg:\n                # Plural forms\n                msgid1, msgid2 = msg.split('\\x00')\n                tmsg = tmsg.split('\\x00')\n                if self._charset:\n                    msgid1 = unicode(msgid1, self._charset)\n                    tmsg = [unicode(x, self._charset) for x in tmsg]\n                for i in range(len(tmsg)):\n                    catalog[(msgid1, i)] = tmsg[i]\n            else:\n                if self._charset:\n                    msg = unicode(msg, self._charset)\n                    tmsg = unicode(tmsg, self._charset)\n                catalog[msg] = tmsg\n            # advance to next entry in the seek tables\n            masteridx += 8\n            transidx += 8\n\n    def gettext(self, message):\n        missing = object()\n        tmsg = self._catalog.get(message, missing)\n        if tmsg is missing:\n            if self._fallback:\n                return self._fallback.gettext(message)\n            return message\n        # Encode the Unicode tmsg back to an 8-bit string, if possible\n        if self._output_charset:\n            return tmsg.encode(self._output_charset)\n        elif self._charset:\n            return tmsg.encode(self._charset)\n        return tmsg\n\n    def lgettext(self, message):\n        missing = object()\n        tmsg = self._catalog.get(message, missing)\n        if tmsg is missing:\n            if self._fallback:\n                return self._fallback.lgettext(message)\n            return message\n        if self._output_charset:\n            return tmsg.encode(self._output_charset)\n        return tmsg.encode(locale.getpreferredencoding())\n\n    def ngettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog[(msgid1, self.plural(n))]\n            if self._output_charset:\n                return tmsg.encode(self._output_charset)\n            elif self._charset:\n                return tmsg.encode(self._charset)\n            return tmsg\n        except KeyError:\n            if self._fallback:\n                return self._fallback.ngettext(msgid1, msgid2, n)\n            if n == 1:\n                return msgid1\n            else:\n                return msgid2\n\n    def lngettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog[(msgid1, self.plural(n))]\n            if self._output_charset:\n                return tmsg.encode(self._output_charset)\n            return tmsg.encode(locale.getpreferredencoding())\n        except KeyError:\n            if self._fallback:\n                return self._fallback.lngettext(msgid1, msgid2, n)\n            if n == 1:\n                return msgid1\n            else:\n                return msgid2\n\n    def ugettext(self, message):\n        missing = object()\n        tmsg = self._catalog.get(message, missing)\n        if tmsg is missing:\n            if self._fallback:\n                return self._fallback.ugettext(message)\n            return unicode(message)\n        return tmsg\n\n    def ungettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog[(msgid1, self.plural(n))]\n        except KeyError:\n            if self._fallback:\n                return self._fallback.ungettext(msgid1, msgid2, n)\n            if n == 1:\n                tmsg = unicode(msgid1)\n            else:\n                tmsg = unicode(msgid2)\n        return tmsg\n\n\n# Locate a .mo file using the gettext strategy\ndef find(domain, localedir=None, languages=None, all=0):\n    # Get some reasonable defaults for arguments that were not supplied\n    if localedir is None:\n        localedir = _default_localedir\n    if languages is None:\n        languages = []\n        for envar in ('LANGUAGE', 'LC_ALL', 'LC_MESSAGES', 'LANG'):\n            val = os.environ.get(envar)\n            if val:\n                languages = val.split(':')\n                break\n        if 'C' not in languages:\n            languages.append('C')\n    # now normalize and expand the languages\n    nelangs = []\n    for lang in languages:\n        for nelang in _expand_lang(lang):\n            if nelang not in nelangs:\n                nelangs.append(nelang)\n    # select a language\n    if all:\n        result = []\n    else:\n        result = None\n    for lang in nelangs:\n        if lang == 'C':\n            break\n        mofile = os.path.join(localedir, lang, 'LC_MESSAGES', '%s.mo' % domain)\n        if os.path.exists(mofile):\n            if all:\n                result.append(mofile)\n            else:\n                return mofile\n    return result\n\n\n\n# a mapping between absolute .mo file path and Translation object\n_translations = {}\n\ndef translation(domain, localedir=None, languages=None,\n                class_=None, fallback=False, codeset=None):\n    if class_ is None:\n        class_ = GNUTranslations\n    mofiles = find(domain, localedir, languages, all=1)\n    if not mofiles:\n        if fallback:\n            return NullTranslations()\n        raise IOError(ENOENT, 'No translation file found for domain', domain)\n    # Avoid opening, reading, and parsing the .mo file after it's been done\n    # once.\n    result = None\n    for mofile in mofiles:\n        key = (class_, os.path.abspath(mofile))\n        t = _translations.get(key)\n        if t is None:\n            with open(mofile, 'rb') as fp:\n                t = _translations.setdefault(key, class_(fp))\n        # Copy the translation object to allow setting fallbacks and\n        # output charset. All other instance data is shared with the\n        # cached object.\n        t = copy.copy(t)\n        if codeset:\n            t.set_output_charset(codeset)\n        if result is None:\n            result = t\n        else:\n            result.add_fallback(t)\n    return result\n\n\ndef install(domain, localedir=None, unicode=False, codeset=None, names=None):\n    t = translation(domain, localedir, fallback=True, codeset=codeset)\n    t.install(unicode, names)\n\n\n\n# a mapping b/w domains and locale directories\n_localedirs = {}\n# a mapping b/w domains and codesets\n_localecodesets = {}\n# current global domain, `messages' used for compatibility w/ GNU gettext\n_current_domain = 'messages'\n\n\ndef textdomain(domain=None):\n    global _current_domain\n    if domain is not None:\n        _current_domain = domain\n    return _current_domain\n\n\ndef bindtextdomain(domain, localedir=None):\n    global _localedirs\n    if localedir is not None:\n        _localedirs[domain] = localedir\n    return _localedirs.get(domain, _default_localedir)\n\n\ndef bind_textdomain_codeset(domain, codeset=None):\n    global _localecodesets\n    if codeset is not None:\n        _localecodesets[domain] = codeset\n    return _localecodesets.get(domain)\n\n\ndef dgettext(domain, message):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        return message\n    return t.gettext(message)\n\ndef ldgettext(domain, message):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        return message\n    return t.lgettext(message)\n\ndef dngettext(domain, msgid1, msgid2, n):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n    return t.ngettext(msgid1, msgid2, n)\n\ndef ldngettext(domain, msgid1, msgid2, n):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n    return t.lngettext(msgid1, msgid2, n)\n\ndef gettext(message):\n    return dgettext(_current_domain, message)\n\ndef lgettext(message):\n    return ldgettext(_current_domain, message)\n\ndef ngettext(msgid1, msgid2, n):\n    return dngettext(_current_domain, msgid1, msgid2, n)\n\ndef lngettext(msgid1, msgid2, n):\n    return ldngettext(_current_domain, msgid1, msgid2, n)\n\n# dcgettext() has been deemed unnecessary and is not implemented.\n\n# James Henstridge's Catalog constructor from GNOME gettext.  Documented usage\n# was:\n#\n#    import gettext\n#    cat = gettext.Catalog(PACKAGE, localedir=LOCALEDIR)\n#    _ = cat.gettext\n#    print _('Hello World')\n\n# The resulting catalog object currently don't support access through a\n# dictionary API, which was supported (but apparently unused) in GNOME\n# gettext.\n\nCatalog = translation\n",
		"file_name": "gettext.py"
	},
	{
		"content": "\"\"\"Token constants (from \"token.h\").\"\"\"\n\n#  This file is automatically generated; please don't muck it up!\n#\n#  To update the symbols in this file, 'cd' to the top directory of\n#  the python source tree after building the interpreter and run:\n#\n#    ./python Lib/token.py\n\n#--start constants--\nENDMARKER = 0\nNAME = 1\nNUMBER = 2\nSTRING = 3\nNEWLINE = 4\nINDENT = 5\nDEDENT = 6\nLPAR = 7\nRPAR = 8\nLSQB = 9\nRSQB = 10\nCOLON = 11\nCOMMA = 12\nSEMI = 13\nPLUS = 14\nMINUS = 15\nSTAR = 16\nSLASH = 17\nVBAR = 18\nAMPER = 19\nLESS = 20\nGREATER = 21\nEQUAL = 22\nDOT = 23\nPERCENT = 24\nBACKQUOTE = 25\nLBRACE = 26\nRBRACE = 27\nEQEQUAL = 28\nNOTEQUAL = 29\nLESSEQUAL = 30\nGREATEREQUAL = 31\nTILDE = 32\nCIRCUMFLEX = 33\nLEFTSHIFT = 34\nRIGHTSHIFT = 35\nDOUBLESTAR = 36\nPLUSEQUAL = 37\nMINEQUAL = 38\nSTAREQUAL = 39\nSLASHEQUAL = 40\nPERCENTEQUAL = 41\nAMPEREQUAL = 42\nVBAREQUAL = 43\nCIRCUMFLEXEQUAL = 44\nLEFTSHIFTEQUAL = 45\nRIGHTSHIFTEQUAL = 46\nDOUBLESTAREQUAL = 47\nDOUBLESLASH = 48\nDOUBLESLASHEQUAL = 49\nAT = 50\nOP = 51\nERRORTOKEN = 52\nN_TOKENS = 53\nNT_OFFSET = 256\n#--end constants--\n\ntok_name = {}\nfor _name, _value in globals().items():\n    if type(_value) is type(0):\n        tok_name[_value] = _name\ndel _name, _value\n\n\ndef ISTERMINAL(x):\n    return x < NT_OFFSET\n\ndef ISNONTERMINAL(x):\n    return x >= NT_OFFSET\n\ndef ISEOF(x):\n    return x == ENDMARKER\n\n\ndef main():\n    import re\n    import sys\n    args = sys.argv[1:]\n    inFileName = args and args[0] or \"Include/token.h\"\n    outFileName = \"Lib/token.py\"\n    if len(args) > 1:\n        outFileName = args[1]\n    try:\n        fp = open(inFileName)\n    except IOError, err:\n        sys.stdout.write(\"I/O error: %s\\n\" % str(err))\n        sys.exit(1)\n    lines = fp.read().split(\"\\n\")\n    fp.close()\n    prog = re.compile(\n        \"#define[ \\t][ \\t]*([A-Z0-9][A-Z0-9_]*)[ \\t][ \\t]*([0-9][0-9]*)\",\n        re.IGNORECASE)\n    tokens = {}\n    for line in lines:\n        match = prog.match(line)\n        if match:\n            name, val = match.group(1, 2)\n            val = int(val)\n            tokens[val] = name          # reverse so we can sort them...\n    keys = tokens.keys()\n    keys.sort()\n    # load the output skeleton from the target:\n    try:\n        fp = open(outFileName)\n    except IOError, err:\n        sys.stderr.write(\"I/O error: %s\\n\" % str(err))\n        sys.exit(2)\n    format = fp.read().split(\"\\n\")\n    fp.close()\n    try:\n        start = format.index(\"#--start constants--\") + 1\n        end = format.index(\"#--end constants--\")\n    except ValueError:\n        sys.stderr.write(\"target does not contain format markers\")\n        sys.exit(3)\n    lines = []\n    for val in keys:\n        lines.append(\"%s = %d\" % (tokens[val], val))\n    format[start:end] = lines\n    try:\n        fp = open(outFileName, 'w')\n    except IOError, err:\n        sys.stderr.write(\"I/O error: %s\\n\" % str(err))\n        sys.exit(4)\n    fp.write(\"\\n\".join(format))\n    fp.close()\n\n\nif __name__ == \"__main__\":\n    main()\n",
		"file_name": "token.py"
	},
	{
		"content": "r\"\"\"File-like objects that read from or write to a string buffer.\n\nThis implements (nearly) all stdio methods.\n\nf = StringIO()      # ready for writing\nf = StringIO(buf)   # ready for reading\nf.close()           # explicitly release resources held\nflag = f.isatty()   # always false\npos = f.tell()      # get current position\nf.seek(pos)         # set current position\nf.seek(pos, mode)   # mode 0: absolute; 1: relative; 2: relative to EOF\nbuf = f.read()      # read until EOF\nbuf = f.read(n)     # read up to n bytes\nbuf = f.readline()  # read until end of line ('\\n') or EOF\nlist = f.readlines()# list of f.readline() results until EOF\nf.truncate([size])  # truncate file at to at most size (default: current pos)\nf.write(buf)        # write at current position\nf.writelines(list)  # for line in list: f.write(line)\nf.getvalue()        # return whole file's contents as a string\n\nNotes:\n- Using a real file is often faster (but less convenient).\n- There's also a much faster implementation in C, called cStringIO, but\n  it's not subclassable.\n- fileno() is left unimplemented so that code which uses it triggers\n  an exception early.\n- Seeking far beyond EOF and then writing will insert real null\n  bytes that occupy space in the buffer.\n- There's a simple test set (see end of this file).\n\"\"\"\ntry:\n    from errno import EINVAL\nexcept ImportError:\n    EINVAL = 22\n\n__all__ = [\"StringIO\"]\n\ndef _complain_ifclosed(closed):\n    if closed:\n        raise ValueError, \"I/O operation on closed file\"\n\nclass StringIO:\n    \"\"\"class StringIO([buffer])\n\n    When a StringIO object is created, it can be initialized to an existing\n    string by passing the string to the constructor. If no string is given,\n    the StringIO will start empty.\n\n    The StringIO object can accept either Unicode or 8-bit strings, but\n    mixing the two may take some care. If both are used, 8-bit strings that\n    cannot be interpreted as 7-bit ASCII (that use the 8th bit) will cause\n    a UnicodeError to be raised when getvalue() is called.\n    \"\"\"\n    def __init__(self, buf = ''):\n        # Force self.buf to be a string or unicode\n        if not isinstance(buf, basestring):\n            buf = str(buf)\n        self.buf = buf\n        self.len = len(buf)\n        self.buflist = []\n        self.pos = 0\n        self.closed = False\n        self.softspace = 0\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        \"\"\"A file object is its own iterator, for example iter(f) returns f\n        (unless f is closed). When a file is used as an iterator, typically\n        in a for loop (for example, for line in f: print line), the next()\n        method is called repeatedly. This method returns the next input line,\n        or raises StopIteration when EOF is hit.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        r = self.readline()\n        if not r:\n            raise StopIteration\n        return r\n\n    def close(self):\n        \"\"\"Free the memory buffer.\n        \"\"\"\n        if not self.closed:\n            self.closed = True\n            del self.buf, self.pos\n\n    def isatty(self):\n        \"\"\"Returns False because StringIO objects are not connected to a\n        tty-like device.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        return False\n\n    def seek(self, pos, mode = 0):\n        \"\"\"Set the file's current position.\n\n        The mode argument is optional and defaults to 0 (absolute file\n        positioning); other values are 1 (seek relative to the current\n        position) and 2 (seek relative to the file's end).\n\n        There is no return value.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        if mode == 1:\n            pos += self.pos\n        elif mode == 2:\n            pos += self.len\n        self.pos = max(0, pos)\n\n    def tell(self):\n        \"\"\"Return the file's current position.\"\"\"\n        _complain_ifclosed(self.closed)\n        return self.pos\n\n    def read(self, n = -1):\n        \"\"\"Read at most size bytes from the file\n        (less if the read hits EOF before obtaining size bytes).\n\n        If the size argument is negative or omitted, read all data until EOF\n        is reached. The bytes are returned as a string object. An empty\n        string is returned when EOF is encountered immediately.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        if n is None or n < 0:\n            newpos = self.len\n        else:\n            newpos = min(self.pos+n, self.len)\n        r = self.buf[self.pos:newpos]\n        self.pos = newpos\n        return r\n\n    def readline(self, length=None):\n        r\"\"\"Read one entire line from the file.\n\n        A trailing newline character is kept in the string (but may be absent\n        when a file ends with an incomplete line). If the size argument is\n        present and non-negative, it is a maximum byte count (including the\n        trailing newline) and an incomplete line may be returned.\n\n        An empty string is returned only when EOF is encountered immediately.\n\n        Note: Unlike stdio's fgets(), the returned string contains null\n        characters ('\\0') if they occurred in the input.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        i = self.buf.find('\\n', self.pos)\n        if i < 0:\n            newpos = self.len\n        else:\n            newpos = i+1\n        if length is not None and length >= 0:\n            if self.pos + length < newpos:\n                newpos = self.pos + length\n        r = self.buf[self.pos:newpos]\n        self.pos = newpos\n        return r\n\n    def readlines(self, sizehint = 0):\n        \"\"\"Read until EOF using readline() and return a list containing the\n        lines thus read.\n\n        If the optional sizehint argument is present, instead of reading up\n        to EOF, whole lines totalling approximately sizehint bytes (or more\n        to accommodate a final whole line).\n        \"\"\"\n        total = 0\n        lines = []\n        line = self.readline()\n        while line:\n            lines.append(line)\n            total += len(line)\n            if 0 < sizehint <= total:\n                break\n            line = self.readline()\n        return lines\n\n    def truncate(self, size=None):\n        \"\"\"Truncate the file's size.\n\n        If the optional size argument is present, the file is truncated to\n        (at most) that size. The size defaults to the current position.\n        The current file position is not changed unless the position\n        is beyond the new file size.\n\n        If the specified size exceeds the file's current size, the\n        file remains unchanged.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if size is None:\n            size = self.pos\n        elif size < 0:\n            raise IOError(EINVAL, \"Negative size not allowed\")\n        elif size < self.pos:\n            self.pos = size\n        self.buf = self.getvalue()[:size]\n        self.len = size\n\n    def write(self, s):\n        \"\"\"Write a string to the file.\n\n        There is no return value.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if not s: return\n        # Force s to be a string or unicode\n        if not isinstance(s, basestring):\n            s = str(s)\n        spos = self.pos\n        slen = self.len\n        if spos == slen:\n            self.buflist.append(s)\n            self.len = self.pos = spos + len(s)\n            return\n        if spos > slen:\n            self.buflist.append('\\0'*(spos - slen))\n            slen = spos\n        newpos = spos + len(s)\n        if spos < slen:\n            if self.buflist:\n                self.buf += ''.join(self.buflist)\n            self.buflist = [self.buf[:spos], s, self.buf[newpos:]]\n            self.buf = ''\n            if newpos > slen:\n                slen = newpos\n        else:\n            self.buflist.append(s)\n            slen = newpos\n        self.len = slen\n        self.pos = newpos\n\n    def writelines(self, iterable):\n        \"\"\"Write a sequence of strings to the file. The sequence can be any\n        iterable object producing strings, typically a list of strings. There\n        is no return value.\n\n        (The name is intended to match readlines(); writelines() does not add\n        line separators.)\n        \"\"\"\n        write = self.write\n        for line in iterable:\n            write(line)\n\n    def flush(self):\n        \"\"\"Flush the internal buffer\n        \"\"\"\n        _complain_ifclosed(self.closed)\n\n    def getvalue(self):\n        \"\"\"\n        Retrieve the entire contents of the \"file\" at any time before\n        the StringIO object's close() method is called.\n\n        The StringIO object can accept either Unicode or 8-bit strings,\n        but mixing the two may take some care. If both are used, 8-bit\n        strings that cannot be interpreted as 7-bit ASCII (that use the\n        8th bit) will cause a UnicodeError to be raised when getvalue()\n        is called.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        return self.buf\n\n\n# A little test suite\n\ndef test():\n    import sys\n    if sys.argv[1:]:\n        file = sys.argv[1]\n    else:\n        file = '/etc/passwd'\n    lines = open(file, 'r').readlines()\n    text = open(file, 'r').read()\n    f = StringIO()\n    for line in lines[:-2]:\n        f.write(line)\n    f.writelines(lines[-2:])\n    if f.getvalue() != text:\n        raise RuntimeError, 'write failed'\n    length = f.tell()\n    print 'File length =', length\n    f.seek(len(lines[0]))\n    f.write(lines[1])\n    f.seek(0)\n    print 'First line =', repr(f.readline())\n    print 'Position =', f.tell()\n    line = f.readline()\n    print 'Second line =', repr(line)\n    f.seek(-len(line), 1)\n    line2 = f.read(len(line))\n    if line != line2:\n        raise RuntimeError, 'bad result after seek back'\n    f.seek(len(line2), 1)\n    list = f.readlines()\n    line = list[-1]\n    f.seek(f.tell() - len(line))\n    line2 = f.read()\n    if line != line2:\n        raise RuntimeError, 'bad result after seek back from EOF'\n    print 'Read', len(list), 'more lines'\n    print 'File length =', f.tell()\n    if f.tell() != length:\n        raise RuntimeError, 'bad length'\n    f.truncate(length/2)\n    f.seek(0, 2)\n    print 'Truncated length =', f.tell()\n    if f.tell() != length/2:\n        raise RuntimeError, 'truncate did not adjust length'\n    f.close()\n\nif __name__ == '__main__':\n    test()\n",
		"file_name": "StringIO.py"
	},
	{
		"content": "\"\"\"Tokenization help for Python programs.\n\ngenerate_tokens(readline) is a generator that breaks a stream of\ntext into Python tokens.  It accepts a readline-like method which is called\nrepeatedly to get the next line of input (or \"\" for EOF).  It generates\n5-tuples with these members:\n\n    the token type (see token.py)\n    the token (a string)\n    the starting (row, column) indices of the token (a 2-tuple of ints)\n    the ending (row, column) indices of the token (a 2-tuple of ints)\n    the original line (string)\n\nIt is designed to match the working of the Python tokenizer exactly, except\nthat it produces COMMENT tokens for comments and gives type OP for all\noperators\n\nOlder entry points\n    tokenize_loop(readline, tokeneater)\n    tokenize(readline, tokeneater=printtoken)\nare the same, except instead of generating tokens, tokeneater is a callback\nfunction to which the 5 fields described above are passed as 5 arguments,\neach time a new token is found.\"\"\"\n\n__author__ = 'Ka-Ping Yee <ping@lfw.org>'\n__credits__ = ('GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, '\n               'Skip Montanaro, Raymond Hettinger')\n\nfrom itertools import chain\nimport string, re\nfrom token import *\n\nimport token\n__all__ = [x for x in dir(token) if not x.startswith(\"_\")]\n__all__ += [\"COMMENT\", \"tokenize\", \"generate_tokens\", \"NL\", \"untokenize\"]\ndel x\ndel token\n\nCOMMENT = N_TOKENS\ntok_name[COMMENT] = 'COMMENT'\nNL = N_TOKENS + 1\ntok_name[NL] = 'NL'\nN_TOKENS += 2\n\ndef group(*choices): return '(' + '|'.join(choices) + ')'\ndef any(*choices): return group(*choices) + '*'\ndef maybe(*choices): return group(*choices) + '?'\n\nWhitespace = r'[ \\f\\t]*'\nComment = r'#[^\\r\\n]*'\nIgnore = Whitespace + any(r'\\\\\\r?\\n' + Whitespace) + maybe(Comment)\nName = r'[a-zA-Z_]\\w*'\n\nHexnumber = r'0[xX][\\da-fA-F]+[lL]?'\nOctnumber = r'(0[oO][0-7]+)|(0[0-7]*)[lL]?'\nBinnumber = r'0[bB][01]+[lL]?'\nDecnumber = r'[1-9]\\d*[lL]?'\nIntnumber = group(Hexnumber, Binnumber, Octnumber, Decnumber)\nExponent = r'[eE][-+]?\\d+'\nPointfloat = group(r'\\d+\\.\\d*', r'\\.\\d+') + maybe(Exponent)\nExpfloat = r'\\d+' + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r'\\d+[jJ]', Floatnumber + r'[jJ]')\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\n# Tail end of ' string.\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\n# Tail end of \" string.\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\n# Tail end of ''' string.\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\n# Tail end of \"\"\" string.\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\nTriple = group(\"[uUbB]?[rR]?'''\", '[uUbB]?[rR]?\"\"\"')\n# Single-line ' or \" string.\nString = group(r\"[uUbB]?[rR]?'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n               r'[uUbB]?[rR]?\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"')\n\n# Because of leftmost-then-longest match semantics, be sure to put the\n# longest operators first (e.g., if = came before ==, == would get\n# recognized as two instances of =).\nOperator = group(r\"\\*\\*=?\", r\">>=?\", r\"<<=?\", r\"<>\", r\"!=\",\n                 r\"//=?\",\n                 r\"[+\\-*/%&|^=<>]=?\",\n                 r\"~\")\n\nBracket = '[][(){}]'\nSpecial = group(r'\\r?\\n', r'[:;.,`@]')\nFunny = group(Operator, Bracket, Special)\n\nPlainToken = group(Number, Funny, String, Name)\nToken = Ignore + PlainToken\n\n# First (or only) line of ' or \" string.\nContStr = group(r\"[uUbB]?[rR]?'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" +\n                group(\"'\", r'\\\\\\r?\\n'),\n                r'[uUbB]?[rR]?\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' +\n                group('\"', r'\\\\\\r?\\n'))\nPseudoExtras = group(r'\\\\\\r?\\n|\\Z', Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\ntokenprog, pseudoprog, single3prog, double3prog = map(\n    re.compile, (Token, PseudoToken, Single3, Double3))\nendprogs = {\"'\": re.compile(Single), '\"': re.compile(Double),\n            \"'''\": single3prog, '\"\"\"': double3prog,\n            \"r'''\": single3prog, 'r\"\"\"': double3prog,\n            \"u'''\": single3prog, 'u\"\"\"': double3prog,\n            \"ur'''\": single3prog, 'ur\"\"\"': double3prog,\n            \"R'''\": single3prog, 'R\"\"\"': double3prog,\n            \"U'''\": single3prog, 'U\"\"\"': double3prog,\n            \"uR'''\": single3prog, 'uR\"\"\"': double3prog,\n            \"Ur'''\": single3prog, 'Ur\"\"\"': double3prog,\n            \"UR'''\": single3prog, 'UR\"\"\"': double3prog,\n            \"b'''\": single3prog, 'b\"\"\"': double3prog,\n            \"br'''\": single3prog, 'br\"\"\"': double3prog,\n            \"B'''\": single3prog, 'B\"\"\"': double3prog,\n            \"bR'''\": single3prog, 'bR\"\"\"': double3prog,\n            \"Br'''\": single3prog, 'Br\"\"\"': double3prog,\n            \"BR'''\": single3prog, 'BR\"\"\"': double3prog,\n            'r': None, 'R': None, 'u': None, 'U': None,\n            'b': None, 'B': None}\n\ntriple_quoted = {}\nfor t in (\"'''\", '\"\"\"',\n          \"r'''\", 'r\"\"\"', \"R'''\", 'R\"\"\"',\n          \"u'''\", 'u\"\"\"', \"U'''\", 'U\"\"\"',\n          \"ur'''\", 'ur\"\"\"', \"Ur'''\", 'Ur\"\"\"',\n          \"uR'''\", 'uR\"\"\"', \"UR'''\", 'UR\"\"\"',\n          \"b'''\", 'b\"\"\"', \"B'''\", 'B\"\"\"',\n          \"br'''\", 'br\"\"\"', \"Br'''\", 'Br\"\"\"',\n          \"bR'''\", 'bR\"\"\"', \"BR'''\", 'BR\"\"\"'):\n    triple_quoted[t] = t\nsingle_quoted = {}\nfor t in (\"'\", '\"',\n          \"r'\", 'r\"', \"R'\", 'R\"',\n          \"u'\", 'u\"', \"U'\", 'U\"',\n          \"ur'\", 'ur\"', \"Ur'\", 'Ur\"',\n          \"uR'\", 'uR\"', \"UR'\", 'UR\"',\n          \"b'\", 'b\"', \"B'\", 'B\"',\n          \"br'\", 'br\"', \"Br'\", 'Br\"',\n          \"bR'\", 'bR\"', \"BR'\", 'BR\"' ):\n    single_quoted[t] = t\n\ntabsize = 8\n\nclass TokenError(Exception): pass\n\nclass StopTokenizing(Exception): pass\n\ndef printtoken(type, token, srow_scol, erow_ecol, line): # for testing\n    srow, scol = srow_scol\n    erow, ecol = erow_ecol\n    print \"%d,%d-%d,%d:\\t%s\\t%s\" % \\\n        (srow, scol, erow, ecol, tok_name[type], repr(token))\n\ndef tokenize(readline, tokeneater=printtoken):\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n# backwards compatible interface\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\nclass Untokenizer:\n\n    def __init__(self):\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start):\n        row, col = start\n        if row < self.prev_row or row == self.prev_row and col < self.prev_col:\n            raise ValueError(\"start ({},{}) precedes previous end ({},{})\"\n                             .format(row, col, self.prev_row, self.prev_col))\n        row_offset = row - self.prev_row\n        if row_offset:\n            self.tokens.append(\"\\\\\\n\" * row_offset)\n            self.prev_col = 0\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable):\n        it = iter(iterable)\n        for t in it:\n            if len(t) == 2:\n                self.compat(t, it)\n                break\n            tok_type, token, start, end, line = t\n            if tok_type == ENDMARKER:\n                break\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token, iterable):\n        indents = []\n        toks_append = self.tokens.append\n        startline = token[0] in (NEWLINE, NL)\n        prevstring = False\n\n        for tok in chain([token], iterable):\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER):\n                tokval += ' '\n\n            # Insert a space between two consecutive strings\n            if toknum == STRING:\n                if prevstring:\n                    tokval = ' ' + tokval\n                prevstring = True\n            else:\n                prevstring = False\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\ndef untokenize(iterable):\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited intput:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tok in generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\ndef generate_tokens(readline):\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    namechars, numchars = string.ascii_letters + '_', '0123456789'\n    contstr, needcont = '', 0\n    contline = None\n    indents = [0]\n\n    while 1:                                   # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = ''\n        lnum += 1\n        pos, max = 0, len(line)\n\n        if contstr:                            # continued string\n            if not line:\n                raise TokenError, (\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (STRING, contstr + line[:end],\n                       strstart, (lnum, end), contline + line)\n                contstr, needcont = '', 0\n                contline = None\n            elif needcont and line[-2:] != '\\\\\\n' and line[-3:] != '\\\\\\r\\n':\n                yield (ERRORTOKEN, contstr + line,\n                           strstart, (lnum, len(line)), contline)\n                contstr = ''\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line: break\n            column = 0\n            while pos < max:                   # measure leading whitespace\n                if line[pos] == ' ':\n                    column += 1\n                elif line[pos] == '\\t':\n                    column = (column//tabsize + 1)*tabsize\n                elif line[pos] == '\\f':\n                    column = 0\n                else:\n                    break\n                pos += 1\n            if pos == max:\n                break\n\n            if line[pos] in '#\\r\\n':           # skip comments or blank lines\n                if line[pos] == '#':\n                    comment_token = line[pos:].rstrip('\\r\\n')\n                    nl_pos = pos + len(comment_token)\n                    yield (COMMENT, comment_token,\n                           (lnum, pos), (lnum, pos + len(comment_token)), line)\n                    yield (NL, line[nl_pos:],\n                           (lnum, nl_pos), (lnum, len(line)), line)\n                else:\n                    yield ((NL, COMMENT)[line[pos] == '#'], line[pos:],\n                           (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:           # count indents or dedents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n            while column < indents[-1]:\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line))\n                indents = indents[:-1]\n                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)\n\n        else:                                  # continued statement\n            if not line:\n                raise TokenError, (\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:                                # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                if start == end:\n                    continue\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or \\\n                   (initial == '.' and token != '.'):      # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in '\\r\\n':\n                    yield (NL if parenlev > 0 else NEWLINE,\n                           token, spos, epos, line)\n                elif initial == '#':\n                    assert not token.endswith(\"\\n\")\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:                           # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)           # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif initial in single_quoted or \\\n                    token[:2] in single_quoted or \\\n                    token[:3] in single_quoted:\n                    if token[-1] == '\\n':                  # continued string\n                        strstart = (lnum, start)\n                        endprog = (endprogs[initial] or endprogs[token[1]] or\n                                   endprogs[token[2]])\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:                                  # ordinary string\n                        yield (STRING, token, spos, epos, line)\n                elif initial in namechars:                 # ordinary name\n                    yield (NAME, token, spos, epos, line)\n                elif initial == '\\\\':                      # continued stmt\n                    continued = 1\n                else:\n                    if initial in '([{':\n                        parenlev += 1\n                    elif initial in ')]}':\n                        parenlev -= 1\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos],\n                           (lnum, pos), (lnum, pos+1), line)\n                pos += 1\n\n    for indent in indents[1:]:                 # pop remaining indent levels\n        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')\n    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')\n\nif __name__ == '__main__':                     # testing\n    import sys\n    if len(sys.argv) > 1:\n        tokenize(open(sys.argv[1]).readline)\n    else:\n        tokenize(sys.stdin.readline)\n",
		"file_name": "tokenize.py"
	},
	{
		"content": "\"\"\"A collection of string operations (most are no longer used).\n\nWarning: most of the code you see here isn't normally used nowadays.\nBeginning with Python 1.6, many of these functions are implemented as\nmethods on the standard string object. They used to be implemented by\na built-in module called strop, but strop is now obsolete itself.\n\nPublic module variables:\n\nwhitespace -- a string containing all characters considered whitespace\nlowercase -- a string containing all characters considered lowercase letters\nuppercase -- a string containing all characters considered uppercase letters\nletters -- a string containing all characters considered letters\ndigits -- a string containing all characters considered decimal digits\nhexdigits -- a string containing all characters considered hexadecimal digits\noctdigits -- a string containing all characters considered octal digits\npunctuation -- a string containing all characters considered punctuation\nprintable -- a string containing all characters considered printable\n\n\"\"\"\n\n# Some strings for ctype-style character classification\nwhitespace = ' \\t\\n\\r\\v\\f'\nlowercase = 'abcdefghijklmnopqrstuvwxyz'\nuppercase = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\nletters = lowercase + uppercase\nascii_lowercase = lowercase\nascii_uppercase = uppercase\nascii_letters = ascii_lowercase + ascii_uppercase\ndigits = '0123456789'\nhexdigits = digits + 'abcdef' + 'ABCDEF'\noctdigits = '01234567'\npunctuation = \"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\nprintable = digits + letters + punctuation + whitespace\n\n# Case conversion helpers\n# Use str to convert Unicode literal in case of -U\nl = map(chr, xrange(256))\n_idmap = str('').join(l)\ndel l\n\n# Functions which aren't available as string methods.\n\n# Capitalize the words in a string, e.g. \" aBc  dEf \" -> \"Abc Def\".\ndef capwords(s, sep=None):\n    \"\"\"capwords(s [,sep]) -> string\n\n    Split the argument into words using split, capitalize each\n    word using capitalize, and join the capitalized words using\n    join.  If the optional second argument sep is absent or None,\n    runs of whitespace characters are replaced by a single space\n    and leading and trailing whitespace are removed, otherwise\n    sep is used to split and join the words.\n\n    \"\"\"\n    return (sep or ' ').join(x.capitalize() for x in s.split(sep))\n\n\n# Construct a translation string\n_idmapL = None\ndef maketrans(fromstr, tostr):\n    \"\"\"maketrans(frm, to) -> string\n\n    Return a translation table (a string of 256 bytes long)\n    suitable for use in string.translate.  The strings frm and to\n    must be of the same length.\n\n    \"\"\"\n    n = len(fromstr)\n    if n != len(tostr):\n        raise ValueError, \"maketrans arguments must have same length\"\n    # this function has been rewritten to suit PyPy better; it is\n    # almost 10x faster than the original.\n    buf = bytearray(256)\n    for i in range(256):\n        buf[i] = i\n    for i in range(n):\n        buf[ord(fromstr[i])] = tostr[i]\n    return str(buf)\n\n\n\n####################################################################\nimport re as _re\n\nclass _multimap:\n    \"\"\"Helper class for combining multiple mappings.\n\n    Used by .{safe_,}substitute() to combine the mapping and keyword\n    arguments.\n    \"\"\"\n    def __init__(self, primary, secondary):\n        self._primary = primary\n        self._secondary = secondary\n\n    def __getitem__(self, key):\n        try:\n            return self._primary[key]\n        except KeyError:\n            return self._secondary[key]\n\n\nclass _TemplateMetaclass(type):\n    pattern = r\"\"\"\n    %(delim)s(?:\n      (?P<escaped>%(delim)s) |   # Escape sequence of two delimiters\n      (?P<named>%(id)s)      |   # delimiter and a Python identifier\n      {(?P<braced>%(id)s)}   |   # delimiter and a braced identifier\n      (?P<invalid>)              # Other ill-formed delimiter exprs\n    )\n    \"\"\"\n\n    def __init__(cls, name, bases, dct):\n        super(_TemplateMetaclass, cls).__init__(name, bases, dct)\n        if 'pattern' in dct:\n            pattern = cls.pattern\n        else:\n            pattern = _TemplateMetaclass.pattern % {\n                'delim' : _re.escape(cls.delimiter),\n                'id'    : cls.idpattern,\n                }\n        cls.pattern = _re.compile(pattern, _re.IGNORECASE | _re.VERBOSE)\n\n\nclass Template:\n    \"\"\"A string class for supporting $-substitutions.\"\"\"\n    __metaclass__ = _TemplateMetaclass\n\n    delimiter = '$'\n    idpattern = r'[_a-z][_a-z0-9]*'\n\n    def __init__(self, template):\n        self.template = template\n\n    # Search for $$, $identifier, ${identifier}, and any bare $'s\n\n    def _invalid(self, mo):\n        i = mo.start('invalid')\n        lines = self.template[:i].splitlines(True)\n        if not lines:\n            colno = 1\n            lineno = 1\n        else:\n            colno = i - len(''.join(lines[:-1]))\n            lineno = len(lines)\n        raise ValueError('Invalid placeholder in string: line %d, col %d' %\n                         (lineno, colno))\n\n    def substitute(self, *args, **kws):\n        if len(args) > 1:\n            raise TypeError('Too many positional arguments')\n        if not args:\n            mapping = kws\n        elif kws:\n            mapping = _multimap(kws, args[0])\n        else:\n            mapping = args[0]\n        # Helper function for .sub()\n        def convert(mo):\n            # Check the most common path first.\n            named = mo.group('named') or mo.group('braced')\n            if named is not None:\n                val = mapping[named]\n                # We use this idiom instead of str() because the latter will\n                # fail if val is a Unicode containing non-ASCII characters.\n                return '%s' % (val,)\n            if mo.group('escaped') is not None:\n                return self.delimiter\n            if mo.group('invalid') is not None:\n                self._invalid(mo)\n            raise ValueError('Unrecognized named group in pattern',\n                             self.pattern)\n        return self.pattern.sub(convert, self.template)\n\n    def safe_substitute(self, *args, **kws):\n        if len(args) > 1:\n            raise TypeError('Too many positional arguments')\n        if not args:\n            mapping = kws\n        elif kws:\n            mapping = _multimap(kws, args[0])\n        else:\n            mapping = args[0]\n        # Helper function for .sub()\n        def convert(mo):\n            named = mo.group('named')\n            if named is not None:\n                try:\n                    # We use this idiom instead of str() because the latter\n                    # will fail if val is a Unicode containing non-ASCII\n                    return '%s' % (mapping[named],)\n                except KeyError:\n                    return self.delimiter + named\n            braced = mo.group('braced')\n            if braced is not None:\n                try:\n                    return '%s' % (mapping[braced],)\n                except KeyError:\n                    return self.delimiter + '{' + braced + '}'\n            if mo.group('escaped') is not None:\n                return self.delimiter\n            if mo.group('invalid') is not None:\n                return self.delimiter\n            raise ValueError('Unrecognized named group in pattern',\n                             self.pattern)\n        return self.pattern.sub(convert, self.template)\n\n\n\n####################################################################\n# NOTE: Everything below here is deprecated.  Use string methods instead.\n# This stuff will go away in Python 3.0.\n\n# Backward compatible names for exceptions\nindex_error = ValueError\natoi_error = ValueError\natof_error = ValueError\natol_error = ValueError\n\n# convert UPPER CASE letters to lower case\ndef lower(s):\n    \"\"\"lower(s) -> string\n\n    Return a copy of the string s converted to lowercase.\n\n    \"\"\"\n    return s.lower()\n\n# Convert lower case letters to UPPER CASE\ndef upper(s):\n    \"\"\"upper(s) -> string\n\n    Return a copy of the string s converted to uppercase.\n\n    \"\"\"\n    return s.upper()\n\n# Swap lower case letters and UPPER CASE\ndef swapcase(s):\n    \"\"\"swapcase(s) -> string\n\n    Return a copy of the string s with upper case characters\n    converted to lowercase and vice versa.\n\n    \"\"\"\n    return s.swapcase()\n\n# Strip leading and trailing tabs and spaces\ndef strip(s, chars=None):\n    \"\"\"strip(s [,chars]) -> string\n\n    Return a copy of the string s with leading and trailing\n    whitespace removed.\n    If chars is given and not None, remove characters in chars instead.\n    If chars is unicode, S will be converted to unicode before stripping.\n\n    \"\"\"\n    return s.strip(chars)\n\n# Strip leading tabs and spaces\ndef lstrip(s, chars=None):\n    \"\"\"lstrip(s [,chars]) -> string\n\n    Return a copy of the string s with leading whitespace removed.\n    If chars is given and not None, remove characters in chars instead.\n\n    \"\"\"\n    return s.lstrip(chars)\n\n# Strip trailing tabs and spaces\ndef rstrip(s, chars=None):\n    \"\"\"rstrip(s [,chars]) -> string\n\n    Return a copy of the string s with trailing whitespace removed.\n    If chars is given and not None, remove characters in chars instead.\n\n    \"\"\"\n    return s.rstrip(chars)\n\n\n# Split a string into a list of space/tab-separated words\ndef split(s, sep=None, maxsplit=-1):\n    \"\"\"split(s [,sep [,maxsplit]]) -> list of strings\n\n    Return a list of the words in the string s, using sep as the\n    delimiter string.  If maxsplit is given, splits at no more than\n    maxsplit places (resulting in at most maxsplit+1 words).  If sep\n    is not specified or is None, any whitespace string is a separator.\n\n    (split and splitfields are synonymous)\n\n    \"\"\"\n    return s.split(sep, maxsplit)\nsplitfields = split\n\n# Split a string into a list of space/tab-separated words\ndef rsplit(s, sep=None, maxsplit=-1):\n    \"\"\"rsplit(s [,sep [,maxsplit]]) -> list of strings\n\n    Return a list of the words in the string s, using sep as the\n    delimiter string, starting at the end of the string and working\n    to the front.  If maxsplit is given, at most maxsplit splits are\n    done. If sep is not specified or is None, any whitespace string\n    is a separator.\n    \"\"\"\n    return s.rsplit(sep, maxsplit)\n\n# Join fields with optional separator\ndef join(words, sep = ' '):\n    \"\"\"join(list [,sep]) -> string\n\n    Return a string composed of the words in list, with\n    intervening occurrences of sep.  The default separator is a\n    single space.\n\n    (joinfields and join are synonymous)\n\n    \"\"\"\n    return sep.join(words)\njoinfields = join\n\n# Find substring, raise exception if not found\ndef index(s, *args):\n    \"\"\"index(s, sub [,start [,end]]) -> int\n\n    Like find but raises ValueError when the substring is not found.\n\n    \"\"\"\n    return s.index(*args)\n\n# Find last substring, raise exception if not found\ndef rindex(s, *args):\n    \"\"\"rindex(s, sub [,start [,end]]) -> int\n\n    Like rfind but raises ValueError when the substring is not found.\n\n    \"\"\"\n    return s.rindex(*args)\n\n# Count non-overlapping occurrences of substring\ndef count(s, *args):\n    \"\"\"count(s, sub[, start[,end]]) -> int\n\n    Return the number of occurrences of substring sub in string\n    s[start:end].  Optional arguments start and end are\n    interpreted as in slice notation.\n\n    \"\"\"\n    return s.count(*args)\n\n# Find substring, return -1 if not found\ndef find(s, *args):\n    \"\"\"find(s, sub [,start [,end]]) -> in\n\n    Return the lowest index in s where substring sub is found,\n    such that sub is contained within s[start,end].  Optional\n    arguments start and end are interpreted as in slice notation.\n\n    Return -1 on failure.\n\n    \"\"\"\n    return s.find(*args)\n\n# Find last substring, return -1 if not found\ndef rfind(s, *args):\n    \"\"\"rfind(s, sub [,start [,end]]) -> int\n\n    Return the highest index in s where substring sub is found,\n    such that sub is contained within s[start,end].  Optional\n    arguments start and end are interpreted as in slice notation.\n\n    Return -1 on failure.\n\n    \"\"\"\n    return s.rfind(*args)\n\n# for a bit of speed\n_float = float\n_int = int\n_long = long\n\n# Convert string to float\ndef atof(s):\n    \"\"\"atof(s) -> float\n\n    Return the floating point number represented by the string s.\n\n    \"\"\"\n    return _float(s)\n\n\n# Convert string to integer\ndef atoi(s , base=10):\n    \"\"\"atoi(s [,base]) -> int\n\n    Return the integer represented by the string s in the given\n    base, which defaults to 10.  The string s must consist of one\n    or more digits, possibly preceded by a sign.  If base is 0, it\n    is chosen from the leading characters of s, 0 for octal, 0x or\n    0X for hexadecimal.  If base is 16, a preceding 0x or 0X is\n    accepted.\n\n    \"\"\"\n    return _int(s, base)\n\n\n# Convert string to long integer\ndef atol(s, base=10):\n    \"\"\"atol(s [,base]) -> long\n\n    Return the long integer represented by the string s in the\n    given base, which defaults to 10.  The string s must consist\n    of one or more digits, possibly preceded by a sign.  If base\n    is 0, it is chosen from the leading characters of s, 0 for\n    octal, 0x or 0X for hexadecimal.  If base is 16, a preceding\n    0x or 0X is accepted.  A trailing L or l is not accepted,\n    unless base is 0.\n\n    \"\"\"\n    return _long(s, base)\n\n\n# Left-justify a string\ndef ljust(s, width, *args):\n    \"\"\"ljust(s, width[, fillchar]) -> string\n\n    Return a left-justified version of s, in a field of the\n    specified width, padded with spaces as needed.  The string is\n    never truncated.  If specified the fillchar is used instead of spaces.\n\n    \"\"\"\n    return s.ljust(width, *args)\n\n# Right-justify a string\ndef rjust(s, width, *args):\n    \"\"\"rjust(s, width[, fillchar]) -> string\n\n    Return a right-justified version of s, in a field of the\n    specified width, padded with spaces as needed.  The string is\n    never truncated.  If specified the fillchar is used instead of spaces.\n\n    \"\"\"\n    return s.rjust(width, *args)\n\n# Center a string\ndef center(s, width, *args):\n    \"\"\"center(s, width[, fillchar]) -> string\n\n    Return a center version of s, in a field of the specified\n    width. padded with spaces as needed.  The string is never\n    truncated.  If specified the fillchar is used instead of spaces.\n\n    \"\"\"\n    return s.center(width, *args)\n\n# Zero-fill a number, e.g., (12, 3) --> '012' and (-3, 3) --> '-03'\n# Decadent feature: the argument may be a string or a number\n# (Use of this is deprecated; it should be a string as with ljust c.s.)\ndef zfill(x, width):\n    \"\"\"zfill(x, width) -> string\n\n    Pad a numeric string x with zeros on the left, to fill a field\n    of the specified width.  The string x is never truncated.\n\n    \"\"\"\n    if not isinstance(x, basestring):\n        x = repr(x)\n    return x.zfill(width)\n\n# Expand tabs in a string.\n# Doesn't take non-printing chars into account, but does understand \\n.\ndef expandtabs(s, tabsize=8):\n    \"\"\"expandtabs(s [,tabsize]) -> string\n\n    Return a copy of the string s with all tab characters replaced\n    by the appropriate number of spaces, depending on the current\n    column, and the tabsize (default 8).\n\n    \"\"\"\n    return s.expandtabs(tabsize)\n\n# Character translation through look-up table.\ndef translate(s, table, deletions=\"\"):\n    \"\"\"translate(s,table [,deletions]) -> string\n\n    Return a copy of the string s, where all characters occurring\n    in the optional argument deletions are removed, and the\n    remaining characters have been mapped through the given\n    translation table, which must be a string of length 256.  The\n    deletions argument is not allowed for Unicode strings.\n\n    \"\"\"\n    if deletions or table is None:\n        return s.translate(table, deletions)\n    else:\n        # Add s[:0] so that if s is Unicode and table is an 8-bit string,\n        # table is converted to Unicode.  This means that table *cannot*\n        # be a dictionary -- for that feature, use u.translate() directly.\n        return s.translate(table + s[:0])\n\n# Capitalize a string, e.g. \"aBc  dEf\" -> \"Abc  def\".\ndef capitalize(s):\n    \"\"\"capitalize(s) -> string\n\n    Return a copy of the string s with only its first character\n    capitalized.\n\n    \"\"\"\n    return s.capitalize()\n\n# Substring replacement (global)\ndef replace(s, old, new, maxreplace=-1):\n    \"\"\"replace (str, old, new[, maxreplace]) -> string\n\n    Return a copy of string str with all occurrences of substring\n    old replaced by new. If the optional argument maxreplace is\n    given, only the first maxreplace occurrences are replaced.\n\n    \"\"\"\n    return s.replace(old, new, maxreplace)\n\n\n# Try importing optional built-in module \"strop\" -- if it exists,\n# it redefines some string operations that are 100-1000 times faster.\n# It also defines values for whitespace, lowercase and uppercase\n# that match <ctype.h>'s definitions.\n\ntry:\n    from strop import maketrans, lowercase, uppercase, whitespace\n    letters = lowercase + uppercase\nexcept ImportError:\n    pass                                          # Use the original versions\n\n########################################################################\n# the Formatter class\n# see PEP 3101 for details and purpose of this class\n\n# The hard parts are reused from the C implementation.  They're exposed as \"_\"\n# prefixed methods of str and unicode.\n\n# The overall parser is implemented in str._formatter_parser.\n# The field name parser is implemented in str._formatter_field_name_split\n\nclass Formatter(object):\n    def format(self, format_string, *args, **kwargs):\n        return self.vformat(format_string, args, kwargs)\n\n    def vformat(self, format_string, args, kwargs):\n        used_args = set()\n        result = self._vformat(format_string, args, kwargs, used_args, 2)\n        self.check_unused_args(used_args, args, kwargs)\n        return result\n\n    def _vformat(self, format_string, args, kwargs, used_args, recursion_depth):\n        if recursion_depth < 0:\n            raise ValueError('Max string recursion exceeded')\n        result = []\n        for literal_text, field_name, format_spec, conversion in \\\n                self.parse(format_string):\n\n            # output the literal text\n            if literal_text:\n                result.append(literal_text)\n\n            # if there's a field, output it\n            if field_name is not None:\n                # this is some markup, find the object and do\n                #  the formatting\n\n                # given the field_name, find the object it references\n                #  and the argument it came from\n                obj, arg_used = self.get_field(field_name, args, kwargs)\n                used_args.add(arg_used)\n\n                # do any conversion on the resulting object\n                obj = self.convert_field(obj, conversion)\n\n                # expand the format spec, if needed\n                format_spec = self._vformat(format_spec, args, kwargs,\n                                            used_args, recursion_depth-1)\n\n                # format the object and append to the result\n                result.append(self.format_field(obj, format_spec))\n\n        return ''.join(result)\n\n\n    def get_value(self, key, args, kwargs):\n        if isinstance(key, (int, long)):\n            return args[key]\n        else:\n            return kwargs[key]\n\n\n    def check_unused_args(self, used_args, args, kwargs):\n        pass\n\n\n    def format_field(self, value, format_spec):\n        return format(value, format_spec)\n\n\n    def convert_field(self, value, conversion):\n        # do any conversion on the resulting object\n        if conversion is None:\n            return value\n        elif conversion == 's':\n            return str(value)\n        elif conversion == 'r':\n            return repr(value)\n        raise ValueError(\"Unknown conversion specifier {0!s}\".format(conversion))\n\n\n    # returns an iterable that contains tuples of the form:\n    # (literal_text, field_name, format_spec, conversion)\n    # literal_text can be zero length\n    # field_name can be None, in which case there's no\n    #  object to format and output\n    # if field_name is not None, it is looked up, formatted\n    #  with format_spec and conversion and then used\n    def parse(self, format_string):\n        return format_string._formatter_parser()\n\n\n    # given a field_name, find the object it references.\n    #  field_name:   the field being looked up, e.g. \"0.name\"\n    #                 or \"lookup[3]\"\n    #  used_args:    a set of which args have been used\n    #  args, kwargs: as passed in to vformat\n    def get_field(self, field_name, args, kwargs):\n        first, rest = field_name._formatter_field_name_split()\n\n        obj = self.get_value(first, args, kwargs)\n\n        # loop through the rest of the field_name, doing\n        #  getattr or getitem as needed\n        for is_attr, i in rest:\n            if is_attr:\n                obj = getattr(obj, i)\n            else:\n                obj = obj[i]\n\n        return obj, first\n",
		"file_name": "string.py"
	},
	{
		"content": "\"\"\"Text wrapping and filling.\n\"\"\"\n\n# Copyright (C) 1999-2001 Gregory P. Ward.\n# Copyright (C) 2002, 2003 Python Software Foundation.\n# Written by Greg Ward <gward@python.net>\n\n__revision__ = \"$Id$\"\n\nimport string, re\n\ntry:\n    _unicode = unicode\nexcept NameError:\n    # If Python is built without Unicode support, the unicode type\n    # will not exist. Fake one.\n    class _unicode(object):\n        pass\n\n# Do the right thing with boolean values for all known Python versions\n# (so this module can be copied to projects that don't depend on Python\n# 2.3, e.g. Optik and Docutils) by uncommenting the block of code below.\n#try:\n#    True, False\n#except NameError:\n#    (True, False) = (1, 0)\n\n__all__ = ['TextWrapper', 'wrap', 'fill', 'dedent']\n\n# Hardcode the recognized whitespace characters to the US-ASCII\n# whitespace characters.  The main reason for doing this is that in\n# ISO-8859-1, 0xa0 is non-breaking whitespace, so in certain locales\n# that character winds up in string.whitespace.  Respecting\n# string.whitespace in those cases would 1) make textwrap treat 0xa0 the\n# same as any other whitespace char, which is clearly wrong (it's a\n# *non-breaking* space), 2) possibly cause problems with Unicode,\n# since 0xa0 is not in range(128).\n_whitespace = '\\t\\n\\x0b\\x0c\\r '\n\nclass TextWrapper:\n    \"\"\"\n    Object for wrapping/filling text.  The public interface consists of\n    the wrap() and fill() methods; the other methods are just there for\n    subclasses to override in order to tweak the default behaviour.\n    If you want to completely replace the main wrapping algorithm,\n    you'll probably have to override _wrap_chunks().\n\n    Several instance attributes control various aspects of wrapping:\n      width (default: 70)\n        the maximum width of wrapped lines (unless break_long_words\n        is false)\n      initial_indent (default: \"\")\n        string that will be prepended to the first line of wrapped\n        output.  Counts towards the line's width.\n      subsequent_indent (default: \"\")\n        string that will be prepended to all lines save the first\n        of wrapped output; also counts towards each line's width.\n      expand_tabs (default: true)\n        Expand tabs in input text to spaces before further processing.\n        Each tab will become 1 .. 8 spaces, depending on its position in\n        its line.  If false, each tab is treated as a single character.\n      replace_whitespace (default: true)\n        Replace all whitespace characters in the input text by spaces\n        after tab expansion.  Note that if expand_tabs is false and\n        replace_whitespace is true, every tab will be converted to a\n        single space!\n      fix_sentence_endings (default: false)\n        Ensure that sentence-ending punctuation is always followed\n        by two spaces.  Off by default because the algorithm is\n        (unavoidably) imperfect.\n      break_long_words (default: true)\n        Break words longer than 'width'.  If false, those words will not\n        be broken, and some lines might be longer than 'width'.\n      break_on_hyphens (default: true)\n        Allow breaking hyphenated words. If true, wrapping will occur\n        preferably on whitespaces and right after hyphens part of\n        compound words.\n      drop_whitespace (default: true)\n        Drop leading and trailing whitespace from lines.\n    \"\"\"\n\n    whitespace_trans = string.maketrans(_whitespace, ' ' * len(_whitespace))\n\n    unicode_whitespace_trans = {}\n    uspace = ord(u' ')\n    for x in map(ord, _whitespace):\n        unicode_whitespace_trans[x] = uspace\n\n    # This funky little regex is just the trick for splitting\n    # text up into word-wrappable chunks.  E.g.\n    #   \"Hello there -- you goof-ball, use the -b option!\"\n    # splits into\n    #   Hello/ /there/ /--/ /you/ /goof-/ball,/ /use/ /the/ /-b/ /option!\n    # (after stripping out empty strings).\n    wordsep_re = re.compile(\n        r'(\\s+|'                                  # any whitespace\n        r'[^\\s\\w]*\\w+[^0-9\\W]-(?=\\w+[^0-9\\W])|'   # hyphenated words\n        r'(?<=[\\w\\!\\\"\\'\\&\\.\\,\\?])-{2,}(?=\\w))')   # em-dash\n\n    # This less funky little regex just split on recognized spaces. E.g.\n    #   \"Hello there -- you goof-ball, use the -b option!\"\n    # splits into\n    #   Hello/ /there/ /--/ /you/ /goof-ball,/ /use/ /the/ /-b/ /option!/\n    wordsep_simple_re = re.compile(r'(\\s+)')\n\n    # XXX this is not locale- or charset-aware -- string.lowercase\n    # is US-ASCII only (and therefore English-only)\n    sentence_end_re = re.compile(r'[%s]'              # lowercase letter\n                                 r'[\\.\\!\\?]'          # sentence-ending punct.\n                                 r'[\\\"\\']?'           # optional end-of-quote\n                                 r'\\Z'                # end of chunk\n                                 % string.lowercase)\n\n\n    def __init__(self,\n                 width=70,\n                 initial_indent=\"\",\n                 subsequent_indent=\"\",\n                 expand_tabs=True,\n                 replace_whitespace=True,\n                 fix_sentence_endings=False,\n                 break_long_words=True,\n                 drop_whitespace=True,\n                 break_on_hyphens=True):\n        self.width = width\n        self.initial_indent = initial_indent\n        self.subsequent_indent = subsequent_indent\n        self.expand_tabs = expand_tabs\n        self.replace_whitespace = replace_whitespace\n        self.fix_sentence_endings = fix_sentence_endings\n        self.break_long_words = break_long_words\n        self.drop_whitespace = drop_whitespace\n        self.break_on_hyphens = break_on_hyphens\n\n        # recompile the regexes for Unicode mode -- done in this clumsy way for\n        # backwards compatibility because it's rather common to monkey-patch\n        # the TextWrapper class' wordsep_re attribute.\n        self.wordsep_re_uni = re.compile(self.wordsep_re.pattern, re.U)\n        self.wordsep_simple_re_uni = re.compile(\n            self.wordsep_simple_re.pattern, re.U)\n\n\n    # -- Private methods -----------------------------------------------\n    # (possibly useful for subclasses to override)\n\n    def _munge_whitespace(self, text):\n        \"\"\"_munge_whitespace(text : string) -> string\n\n        Munge whitespace in text: expand tabs and convert all other\n        whitespace characters to spaces.  Eg. \" foo\\tbar\\n\\nbaz\"\n        becomes \" foo    bar  baz\".\n        \"\"\"\n        if self.expand_tabs:\n            text = text.expandtabs()\n        if self.replace_whitespace:\n            if isinstance(text, str):\n                text = text.translate(self.whitespace_trans)\n            elif isinstance(text, _unicode):\n                text = text.translate(self.unicode_whitespace_trans)\n        return text\n\n\n    def _split(self, text):\n        \"\"\"_split(text : string) -> [string]\n\n        Split the text to wrap into indivisible chunks.  Chunks are\n        not quite the same as words; see _wrap_chunks() for full\n        details.  As an example, the text\n          Look, goof-ball -- use the -b option!\n        breaks into the following chunks:\n          'Look,', ' ', 'goof-', 'ball', ' ', '--', ' ',\n          'use', ' ', 'the', ' ', '-b', ' ', 'option!'\n        if break_on_hyphens is True, or in:\n          'Look,', ' ', 'goof-ball', ' ', '--', ' ',\n          'use', ' ', 'the', ' ', '-b', ' ', option!'\n        otherwise.\n        \"\"\"\n        if isinstance(text, _unicode):\n            if self.break_on_hyphens:\n                pat = self.wordsep_re_uni\n            else:\n                pat = self.wordsep_simple_re_uni\n        else:\n            if self.break_on_hyphens:\n                pat = self.wordsep_re\n            else:\n                pat = self.wordsep_simple_re\n        chunks = pat.split(text)\n        chunks = filter(None, chunks)  # remove empty chunks\n        return chunks\n\n    def _fix_sentence_endings(self, chunks):\n        \"\"\"_fix_sentence_endings(chunks : [string])\n\n        Correct for sentence endings buried in 'chunks'.  Eg. when the\n        original text contains \"... foo.\\nBar ...\", munge_whitespace()\n        and split() will convert that to [..., \"foo.\", \" \", \"Bar\", ...]\n        which has one too few spaces; this method simply changes the one\n        space to two.\n        \"\"\"\n        i = 0\n        patsearch = self.sentence_end_re.search\n        while i < len(chunks)-1:\n            if chunks[i+1] == \" \" and patsearch(chunks[i]):\n                chunks[i+1] = \"  \"\n                i += 2\n            else:\n                i += 1\n\n    def _handle_long_word(self, reversed_chunks, cur_line, cur_len, width):\n        \"\"\"_handle_long_word(chunks : [string],\n                             cur_line : [string],\n                             cur_len : int, width : int)\n\n        Handle a chunk of text (most likely a word, not whitespace) that\n        is too long to fit in any line.\n        \"\"\"\n        # Figure out when indent is larger than the specified width, and make\n        # sure at least one character is stripped off on every pass\n        if width < 1:\n            space_left = 1\n        else:\n            space_left = width - cur_len\n\n        # If we're allowed to break long words, then do so: put as much\n        # of the next chunk onto the current line as will fit.\n        if self.break_long_words:\n            cur_line.append(reversed_chunks[-1][:space_left])\n            reversed_chunks[-1] = reversed_chunks[-1][space_left:]\n\n        # Otherwise, we have to preserve the long word intact.  Only add\n        # it to the current line if there's nothing already there --\n        # that minimizes how much we violate the width constraint.\n        elif not cur_line:\n            cur_line.append(reversed_chunks.pop())\n\n        # If we're not allowed to break long words, and there's already\n        # text on the current line, do nothing.  Next time through the\n        # main loop of _wrap_chunks(), we'll wind up here again, but\n        # cur_len will be zero, so the next line will be entirely\n        # devoted to the long word that we can't handle right now.\n\n    def _wrap_chunks(self, chunks):\n        \"\"\"_wrap_chunks(chunks : [string]) -> [string]\n\n        Wrap a sequence of text chunks and return a list of lines of\n        length 'self.width' or less.  (If 'break_long_words' is false,\n        some lines may be longer than this.)  Chunks correspond roughly\n        to words and the whitespace between them: each chunk is\n        indivisible (modulo 'break_long_words'), but a line break can\n        come between any two chunks.  Chunks should not have internal\n        whitespace; ie. a chunk is either all whitespace or a \"word\".\n        Whitespace chunks will be removed from the beginning and end of\n        lines, but apart from that whitespace is preserved.\n        \"\"\"\n        lines = []\n        if self.width <= 0:\n            raise ValueError(\"invalid width %r (must be > 0)\" % self.width)\n\n        # Arrange in reverse order so items can be efficiently popped\n        # from a stack of chucks.\n        chunks.reverse()\n\n        while chunks:\n\n            # Start the list of chunks that will make up the current line.\n            # cur_len is just the length of all the chunks in cur_line.\n            cur_line = []\n            cur_len = 0\n\n            # Figure out which static string will prefix this line.\n            if lines:\n                indent = self.subsequent_indent\n            else:\n                indent = self.initial_indent\n\n            # Maximum width for this line.\n            width = self.width - len(indent)\n\n            # First chunk on line is whitespace -- drop it, unless this\n            # is the very beginning of the text (ie. no lines started yet).\n            if self.drop_whitespace and chunks[-1].strip() == '' and lines:\n                del chunks[-1]\n\n            while chunks:\n                l = len(chunks[-1])\n\n                # Can at least squeeze this chunk onto the current line.\n                if cur_len + l <= width:\n                    cur_line.append(chunks.pop())\n                    cur_len += l\n\n                # Nope, this line is full.\n                else:\n                    break\n\n            # The current line is full, and the next chunk is too big to\n            # fit on *any* line (not just this one).\n            if chunks and len(chunks[-1]) > width:\n                self._handle_long_word(chunks, cur_line, cur_len, width)\n\n            # If the last chunk on this line is all whitespace, drop it.\n            if self.drop_whitespace and cur_line and cur_line[-1].strip() == '':\n                del cur_line[-1]\n\n            # Convert current line back to a string and store it in list\n            # of all lines (return value).\n            if cur_line:\n                lines.append(indent + ''.join(cur_line))\n\n        return lines\n\n\n    # -- Public interface ----------------------------------------------\n\n    def wrap(self, text):\n        \"\"\"wrap(text : string) -> [string]\n\n        Reformat the single paragraph in 'text' so it fits in lines of\n        no more than 'self.width' columns, and return a list of wrapped\n        lines.  Tabs in 'text' are expanded with string.expandtabs(),\n        and all other whitespace characters (including newline) are\n        converted to space.\n        \"\"\"\n        text = self._munge_whitespace(text)\n        chunks = self._split(text)\n        if self.fix_sentence_endings:\n            self._fix_sentence_endings(chunks)\n        return self._wrap_chunks(chunks)\n\n    def fill(self, text):\n        \"\"\"fill(text : string) -> string\n\n        Reformat the single paragraph in 'text' to fit in lines of no\n        more than 'self.width' columns, and return a new string\n        containing the entire wrapped paragraph.\n        \"\"\"\n        return \"\\n\".join(self.wrap(text))\n\n\n# -- Convenience interface ---------------------------------------------\n\ndef wrap(text, width=70, **kwargs):\n    \"\"\"Wrap a single paragraph of text, returning a list of wrapped lines.\n\n    Reformat the single paragraph in 'text' so it fits in lines of no\n    more than 'width' columns, and return a list of wrapped lines.  By\n    default, tabs in 'text' are expanded with string.expandtabs(), and\n    all other whitespace characters (including newline) are converted to\n    space.  See TextWrapper class for available keyword args to customize\n    wrapping behaviour.\n    \"\"\"\n    w = TextWrapper(width=width, **kwargs)\n    return w.wrap(text)\n\ndef fill(text, width=70, **kwargs):\n    \"\"\"Fill a single paragraph of text, returning a new string.\n\n    Reformat the single paragraph in 'text' to fit in lines of no more\n    than 'width' columns, and return a new string containing the entire\n    wrapped paragraph.  As with wrap(), tabs are expanded and other\n    whitespace characters converted to space.  See TextWrapper class for\n    available keyword args to customize wrapping behaviour.\n    \"\"\"\n    w = TextWrapper(width=width, **kwargs)\n    return w.fill(text)\n\n\n# -- Loosely related functionality -------------------------------------\n\n_whitespace_only_re = re.compile('^[ \\t]+$', re.MULTILINE)\n_leading_whitespace_re = re.compile('(^[ \\t]*)(?:[^ \\t\\n])', re.MULTILINE)\n\ndef dedent(text):\n    \"\"\"Remove any common leading whitespace from every line in `text`.\n\n    This can be used to make triple-quoted strings line up with the left\n    edge of the display, while still presenting them in the source code\n    in indented form.\n\n    Note that tabs and spaces are both treated as whitespace, but they\n    are not equal: the lines \"  hello\" and \"\\thello\" are\n    considered to have no common leading whitespace.  (This behaviour is\n    new in Python 2.5; older versions of this module incorrectly\n    expanded tabs before searching for common leading whitespace.)\n    \"\"\"\n    # Look for the longest leading string of spaces and tabs common to\n    # all lines.\n    margin = None\n    text = _whitespace_only_re.sub('', text)\n    indents = _leading_whitespace_re.findall(text)\n    for indent in indents:\n        if margin is None:\n            margin = indent\n\n        # Current line more deeply indented than previous winner:\n        # no change (previous winner is still on top).\n        elif indent.startswith(margin):\n            pass\n\n        # Current line consistent with and no deeper than previous winner:\n        # it's the new winner.\n        elif margin.startswith(indent):\n            margin = indent\n\n        # Current line and previous winner have no common whitespace:\n        # there is no margin.\n        else:\n            margin = \"\"\n            break\n\n    # sanity check (testing/debugging only)\n    if 0 and margin:\n        for line in text.split(\"\\n\"):\n            assert not line or line.startswith(margin), \\\n                   \"line = %r, margin = %r\" % (line, margin)\n\n    if margin:\n        text = re.sub(r'(?m)^' + margin, '', text)\n    return text\n\nif __name__ == \"__main__\":\n    #print dedent(\"\\tfoo\\n\\tbar\")\n    #print dedent(\"  \\thello there\\n  \\t  how are you?\")\n    print dedent(\"Hello there.\\n  This is indented.\")\n",
		"file_name": "textwrap.py"
	},
	{
		"content": "\"\"\"Drop-in replacement for the thread module.\n\nMeant to be used as a brain-dead substitute so that threaded code does\nnot need to be rewritten for when the thread module is not present.\n\nSuggested usage is::\n\n    try:\n        import thread\n    except ImportError:\n        import dummy_thread as thread\n\n\"\"\"\n# Exports only things specified by thread documentation;\n# skipping obsolete synonyms allocate(), start_new(), exit_thread().\n__all__ = ['error', 'start_new_thread', 'exit', 'get_ident', 'allocate_lock',\n           'interrupt_main', 'LockType']\n\nimport traceback as _traceback\n\nclass error(Exception):\n    \"\"\"Dummy implementation of thread.error.\"\"\"\n\n    def __init__(self, *args):\n        self.args = args\n\ndef start_new_thread(function, args, kwargs={}):\n    \"\"\"Dummy implementation of thread.start_new_thread().\n\n    Compatibility is maintained by making sure that ``args`` is a\n    tuple and ``kwargs`` is a dictionary.  If an exception is raised\n    and it is SystemExit (which can be done by thread.exit()) it is\n    caught and nothing is done; all other exceptions are printed out\n    by using traceback.print_exc().\n\n    If the executed function calls interrupt_main the KeyboardInterrupt will be\n    raised when the function returns.\n\n    \"\"\"\n    if type(args) != type(tuple()):\n        raise TypeError(\"2nd arg must be a tuple\")\n    if type(kwargs) != type(dict()):\n        raise TypeError(\"3rd arg must be a dict\")\n    global _main\n    _main = False\n    try:\n        function(*args, **kwargs)\n    except SystemExit:\n        pass\n    except:\n        _traceback.print_exc()\n    _main = True\n    global _interrupt\n    if _interrupt:\n        _interrupt = False\n        raise KeyboardInterrupt\n\ndef exit():\n    \"\"\"Dummy implementation of thread.exit().\"\"\"\n    raise SystemExit\n\ndef get_ident():\n    \"\"\"Dummy implementation of thread.get_ident().\n\n    Since this module should only be used when threadmodule is not\n    available, it is safe to assume that the current process is the\n    only thread.  Thus a constant can be safely returned.\n    \"\"\"\n    return -1\n\ndef allocate_lock():\n    \"\"\"Dummy implementation of thread.allocate_lock().\"\"\"\n    return LockType()\n\ndef stack_size(size=None):\n    \"\"\"Dummy implementation of thread.stack_size().\"\"\"\n    if size is not None:\n        raise error(\"setting thread stack size not supported\")\n    return 0\n\nclass LockType(object):\n    \"\"\"Class implementing dummy implementation of thread.LockType.\n\n    Compatibility is maintained by maintaining self.locked_status\n    which is a boolean that stores the state of the lock.  Pickling of\n    the lock, though, should not be done since if the thread module is\n    then used with an unpickled ``lock()`` from here problems could\n    occur from this class not having atomic methods.\n\n    \"\"\"\n\n    def __init__(self):\n        self.locked_status = False\n\n    def acquire(self, waitflag=None):\n        \"\"\"Dummy implementation of acquire().\n\n        For blocking calls, self.locked_status is automatically set to\n        True and returned appropriately based on value of\n        ``waitflag``.  If it is non-blocking, then the value is\n        actually checked and not set if it is already acquired.  This\n        is all done so that threading.Condition's assert statements\n        aren't triggered and throw a little fit.\n\n        \"\"\"\n        if waitflag is None or waitflag:\n            self.locked_status = True\n            return True\n        else:\n            if not self.locked_status:\n                self.locked_status = True\n                return True\n            else:\n                return False\n\n    __enter__ = acquire\n\n    def __exit__(self, typ, val, tb):\n        self.release()\n\n    def release(self):\n        \"\"\"Release the dummy lock.\"\"\"\n        # XXX Perhaps shouldn't actually bother to test?  Could lead\n        #     to problems for complex, threaded code.\n        if not self.locked_status:\n            raise error\n        self.locked_status = False\n        return True\n\n    def locked(self):\n        return self.locked_status\n\n# Used to signal that interrupt_main was called in a \"thread\"\n_interrupt = False\n# True when not executing in a \"thread\"\n_main = True\n\ndef interrupt_main():\n    \"\"\"Set _interrupt flag to True to have start_new_thread raise\n    KeyboardInterrupt upon exiting.\"\"\"\n    if _main:\n        raise KeyboardInterrupt\n    else:\n        global _interrupt\n        _interrupt = True\n",
		"file_name": "dummy_thread.py"
	},
	{
		"content": "# Wrapper module for _socket, providing some additional facilities\n# implemented in Python.\n\n\"\"\"\\\nThis module provides socket operations and some related functions.\nOn Unix, it supports IP (Internet Protocol) and Unix domain sockets.\nOn other systems, it only supports IP. Functions specific for a\nsocket are available as methods of the socket object.\n\nFunctions:\n\nsocket() -- create a new socket object\nsocketpair() -- create a pair of new socket objects [*]\nfromfd() -- create a socket object from an open file descriptor [*]\ngethostname() -- return the current hostname\ngethostbyname() -- map a hostname to its IP number\ngethostbyaddr() -- map an IP number or hostname to DNS info\ngetservbyname() -- map a service name and a protocol name to a port number\ngetprotobyname() -- map a protocol name (e.g. 'tcp') to a number\nntohs(), ntohl() -- convert 16, 32 bit int from network to host byte order\nhtons(), htonl() -- convert 16, 32 bit int from host to network byte order\ninet_aton() -- convert IP addr string (123.45.67.89) to 32-bit packed format\ninet_ntoa() -- convert 32-bit packed format IP to string (123.45.67.89)\nssl() -- secure socket layer support (only available if configured)\nsocket.getdefaulttimeout() -- get the default timeout value\nsocket.setdefaulttimeout() -- set the default timeout value\ncreate_connection() -- connects to an address, with an optional timeout and\n                       optional source address.\n\n [*] not available on all platforms!\n\nSpecial objects:\n\nSocketType -- type object for socket objects\nerror -- exception raised for I/O errors\nhas_ipv6 -- boolean value indicating if IPv6 is supported\n\nInteger constants:\n\nAF_INET, AF_UNIX -- socket domains (first argument to socket() call)\nSOCK_STREAM, SOCK_DGRAM, SOCK_RAW -- socket types (second argument)\n\nMany other constants may be defined; these may be used in calls to\nthe setsockopt() and getsockopt() methods.\n\"\"\"\n\nimport _socket\nfrom _socket import *\n\ntry:\n    import _ssl\nexcept ImportError:\n    # no SSL support\n    pass\nelse:\n    def ssl(sock, keyfile=None, certfile=None):\n        # we do an internal import here because the ssl\n        # module imports the socket module\n        import ssl as _realssl\n        warnings.warn(\"socket.ssl() is deprecated.  Use ssl.wrap_socket() instead.\",\n                      DeprecationWarning, stacklevel=2)\n        return _realssl.sslwrap_simple(sock, keyfile, certfile)\n\n    # we need to import the same constants we used to...\n    from _ssl import SSLError as sslerror\n    from _ssl import \\\n         RAND_add, \\\n         RAND_egd, \\\n         RAND_status, \\\n         SSL_ERROR_ZERO_RETURN, \\\n         SSL_ERROR_WANT_READ, \\\n         SSL_ERROR_WANT_WRITE, \\\n         SSL_ERROR_WANT_X509_LOOKUP, \\\n         SSL_ERROR_SYSCALL, \\\n         SSL_ERROR_SSL, \\\n         SSL_ERROR_WANT_CONNECT, \\\n         SSL_ERROR_EOF, \\\n         SSL_ERROR_INVALID_ERROR_CODE\n\nimport os, sys, warnings\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\ntry:\n    import errno\nexcept ImportError:\n    errno = None\nEBADF = getattr(errno, 'EBADF', 9)\nEINTR = getattr(errno, 'EINTR', 4)\n\n__all__ = [\"getfqdn\", \"create_connection\"]\n__all__.extend(os._get_exports_list(_socket))\n\n\n_realsocket = socket\n_type = type\n\n# WSA error codes\nif sys.platform.lower().startswith(\"win\"):\n    errorTab = {}\n    errorTab[10004] = \"The operation was interrupted.\"\n    errorTab[10009] = \"A bad file handle was passed.\"\n    errorTab[10013] = \"Permission denied.\"\n    errorTab[10014] = \"A fault occurred on the network??\" # WSAEFAULT\n    errorTab[10022] = \"An invalid operation was attempted.\"\n    errorTab[10035] = \"The socket operation would block\"\n    errorTab[10036] = \"A blocking operation is already in progress.\"\n    errorTab[10048] = \"The network address is in use.\"\n    errorTab[10054] = \"The connection has been reset.\"\n    errorTab[10058] = \"The network has been shut down.\"\n    errorTab[10060] = \"The operation timed out.\"\n    errorTab[10061] = \"Connection refused.\"\n    errorTab[10063] = \"The name is too long.\"\n    errorTab[10064] = \"The host is down.\"\n    errorTab[10065] = \"The host is unreachable.\"\n    __all__.append(\"errorTab\")\n\n\n\ndef getfqdn(name=''):\n    \"\"\"Get fully qualified domain name from name.\n\n    An empty argument is interpreted as meaning the local host.\n\n    First the hostname returned by gethostbyaddr() is checked, then\n    possibly existing aliases. In case no FQDN is available, hostname\n    from gethostname() is returned.\n    \"\"\"\n    name = name.strip()\n    if not name or name == '0.0.0.0':\n        name = gethostname()\n    try:\n        hostname, aliases, ipaddrs = gethostbyaddr(name)\n    except error:\n        pass\n    else:\n        aliases.insert(0, hostname)\n        for name in aliases:\n            if '.' in name:\n                break\n        else:\n            name = hostname\n    return name\n\n\n_socketmethods = (\n    'bind', 'connect', 'connect_ex', 'fileno', 'listen',\n    'getpeername', 'getsockname', 'getsockopt', 'setsockopt',\n    'sendall', 'setblocking',\n    'settimeout', 'gettimeout', 'shutdown')\n\nif os.name == \"nt\":\n    _socketmethods = _socketmethods + ('ioctl',)\n\nif sys.platform == \"riscos\":\n    _socketmethods = _socketmethods + ('sleeptaskw',)\n\nclass _closedsocket(object):\n    __slots__ = []\n    def _dummy(*args):\n        raise error(EBADF, 'Bad file descriptor')\n    # All _delegate_methods must also be initialized here.\n    send = recv = recv_into = sendto = recvfrom = recvfrom_into = _dummy\n    __getattr__ = _dummy\n    def _drop(self):\n        pass\n\n# Wrapper around platform socket objects. This implements\n# a platform-independent dup() functionality. The\n# implementation currently relies on reference counting\n# to close the underlying socket object.\nclass _socketobject(object):\n\n    __doc__ = _realsocket.__doc__\n\n    __slots__ = [\"_sock\", \"__weakref__\"]\n\n    def __init__(self, family=AF_INET, type=SOCK_STREAM, proto=0, _sock=None):\n        if _sock is None:\n            _sock = _realsocket(family, type, proto)\n        else:\n            # PyPy note about refcounting: implemented with _reuse()/_drop()\n            # on the class '_socket.socket'.  Python 3 did it differently\n            # with a reference counter on this class 'socket._socketobject'\n            # instead, but it is a less compatible change.\n            \n            # Note that a few libraries (like eventlet) poke at the\n            # private implementation of socket.py, passing custom\n            # objects to _socketobject().  These libraries need the\n            # following fix for use on PyPy: the custom objects need\n            # methods _reuse() and _drop() that maintains an explicit\n            # reference counter, starting at 0.  When it drops back to\n            # zero, close() must be called.\n            _sock._reuse()\n\n        self._sock = _sock\n\n    def send(self, data, flags=0):\n        return self._sock.send(data, flags)\n    send.__doc__ = _realsocket.send.__doc__\n\n    def recv(self, buffersize, flags=0):\n        return self._sock.recv(buffersize, flags)\n    recv.__doc__ = _realsocket.recv.__doc__\n\n    def recv_into(self, buffer, nbytes=0, flags=0):\n        return self._sock.recv_into(buffer, nbytes, flags)\n    recv_into.__doc__ = _realsocket.recv_into.__doc__\n\n    def recvfrom(self, buffersize, flags=0):\n        return self._sock.recvfrom(buffersize, flags)\n    recvfrom.__doc__ = _realsocket.recvfrom.__doc__\n\n    def recvfrom_into(self, buffer, nbytes=0, flags=0):\n        return self._sock.recvfrom_into(buffer, nbytes, flags)\n    recvfrom_into.__doc__ = _realsocket.recvfrom_into.__doc__\n\n    def sendto(self, data, param2, param3=None):\n        if param3 is None:\n            return self._sock.sendto(data, param2)\n        else:\n            return self._sock.sendto(data, param2, param3)\n    sendto.__doc__ = _realsocket.sendto.__doc__\n\n    def close(self):\n        s = self._sock\n        self._sock = _closedsocket()\n        s._drop()\n    close.__doc__ = _realsocket.close.__doc__\n\n    def accept(self):\n        sock, addr = self._sock.accept()\n        sockobj = _socketobject(_sock=sock)\n        sock._drop()    # already a copy in the _socketobject()\n        return sockobj, addr\n    accept.__doc__ = _realsocket.accept.__doc__\n\n    def dup(self):\n        \"\"\"dup() -> socket object\n\n        Return a new socket object connected to the same system resource.\"\"\"\n        return _socketobject(_sock=self._sock)\n\n    def makefile(self, mode='r', bufsize=-1):\n        \"\"\"makefile([mode[, bufsize]]) -> file object\n\n        Return a regular file object corresponding to the socket.  The mode\n        and bufsize arguments are as for the built-in open() function.\"\"\"\n        return _fileobject(self._sock, mode, bufsize)\n\n    family = property(lambda self: self._sock.family, doc=\"the socket family\")\n    type = property(lambda self: self._sock.type, doc=\"the socket type\")\n    proto = property(lambda self: self._sock.proto, doc=\"the socket protocol\")\n\n    # Delegate many calls to the raw socket object.\n    _s = (\"def %(name)s(self, %(args)s): return self._sock.%(name)s(%(args)s)\\n\\n\"\n          \"%(name)s.__doc__ = _realsocket.%(name)s.__doc__\\n\")\n    for _m in _socketmethods:\n        # yupi! we're on pypy, all code objects have this interface\n        argcount = getattr(_realsocket, _m).im_func.func_code.co_argcount - 1\n        exec _s % {'name': _m, 'args': ', '.join('arg%d' % i for i in range(argcount))}\n    del _m, _s, argcount\n\n    # Delegation methods with default arguments, that the code above\n    # cannot handle correctly\n    def sendall(self, data, flags=0):\n        self._sock.sendall(data, flags)\n    sendall.__doc__ = _realsocket.sendall.__doc__\n\n    def getsockopt(self, level, optname, buflen=None):\n        if buflen is None:\n            return self._sock.getsockopt(level, optname)\n        return self._sock.getsockopt(level, optname, buflen)\n    getsockopt.__doc__ = _realsocket.getsockopt.__doc__\n\nsocket = SocketType = _socketobject\n\nclass _fileobject(object):\n    \"\"\"Faux file object attached to a socket object.\"\"\"\n\n    default_bufsize = 8192\n    name = \"<socket>\"\n\n    __slots__ = [\"mode\", \"bufsize\", \"softspace\",\n                 # \"closed\" is a property, see below\n                 \"_sock\", \"_rbufsize\", \"_wbufsize\", \"_rbuf\", \"_wbuf\", \"_wbuf_len\",\n                 \"_close\"]\n\n    def __init__(self, sock, mode='rb', bufsize=-1, close=False):\n        # Note that a few libraries (like eventlet) poke at the\n        # private implementation of socket.py, passing custom\n        # objects to _fileobject().  These libraries need the\n        # following fix for use on PyPy: the custom objects need\n        # methods _reuse() and _drop() that maintains an explicit\n        # reference counter, starting at 0.  When it drops back to\n        # zero, close() must be called.\n        sock._reuse()\n        self._sock = sock\n        self.mode = mode # Not actually used in this version\n        if bufsize < 0:\n            bufsize = self.default_bufsize\n        self.bufsize = bufsize\n        self.softspace = False\n        # _rbufsize is the suggested recv buffer size.  It is *strictly*\n        # obeyed within readline() for recv calls.  If it is larger than\n        # default_bufsize it will be used for recv calls within read().\n        if bufsize == 0:\n            self._rbufsize = 1\n        elif bufsize == 1:\n            self._rbufsize = self.default_bufsize\n        else:\n            self._rbufsize = bufsize\n        self._wbufsize = bufsize\n        # We use StringIO for the read buffer to avoid holding a list\n        # of variously sized string objects which have been known to\n        # fragment the heap due to how they are malloc()ed and often\n        # realloc()ed down much smaller than their original allocation.\n        self._rbuf = StringIO()\n        self._wbuf = [] # A list of strings\n        self._wbuf_len = 0\n        self._close = close\n\n    def _getclosed(self):\n        return self._sock is None\n    closed = property(_getclosed, doc=\"True if the file is closed\")\n\n    def close(self):\n        try:\n            if self._sock:\n                self.flush()\n        finally:\n            s = self._sock\n            self._sock = None\n            if s is not None:\n                if self._close:\n                    s.close()\n                else:\n                    s._drop()\n\n    def __del__(self):\n        try:\n            self.close()\n        except:\n            # close() may fail if __init__ didn't complete\n            pass\n\n    def flush(self):\n        if self._wbuf:\n            data = \"\".join(self._wbuf)\n            self._wbuf = []\n            self._wbuf_len = 0\n            buffer_size = max(self._rbufsize, self.default_bufsize)\n            data_size = len(data)\n            write_offset = 0\n            view = memoryview(data)\n            try:\n                while write_offset < data_size:\n                    self._sock.sendall(view[write_offset:write_offset+buffer_size])\n                    write_offset += buffer_size\n            finally:\n                if write_offset < data_size:\n                    remainder = data[write_offset:]\n                    del view, data  # explicit free\n                    self._wbuf.append(remainder)\n                    self._wbuf_len = len(remainder)\n\n    def fileno(self):\n        return self._sock.fileno()\n\n    def write(self, data):\n        data = str(data) # XXX Should really reject non-string non-buffers\n        if not data:\n            return\n        self._wbuf.append(data)\n        self._wbuf_len += len(data)\n        if (self._wbufsize == 0 or\n            (self._wbufsize == 1 and '\\n' in data) or\n            (self._wbufsize > 1 and self._wbuf_len >= self._wbufsize)):\n            self.flush()\n\n    def writelines(self, list):\n        # XXX We could do better here for very long lists\n        # XXX Should really reject non-string non-buffers\n        lines = filter(None, map(str, list))\n        self._wbuf_len += sum(map(len, lines))\n        self._wbuf.extend(lines)\n        if (self._wbufsize <= 1 or\n            self._wbuf_len >= self._wbufsize):\n            self.flush()\n\n    def read(self, size=-1):\n        # Use max, disallow tiny reads in a loop as they are very inefficient.\n        # We never leave read() with any leftover data from a new recv() call\n        # in our internal buffer.\n        rbufsize = max(self._rbufsize, self.default_bufsize)\n        # Our use of StringIO rather than lists of string objects returned by\n        # recv() minimizes memory usage and fragmentation that occurs when\n        # rbufsize is large compared to the typical return value of recv().\n        buf = self._rbuf\n        buf.seek(0, 2)  # seek end\n        if size < 0:\n            # Read until EOF\n            self._rbuf = StringIO()  # reset _rbuf.  we consume it via buf.\n            while True:\n                try:\n                    data = self._sock.recv(rbufsize)\n                except error, e:\n                    if e.args[0] == EINTR:\n                        continue\n                    raise\n                if not data:\n                    break\n                buf.write(data)\n            return buf.getvalue()\n        else:\n            # Read until size bytes or EOF seen, whichever comes first\n            buf_len = buf.tell()\n            if buf_len >= size:\n                # Already have size bytes in our buffer?  Extract and return.\n                buf.seek(0)\n                rv = buf.read(size)\n                self._rbuf = StringIO()\n                self._rbuf.write(buf.read())\n                return rv\n\n            self._rbuf = StringIO()  # reset _rbuf.  we consume it via buf.\n            while True:\n                left = size - buf_len\n                # recv() will malloc the amount of memory given as its\n                # parameter even though it often returns much less data\n                # than that.  The returned data string is short lived\n                # as we copy it into a StringIO and free it.  This avoids\n                # fragmentation issues on many platforms.\n                try:\n                    data = self._sock.recv(left)\n                except error, e:\n                    if e.args[0] == EINTR:\n                        continue\n                    raise\n                if not data:\n                    break\n                n = len(data)\n                if n == size and not buf_len:\n                    # Shortcut.  Avoid buffer data copies when:\n                    # - We have no data in our buffer.\n                    # AND\n                    # - Our call to recv returned exactly the\n                    #   number of bytes we were asked to read.\n                    return data\n                if n == left:\n                    buf.write(data)\n                    del data  # explicit free\n                    break\n                assert n <= left, \"recv(%d) returned %d bytes\" % (left, n)\n                buf.write(data)\n                buf_len += n\n                del data  # explicit free\n                #assert buf_len == buf.tell()\n            return buf.getvalue()\n\n    def readline(self, size=-1):\n        buf = self._rbuf\n        buf.seek(0, 2)  # seek end\n        if buf.tell() > 0:\n            # check if we already have it in our buffer\n            buf.seek(0)\n            bline = buf.readline(size)\n            if bline.endswith('\\n') or len(bline) == size:\n                self._rbuf = StringIO()\n                self._rbuf.write(buf.read())\n                return bline\n            del bline\n        if size < 0:\n            # Read until \\n or EOF, whichever comes first\n            if self._rbufsize <= 1:\n                # Speed up unbuffered case\n                buf.seek(0)\n                buffers = [buf.read()]\n                self._rbuf = StringIO()  # reset _rbuf.  we consume it via buf.\n                data = None\n                recv = self._sock.recv\n                while True:\n                    try:\n                        while data != \"\\n\":\n                            data = recv(1)\n                            if not data:\n                                break\n                            buffers.append(data)\n                    except error, e:\n                        # The try..except to catch EINTR was moved outside the\n                        # recv loop to avoid the per byte overhead.\n                        if e.args[0] == EINTR:\n                            continue\n                        raise\n                    break\n                return \"\".join(buffers)\n\n            buf.seek(0, 2)  # seek end\n            self._rbuf = StringIO()  # reset _rbuf.  we consume it via buf.\n            while True:\n                try:\n                    data = self._sock.recv(self._rbufsize)\n                except error, e:\n                    if e.args[0] == EINTR:\n                        continue\n                    raise\n                if not data:\n                    break\n                nl = data.find('\\n')\n                if nl >= 0:\n                    nl += 1\n                    buf.write(data[:nl])\n                    self._rbuf.write(data[nl:])\n                    del data\n                    break\n                buf.write(data)\n            return buf.getvalue()\n        else:\n            # Read until size bytes or \\n or EOF seen, whichever comes first\n            buf.seek(0, 2)  # seek end\n            buf_len = buf.tell()\n            if buf_len >= size:\n                buf.seek(0)\n                rv = buf.read(size)\n                self._rbuf = StringIO()\n                self._rbuf.write(buf.read())\n                return rv\n            self._rbuf = StringIO()  # reset _rbuf.  we consume it via buf.\n            while True:\n                try:\n                    data = self._sock.recv(self._rbufsize)\n                except error, e:\n                    if e.args[0] == EINTR:\n                        continue\n                    raise\n                if not data:\n                    break\n                left = size - buf_len\n                # did we just receive a newline?\n                nl = data.find('\\n', 0, left)\n                if nl >= 0:\n                    nl += 1\n                    # save the excess data to _rbuf\n                    self._rbuf.write(data[nl:])\n                    if buf_len:\n                        buf.write(data[:nl])\n                        break\n                    else:\n                        # Shortcut.  Avoid data copy through buf when returning\n                        # a substring of our first recv().\n                        return data[:nl]\n                n = len(data)\n                if n == size and not buf_len:\n                    # Shortcut.  Avoid data copy through buf when\n                    # returning exactly all of our first recv().\n                    return data\n                if n >= left:\n                    buf.write(data[:left])\n                    self._rbuf.write(data[left:])\n                    break\n                buf.write(data)\n                buf_len += n\n                #assert buf_len == buf.tell()\n            return buf.getvalue()\n\n    def readlines(self, sizehint=0):\n        total = 0\n        list = []\n        while True:\n            line = self.readline()\n            if not line:\n                break\n            list.append(line)\n            total += len(line)\n            if sizehint and total >= sizehint:\n                break\n        return list\n\n    # Iterator protocols\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        line = self.readline()\n        if not line:\n            raise StopIteration\n        return line\n\n_GLOBAL_DEFAULT_TIMEOUT = object()\n\ndef create_connection(address, timeout=_GLOBAL_DEFAULT_TIMEOUT,\n                      source_address=None):\n    \"\"\"Connect to *address* and return the socket object.\n\n    Convenience function.  Connect to *address* (a 2-tuple ``(host,\n    port)``) and return the socket object.  Passing the optional\n    *timeout* parameter will set the timeout on the socket instance\n    before attempting to connect.  If no *timeout* is supplied, the\n    global default timeout setting returned by :func:`getdefaulttimeout`\n    is used.  If *source_address* is set it must be a tuple of (host, port)\n    for the socket to bind as a source address before making the connection.\n    An host of '' or port 0 tells the OS to use the default.\n    \"\"\"\n\n    host, port = address\n    err = None\n    for res in getaddrinfo(host, port, 0, SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            sock = socket(af, socktype, proto)\n            if timeout is not _GLOBAL_DEFAULT_TIMEOUT:\n                sock.settimeout(timeout)\n            if source_address:\n                sock.bind(source_address)\n            sock.connect(sa)\n            return sock\n\n        except error as _:\n            err = _\n            if sock is not None:\n                sock.close()\n\n    if err is not None:\n        raise err\n    else:\n        raise error(\"getaddrinfo returns an empty list\")\n",
		"file_name": "socket.py"
	},
	{
		"content": "\"\"\"Temporary files.\n\nThis module provides generic, low- and high-level interfaces for\ncreating temporary files and directories.  The interfaces listed\nas \"safe\" just below can be used without fear of race conditions.\nThose listed as \"unsafe\" cannot, and are provided for backward\ncompatibility only.\n\nThis module also provides some data items to the user:\n\n  TMP_MAX  - maximum number of names that will be tried before\n             giving up.\n  template - the default prefix for all temporary names.\n             You may change this to control the default prefix.\n  tempdir  - If this is set to a string before the first use of\n             any routine from this module, it will be considered as\n             another candidate location to store temporary files.\n\"\"\"\n\n__all__ = [\n    \"NamedTemporaryFile\", \"TemporaryFile\", # high level safe interfaces\n    \"SpooledTemporaryFile\",\n    \"mkstemp\", \"mkdtemp\",                  # low level safe interfaces\n    \"mktemp\",                              # deprecated unsafe interface\n    \"TMP_MAX\", \"gettempprefix\",            # constants\n    \"tempdir\", \"gettempdir\"\n   ]\n\n\n# Imports.\n\nimport io as _io\nimport os as _os\nimport errno as _errno\nfrom random import Random as _Random\n\ntry:\n    from cStringIO import StringIO as _StringIO\nexcept ImportError:\n    from StringIO import StringIO as _StringIO\n\ntry:\n    import fcntl as _fcntl\nexcept ImportError:\n    def _set_cloexec(fd):\n        pass\nelse:\n    def _set_cloexec(fd):\n        try:\n            flags = _fcntl.fcntl(fd, _fcntl.F_GETFD, 0)\n        except IOError:\n            pass\n        else:\n            # flags read successfully, modify\n            flags |= _fcntl.FD_CLOEXEC\n            _fcntl.fcntl(fd, _fcntl.F_SETFD, flags)\n\n\ntry:\n    import thread as _thread\nexcept ImportError:\n    import dummy_thread as _thread\n_allocate_lock = _thread.allocate_lock\n\n_text_openflags = _os.O_RDWR | _os.O_CREAT | _os.O_EXCL\nif hasattr(_os, 'O_NOINHERIT'):\n    _text_openflags |= _os.O_NOINHERIT\nif hasattr(_os, 'O_NOFOLLOW'):\n    _text_openflags |= _os.O_NOFOLLOW\n\n_bin_openflags = _text_openflags\nif hasattr(_os, 'O_BINARY'):\n    _bin_openflags |= _os.O_BINARY\n\nif hasattr(_os, 'TMP_MAX'):\n    TMP_MAX = _os.TMP_MAX\nelse:\n    TMP_MAX = 10000\n\ntemplate = \"tmp\"\n\n# Internal routines.\n\n_once_lock = _allocate_lock()\n\nif hasattr(_os, \"lstat\"):\n    _stat = _os.lstat\nelif hasattr(_os, \"stat\"):\n    _stat = _os.stat\nelse:\n    # Fallback.  All we need is something that raises os.error if the\n    # file doesn't exist.\n    def _stat(fn):\n        try:\n            f = open(fn)\n        except IOError:\n            raise _os.error\n        f.close()\n\ndef _exists(fn):\n    try:\n        _stat(fn)\n    except _os.error:\n        return False\n    else:\n        return True\n\nclass _RandomNameSequence:\n    \"\"\"An instance of _RandomNameSequence generates an endless\n    sequence of unpredictable strings which can safely be incorporated\n    into file names.  Each string is six characters long.  Multiple\n    threads can safely use the same instance at the same time.\n\n    _RandomNameSequence is an iterator.\"\"\"\n\n    characters = (\"abcdefghijklmnopqrstuvwxyz\" +\n                  \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" +\n                  \"0123456789_\")\n\n    def __init__(self):\n        self.mutex = _allocate_lock()\n        self.normcase = _os.path.normcase\n\n    @property\n    def rng(self):\n        cur_pid = _os.getpid()\n        if cur_pid != getattr(self, '_rng_pid', None):\n            self._rng = _Random()\n            self._rng_pid = cur_pid\n        return self._rng\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        m = self.mutex\n        c = self.characters\n        choose = self.rng.choice\n\n        m.acquire()\n        try:\n            letters = [choose(c) for dummy in \"123456\"]\n        finally:\n            m.release()\n\n        return self.normcase(''.join(letters))\n\ndef _candidate_tempdir_list():\n    \"\"\"Generate a list of candidate temporary directories which\n    _get_default_tempdir will try.\"\"\"\n\n    dirlist = []\n\n    # First, try the environment.\n    for envname in 'TMPDIR', 'TEMP', 'TMP':\n        dirname = _os.getenv(envname)\n        if dirname: dirlist.append(dirname)\n\n    # Failing that, try OS-specific locations.\n    if _os.name == 'riscos':\n        dirname = _os.getenv('Wimp$ScrapDir')\n        if dirname: dirlist.append(dirname)\n    elif _os.name == 'nt':\n        dirlist.extend([ r'c:\\temp', r'c:\\tmp', r'\\temp', r'\\tmp' ])\n    else:\n        dirlist.extend([ '/tmp', '/var/tmp', '/usr/tmp' ])\n\n    # As a last resort, the current directory.\n    try:\n        dirlist.append(_os.getcwd())\n    except (AttributeError, _os.error):\n        dirlist.append(_os.curdir)\n\n    return dirlist\n\ndef _get_default_tempdir():\n    \"\"\"Calculate the default directory to use for temporary files.\n    This routine should be called exactly once.\n\n    We determine whether or not a candidate temp dir is usable by\n    trying to create and write to a file in that directory.  If this\n    is successful, the test file is deleted.  To prevent denial of\n    service, the name of the test file must be randomized.\"\"\"\n\n    namer = _RandomNameSequence()\n    dirlist = _candidate_tempdir_list()\n    flags = _text_openflags\n\n    for dir in dirlist:\n        if dir != _os.curdir:\n            dir = _os.path.normcase(_os.path.abspath(dir))\n        # Try only a few names per directory.\n        for seq in xrange(100):\n            name = namer.next()\n            filename = _os.path.join(dir, name)\n            try:\n                fd = _os.open(filename, flags, 0o600)\n                try:\n                    try:\n                        with _io.open(fd, 'wb', closefd=False) as fp:\n                            fp.write(b'blat')\n                    finally:\n                        _os.close(fd)\n                finally:\n                    _os.unlink(filename)\n                return dir\n            except (OSError, IOError) as e:\n                if e.args[0] != _errno.EEXIST:\n                    break # no point trying more names in this directory\n                pass\n    raise IOError, (_errno.ENOENT,\n                    (\"No usable temporary directory found in %s\" % dirlist))\n\n_name_sequence = None\n\ndef _get_candidate_names():\n    \"\"\"Common setup sequence for all user-callable interfaces.\"\"\"\n\n    global _name_sequence\n    if _name_sequence is None:\n        _once_lock.acquire()\n        try:\n            if _name_sequence is None:\n                _name_sequence = _RandomNameSequence()\n        finally:\n            _once_lock.release()\n    return _name_sequence\n\n\ndef _mkstemp_inner(dir, pre, suf, flags):\n    \"\"\"Code common to mkstemp, TemporaryFile, and NamedTemporaryFile.\"\"\"\n\n    names = _get_candidate_names()\n\n    for seq in xrange(TMP_MAX):\n        name = names.next()\n        file = _os.path.join(dir, pre + name + suf)\n        try:\n            fd = _os.open(file, flags, 0600)\n            _set_cloexec(fd)\n            return (fd, _os.path.abspath(file))\n        except OSError, e:\n            if e.errno == _errno.EEXIST:\n                continue # try again\n            if _os.name == 'nt' and e.errno == _errno.EACCES:\n                # On windows, when a directory with the chosen name already\n                # exists, EACCES error code is returned instead of EEXIST.\n                continue\n            raise\n\n    raise IOError, (_errno.EEXIST, \"No usable temporary file name found\")\n\n\n# User visible interfaces.\n\ndef gettempprefix():\n    \"\"\"Accessor for tempdir.template.\"\"\"\n    return template\n\ntempdir = None\n\ndef gettempdir():\n    \"\"\"Accessor for tempfile.tempdir.\"\"\"\n    global tempdir\n    if tempdir is None:\n        _once_lock.acquire()\n        try:\n            if tempdir is None:\n                tempdir = _get_default_tempdir()\n        finally:\n            _once_lock.release()\n    return tempdir\n\ndef mkstemp(suffix=\"\", prefix=template, dir=None, text=False):\n    \"\"\"User-callable function to create and return a unique temporary\n    file.  The return value is a pair (fd, name) where fd is the\n    file descriptor returned by os.open, and name is the filename.\n\n    If 'suffix' is specified, the file name will end with that suffix,\n    otherwise there will be no suffix.\n\n    If 'prefix' is specified, the file name will begin with that prefix,\n    otherwise a default prefix is used.\n\n    If 'dir' is specified, the file will be created in that directory,\n    otherwise a default directory is used.\n\n    If 'text' is specified and true, the file is opened in text\n    mode.  Else (the default) the file is opened in binary mode.  On\n    some operating systems, this makes no difference.\n\n    The file is readable and writable only by the creating user ID.\n    If the operating system uses permission bits to indicate whether a\n    file is executable, the file is executable by no one. The file\n    descriptor is not inherited by children of this process.\n\n    Caller is responsible for deleting the file when done with it.\n    \"\"\"\n\n    if dir is None:\n        dir = gettempdir()\n\n    if text:\n        flags = _text_openflags\n    else:\n        flags = _bin_openflags\n\n    return _mkstemp_inner(dir, prefix, suffix, flags)\n\n\ndef mkdtemp(suffix=\"\", prefix=template, dir=None):\n    \"\"\"User-callable function to create and return a unique temporary\n    directory.  The return value is the pathname of the directory.\n\n    Arguments are as for mkstemp, except that the 'text' argument is\n    not accepted.\n\n    The directory is readable, writable, and searchable only by the\n    creating user.\n\n    Caller is responsible for deleting the directory when done with it.\n    \"\"\"\n\n    if dir is None:\n        dir = gettempdir()\n\n    names = _get_candidate_names()\n\n    for seq in xrange(TMP_MAX):\n        name = names.next()\n        file = _os.path.join(dir, prefix + name + suffix)\n        try:\n            _os.mkdir(file, 0700)\n            return file\n        except OSError, e:\n            if e.errno == _errno.EEXIST:\n                continue # try again\n            raise\n\n    raise IOError, (_errno.EEXIST, \"No usable temporary directory name found\")\n\ndef mktemp(suffix=\"\", prefix=template, dir=None):\n    \"\"\"User-callable function to return a unique temporary file name.  The\n    file is not created.\n\n    Arguments are as for mkstemp, except that the 'text' argument is\n    not accepted.\n\n    This function is unsafe and should not be used.  The file name\n    refers to a file that did not exist at some point, but by the time\n    you get around to creating it, someone else may have beaten you to\n    the punch.\n    \"\"\"\n\n##    from warnings import warn as _warn\n##    _warn(\"mktemp is a potential security risk to your program\",\n##          RuntimeWarning, stacklevel=2)\n\n    if dir is None:\n        dir = gettempdir()\n\n    names = _get_candidate_names()\n    for seq in xrange(TMP_MAX):\n        name = names.next()\n        file = _os.path.join(dir, prefix + name + suffix)\n        if not _exists(file):\n            return file\n\n    raise IOError, (_errno.EEXIST, \"No usable temporary filename found\")\n\n\nclass _TemporaryFileWrapper:\n    \"\"\"Temporary file wrapper\n\n    This class provides a wrapper around files opened for\n    temporary use.  In particular, it seeks to automatically\n    remove the file when it is no longer needed.\n    \"\"\"\n\n    def __init__(self, file, name, delete=True):\n        self.file = file\n        self.name = name\n        self.close_called = False\n        self.delete = delete\n\n    def __getattr__(self, name):\n        # Attribute lookups are delegated to the underlying file\n        # and cached for non-numeric results\n        # (i.e. methods are cached, closed and friends are not)\n        file = self.__dict__['file']\n        a = getattr(file, name)\n        if not issubclass(type(a), type(0)):\n            setattr(self, name, a)\n        return a\n\n    # The underlying __enter__ method returns the wrong object\n    # (self.file) so override it to return the wrapper\n    def __enter__(self):\n        self.file.__enter__()\n        return self\n\n    # NT provides delete-on-close as a primitive, so we don't need\n    # the wrapper to do anything special.  We still use it so that\n    # file.name is useful (i.e. not \"(fdopen)\") with NamedTemporaryFile.\n    if _os.name != 'nt':\n        # Cache the unlinker so we don't get spurious errors at\n        # shutdown when the module-level \"os\" is None'd out.  Note\n        # that this must be referenced as self.unlink, because the\n        # name TemporaryFileWrapper may also get None'd out before\n        # __del__ is called.\n        unlink = _os.unlink\n\n        def close(self):\n            if not self.close_called:\n                self.close_called = True\n                self.file.close()\n                if self.delete:\n                    self.unlink(self.name)\n\n        def __del__(self):\n            self.close()\n\n        # Need to trap __exit__ as well to ensure the file gets\n        # deleted when used in a with statement\n        def __exit__(self, exc, value, tb):\n            result = self.file.__exit__(exc, value, tb)\n            self.close()\n            return result\n    else:\n        def __exit__(self, exc, value, tb):\n            self.file.__exit__(exc, value, tb)\n\n\ndef NamedTemporaryFile(mode='w+b', bufsize=-1, suffix=\"\",\n                       prefix=template, dir=None, delete=True):\n    \"\"\"Create and return a temporary file.\n    Arguments:\n    'prefix', 'suffix', 'dir' -- as for mkstemp.\n    'mode' -- the mode argument to os.fdopen (default \"w+b\").\n    'bufsize' -- the buffer size argument to os.fdopen (default -1).\n    'delete' -- whether the file is deleted on close (default True).\n    The file is created as mkstemp() would do it.\n\n    Returns an object with a file-like interface; the name of the file\n    is accessible as file.name.  The file will be automatically deleted\n    when it is closed unless the 'delete' argument is set to False.\n    \"\"\"\n\n    if dir is None:\n        dir = gettempdir()\n\n    if 'b' in mode:\n        flags = _bin_openflags\n    else:\n        flags = _text_openflags\n\n    # Setting O_TEMPORARY in the flags causes the OS to delete\n    # the file when it is closed.  This is only supported by Windows.\n    if _os.name == 'nt' and delete:\n        flags |= _os.O_TEMPORARY\n\n    (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)\n    try:\n        file = _os.fdopen(fd, mode, bufsize)\n        return _TemporaryFileWrapper(file, name, delete)\n    except:\n        _os.close(fd)\n        raise\n\nif _os.name != 'posix' or _os.sys.platform == 'cygwin':\n    # On non-POSIX and Cygwin systems, assume that we cannot unlink a file\n    # while it is open.\n    TemporaryFile = NamedTemporaryFile\n\nelse:\n    def TemporaryFile(mode='w+b', bufsize=-1, suffix=\"\",\n                      prefix=template, dir=None):\n        \"\"\"Create and return a temporary file.\n        Arguments:\n        'prefix', 'suffix', 'dir' -- as for mkstemp.\n        'mode' -- the mode argument to os.fdopen (default \"w+b\").\n        'bufsize' -- the buffer size argument to os.fdopen (default -1).\n        The file is created as mkstemp() would do it.\n\n        Returns an object with a file-like interface.  The file has no\n        name, and will cease to exist when it is closed.\n        \"\"\"\n\n        if dir is None:\n            dir = gettempdir()\n\n        if 'b' in mode:\n            flags = _bin_openflags\n        else:\n            flags = _text_openflags\n\n        (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)\n        try:\n            _os.unlink(name)\n            return _os.fdopen(fd, mode, bufsize)\n        except:\n            _os.close(fd)\n            raise\n\nclass SpooledTemporaryFile:\n    \"\"\"Temporary file wrapper, specialized to switch from\n    StringIO to a real file when it exceeds a certain size or\n    when a fileno is needed.\n    \"\"\"\n    _rolled = False\n\n    def __init__(self, max_size=0, mode='w+b', bufsize=-1,\n                 suffix=\"\", prefix=template, dir=None):\n        self._file = _StringIO()\n        self._max_size = max_size\n        self._rolled = False\n        self._TemporaryFileArgs = (mode, bufsize, suffix, prefix, dir)\n\n    def _check(self, file):\n        if self._rolled: return\n        max_size = self._max_size\n        if max_size and file.tell() > max_size:\n            self.rollover()\n\n    def rollover(self):\n        if self._rolled: return\n        file = self._file\n        newfile = self._file = TemporaryFile(*self._TemporaryFileArgs)\n        del self._TemporaryFileArgs\n\n        newfile.write(file.getvalue())\n        newfile.seek(file.tell(), 0)\n\n        self._rolled = True\n\n    # The method caching trick from NamedTemporaryFile\n    # won't work here, because _file may change from a\n    # _StringIO instance to a real file. So we list\n    # all the methods directly.\n\n    # Context management protocol\n    def __enter__(self):\n        if self._file.closed:\n            raise ValueError(\"Cannot enter context with closed file\")\n        return self\n\n    def __exit__(self, exc, value, tb):\n        self._file.close()\n\n    # file protocol\n    def __iter__(self):\n        return self._file.__iter__()\n\n    def close(self):\n        self._file.close()\n\n    @property\n    def closed(self):\n        return self._file.closed\n\n    def fileno(self):\n        self.rollover()\n        return self._file.fileno()\n\n    def flush(self):\n        self._file.flush()\n\n    def isatty(self):\n        return self._file.isatty()\n\n    @property\n    def mode(self):\n        try:\n            return self._file.mode\n        except AttributeError:\n            return self._TemporaryFileArgs[0]\n\n    @property\n    def name(self):\n        try:\n            return self._file.name\n        except AttributeError:\n            return None\n\n    def next(self):\n        return self._file.next\n\n    def read(self, *args):\n        return self._file.read(*args)\n\n    def readline(self, *args):\n        return self._file.readline(*args)\n\n    def readlines(self, *args):\n        return self._file.readlines(*args)\n\n    def seek(self, *args):\n        self._file.seek(*args)\n\n    @property\n    def softspace(self):\n        return self._file.softspace\n\n    def tell(self):\n        return self._file.tell()\n\n    def truncate(self):\n        self._file.truncate()\n\n    def write(self, s):\n        file = self._file\n        rv = file.write(s)\n        self._check(file)\n        return rv\n\n    def writelines(self, iterable):\n        file = self._file\n        rv = file.writelines(iterable)\n        self._check(file)\n        return rv\n\n    def xreadlines(self, *args):\n        if hasattr(self._file, 'xreadlines'):  # real file\n            return iter(self._file)\n        else:  # StringIO()\n            return iter(self._file.readlines(*args))\n",
		"file_name": "tempfile.py"
	},
	{
		"content": "\"\"\"The io module provides the Python interfaces to stream handling. The\nbuiltin open function is defined in this module.\n\nAt the top of the I/O hierarchy is the abstract base class IOBase. It\ndefines the basic interface to a stream. Note, however, that there is no\nseparation between reading and writing to streams; implementations are\nallowed to raise an IOError if they do not support a given operation.\n\nExtending IOBase is RawIOBase which deals simply with the reading and\nwriting of raw bytes to a stream. FileIO subclasses RawIOBase to provide\nan interface to OS files.\n\nBufferedIOBase deals with buffering on a raw byte stream (RawIOBase). Its\nsubclasses, BufferedWriter, BufferedReader, and BufferedRWPair buffer\nstreams that are readable, writable, and both respectively.\nBufferedRandom provides a buffered interface to random access\nstreams. BytesIO is a simple stream of in-memory bytes.\n\nAnother IOBase subclass, TextIOBase, deals with the encoding and decoding\nof streams into text. TextIOWrapper, which extends it, is a buffered text\ninterface to a buffered raw stream (`BufferedIOBase`). Finally, StringIO\nis a in-memory stream for text.\n\nArgument names are not part of the specification, and only the arguments\nof open() are intended to be used as keyword arguments.\n\ndata:\n\nDEFAULT_BUFFER_SIZE\n\n   An int containing the default buffer size used by the module's buffered\n   I/O classes. open() uses the file's blksize (as obtained by os.stat) if\n   possible.\n\"\"\"\n# New I/O library conforming to PEP 3116.\n\n__author__ = (\"Guido van Rossum <guido@python.org>, \"\n              \"Mike Verdone <mike.verdone@gmail.com>, \"\n              \"Mark Russell <mark.russell@zen.co.uk>, \"\n              \"Antoine Pitrou <solipsis@pitrou.net>, \"\n              \"Amaury Forgeot d'Arc <amauryfa@gmail.com>, \"\n              \"Benjamin Peterson <benjamin@python.org>\")\n\n__all__ = [\"BlockingIOError\", \"open\", \"IOBase\", \"RawIOBase\", \"FileIO\",\n           \"BytesIO\", \"StringIO\", \"BufferedIOBase\",\n           \"BufferedReader\", \"BufferedWriter\", \"BufferedRWPair\",\n           \"BufferedRandom\", \"TextIOBase\", \"TextIOWrapper\",\n           \"UnsupportedOperation\", \"SEEK_SET\", \"SEEK_CUR\", \"SEEK_END\"]\n\n\nimport _io\nimport abc\n\nfrom _io import (DEFAULT_BUFFER_SIZE, BlockingIOError, UnsupportedOperation,\n                 open, FileIO, BytesIO, StringIO, BufferedReader,\n                 BufferedWriter, BufferedRWPair, BufferedRandom,\n                 IncrementalNewlineDecoder, TextIOWrapper)\n\nOpenWrapper = _io.open # for compatibility with _pyio\n\n# for seek()\nSEEK_SET = 0\nSEEK_CUR = 1\nSEEK_END = 2\n\n# Declaring ABCs in C is tricky so we do it here.\n# Method descriptions and default implementations are inherited from the C\n# version however.\nclass IOBase(_io._IOBase):\n    __metaclass__ = abc.ABCMeta\n    __doc__ = _io._IOBase.__doc__\n\nclass RawIOBase(_io._RawIOBase, IOBase):\n    __doc__ = _io._RawIOBase.__doc__\n\nclass BufferedIOBase(_io._BufferedIOBase, IOBase):\n    __doc__ = _io._BufferedIOBase.__doc__\n\nclass TextIOBase(_io._TextIOBase, IOBase):\n    __doc__ = _io._TextIOBase.__doc__\n\nRawIOBase.register(FileIO)\n\nfor klass in (BytesIO, BufferedReader, BufferedWriter, BufferedRandom,\n              BufferedRWPair):\n    BufferedIOBase.register(klass)\n\nfor klass in (StringIO, TextIOWrapper):\n    TextIOBase.register(klass)\ndel klass\n",
		"file_name": "io.py"
	},
	{
		"content": "\"\"\"Random variable generators.\n\n    integers\n    --------\n           uniform within range\n\n    sequences\n    ---------\n           pick random element\n           pick random sample\n           generate random permutation\n\n    distributions on the real line:\n    ------------------------------\n           uniform\n           triangular\n           normal (Gaussian)\n           lognormal\n           negative exponential\n           gamma\n           beta\n           pareto\n           Weibull\n\n    distributions on the circle (angles 0 to 2pi)\n    ---------------------------------------------\n           circular uniform\n           von Mises\n\nGeneral notes on the underlying Mersenne Twister core generator:\n\n* The period is 2**19937-1.\n* It is one of the most extensively tested generators in existence.\n* Without a direct way to compute N steps forward, the semantics of\n  jumpahead(n) are weakened to simply jump to another distant state and rely\n  on the large period to avoid overlapping sequences.\n* The random() method is implemented in C, executes in a single Python step,\n  and is, therefore, threadsafe.\n\n\"\"\"\n\nfrom __future__ import division\nfrom warnings import warn as _warn\nfrom math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil\nfrom math import sqrt as _sqrt, acos as _acos, cos as _cos, sin as _sin\nfrom os import urandom as _urandom\nfrom binascii import hexlify as _hexlify\nimport hashlib as _hashlib\n\n__all__ = [\"Random\",\"seed\",\"random\",\"uniform\",\"randint\",\"choice\",\"sample\",\n           \"randrange\",\"shuffle\",\"normalvariate\",\"lognormvariate\",\n           \"expovariate\",\"vonmisesvariate\",\"gammavariate\",\"triangular\",\n           \"gauss\",\"betavariate\",\"paretovariate\",\"weibullvariate\",\n           \"getstate\",\"setstate\",\"jumpahead\", \"WichmannHill\", \"getrandbits\",\n           \"SystemRandom\"]\n\nNV_MAGICCONST = 4 * _exp(-0.5)/_sqrt(2.0)\nTWOPI = 2.0*_pi\nLOG4 = _log(4.0)\nSG_MAGICCONST = 1.0 + _log(4.5)\nBPF = 53        # Number of bits in a float\nRECIP_BPF = 2**-BPF\n\n\n# Translated by Guido van Rossum from C source provided by\n# Adrian Baddeley.  Adapted by Raymond Hettinger for use with\n# the Mersenne Twister  and os.urandom() core generators.\n\nimport _random\n\nclass Random(_random.Random):\n    \"\"\"Random number generator base class used by bound module functions.\n\n    Used to instantiate instances of Random to get generators that don't\n    share state.  Especially useful for multi-threaded programs, creating\n    a different instance of Random for each thread, and using the jumpahead()\n    method to ensure that the generated sequences seen by each thread don't\n    overlap.\n\n    Class Random can also be subclassed if you want to use a different basic\n    generator of your own devising: in that case, override the following\n    methods: random(), seed(), getstate(), setstate() and jumpahead().\n    Optionally, implement a getrandbits() method so that randrange() can cover\n    arbitrarily large ranges.\n\n    \"\"\"\n\n    VERSION = 3     # used by getstate/setstate\n\n    def __init__(self, x=None):\n        \"\"\"Initialize an instance.\n\n        Optional argument x controls seeding, as for Random.seed().\n        \"\"\"\n\n        self.seed(x)\n        self.gauss_next = None\n\n    def seed(self, a=None):\n        \"\"\"Initialize internal state from hashable object.\n\n        None or no argument seeds from current time or from an operating\n        system specific randomness source if available.\n\n        If a is not None or an int or long, hash(a) is used instead.\n        \"\"\"\n\n        if a is None:\n            try:\n                # Seed with enough bytes to span the 19937 bit\n                # state space for the Mersenne Twister\n                a = long(_hexlify(_urandom(2500)), 16)\n            except NotImplementedError:\n                import time\n                a = long(time.time() * 256) # use fractional seconds\n\n        super(Random, self).seed(a)\n        self.gauss_next = None\n\n    def getstate(self):\n        \"\"\"Return internal state; can be passed to setstate() later.\"\"\"\n        return self.VERSION, super(Random, self).getstate(), self.gauss_next\n\n    def setstate(self, state):\n        \"\"\"Restore internal state from object returned by getstate().\"\"\"\n        version = state[0]\n        if version == 3:\n            version, internalstate, self.gauss_next = state\n            super(Random, self).setstate(internalstate)\n        elif version == 2:\n            version, internalstate, self.gauss_next = state\n            # In version 2, the state was saved as signed ints, which causes\n            #   inconsistencies between 32/64-bit systems. The state is\n            #   really unsigned 32-bit ints, so we convert negative ints from\n            #   version 2 to positive longs for version 3.\n            try:\n                internalstate = tuple( long(x) % (2**32) for x in internalstate )\n            except ValueError, e:\n                raise TypeError, e\n            super(Random, self).setstate(internalstate)\n        else:\n            raise ValueError(\"state with version %s passed to \"\n                             \"Random.setstate() of version %s\" %\n                             (version, self.VERSION))\n\n    def jumpahead(self, n):\n        \"\"\"Change the internal state to one that is likely far away\n        from the current state.  This method will not be in Py3.x,\n        so it is better to simply reseed.\n        \"\"\"\n        # The super.jumpahead() method uses shuffling to change state,\n        # so it needs a large and \"interesting\" n to work with.  Here,\n        # we use hashing to create a large n for the shuffle.\n        s = repr(n) + repr(self.getstate())\n        n = int(_hashlib.new('sha512', s).hexdigest(), 16)\n        super(Random, self).jumpahead(n)\n\n## ---- Methods below this point do not need to be overridden when\n## ---- subclassing for the purpose of using a different core generator.\n\n## -------------------- pickle support  -------------------\n\n    def __getstate__(self): # for pickle\n        return self.getstate()\n\n    def __setstate__(self, state):  # for pickle\n        self.setstate(state)\n\n    def __reduce__(self):\n        return self.__class__, (), self.getstate()\n\n## -------------------- integer methods  -------------------\n\n    def randrange(self, start, stop=None, step=1, _int=int, _maxwidth=1L<<BPF):\n        \"\"\"Choose a random item from range(start, stop[, step]).\n\n        This fixes the problem with randint() which includes the\n        endpoint; in Python this is usually not what you want.\n\n        \"\"\"\n\n        # This code is a bit messy to make it fast for the\n        # common case while still doing adequate error checking.\n        istart = _int(start)\n        if istart != start:\n            raise ValueError, \"non-integer arg 1 for randrange()\"\n        if stop is None:\n            if istart > 0:\n                if istart >= _maxwidth:\n                    return self._randbelow(istart)\n                return _int(self.random() * istart)\n            raise ValueError, \"empty range for randrange()\"\n\n        # stop argument supplied.\n        istop = _int(stop)\n        if istop != stop:\n            raise ValueError, \"non-integer stop for randrange()\"\n        width = istop - istart\n        if step == 1 and width > 0:\n            # Note that\n            #     int(istart + self.random()*width)\n            # instead would be incorrect.  For example, consider istart\n            # = -2 and istop = 0.  Then the guts would be in\n            # -2.0 to 0.0 exclusive on both ends (ignoring that random()\n            # might return 0.0), and because int() truncates toward 0, the\n            # final result would be -1 or 0 (instead of -2 or -1).\n            #     istart + int(self.random()*width)\n            # would also be incorrect, for a subtler reason:  the RHS\n            # can return a long, and then randrange() would also return\n            # a long, but we're supposed to return an int (for backward\n            # compatibility).\n\n            if width >= _maxwidth:\n                return _int(istart + self._randbelow(width))\n            return _int(istart + _int(self.random()*width))\n        if step == 1:\n            raise ValueError, \"empty range for randrange() (%d,%d, %d)\" % (istart, istop, width)\n\n        # Non-unit step argument supplied.\n        istep = _int(step)\n        if istep != step:\n            raise ValueError, \"non-integer step for randrange()\"\n        if istep > 0:\n            n = (width + istep - 1) // istep\n        elif istep < 0:\n            n = (width + istep + 1) // istep\n        else:\n            raise ValueError, \"zero step for randrange()\"\n\n        if n <= 0:\n            raise ValueError, \"empty range for randrange()\"\n\n        if n >= _maxwidth:\n            return istart + istep*self._randbelow(n)\n        return istart + istep*_int(self.random() * n)\n\n    def randint(self, a, b):\n        \"\"\"Return random integer in range [a, b], including both end points.\n        \"\"\"\n\n        return self.randrange(a, b+1)\n\n    def _randbelow(self, n, _log=_log, _int=int, _maxwidth=1L<<BPF):\n        \"\"\"Return a random int in the range [0,n)\n\n        Handles the case where n has more bits than returned\n        by a single call to the underlying generator.\n        \"\"\"\n\n        try:\n            getrandbits = self.getrandbits\n        except AttributeError:\n            pass\n        else:\n            # Only call self.getrandbits if the original random() builtin method\n            # has not been overridden or if a new getrandbits() was supplied.\n            # This assures that the two methods correspond.\n            if (self.random == super(Random, self).random or\n                    getrandbits != super(Random, self).getrandbits):\n                k = _int(1.00001 + _log(n-1, 2.0))   # 2**k > n-1 > 2**(k-2)\n                r = getrandbits(k)\n                while r >= n:\n                    r = getrandbits(k)\n                return r\n        if n >= _maxwidth:\n            _warn(\"Underlying random() generator does not supply \\n\"\n                \"enough bits to choose from a population range this large\")\n        return _int(self.random() * n)\n\n## -------------------- sequence methods  -------------------\n\n    def choice(self, seq):\n        \"\"\"Choose a random element from a non-empty sequence.\"\"\"\n        return seq[int(self.random() * len(seq))]  # raises IndexError if seq is empty\n\n    def shuffle(self, x, random=None):\n        \"\"\"x, random=random.random -> shuffle list x in place; return None.\n\n        Optional arg random is a 0-argument function returning a random\n        float in [0.0, 1.0); by default, the standard random.random.\n\n        \"\"\"\n\n        if random is None:\n            random = self.random\n        _int = int\n        for i in reversed(xrange(1, len(x))):\n            # pick an element in x[:i+1] with which to exchange x[i]\n            j = _int(random() * (i+1))\n            x[i], x[j] = x[j], x[i]\n\n    def sample(self, population, k):\n        \"\"\"Chooses k unique random elements from a population sequence.\n\n        Returns a new list containing elements from the population while\n        leaving the original population unchanged.  The resulting list is\n        in selection order so that all sub-slices will also be valid random\n        samples.  This allows raffle winners (the sample) to be partitioned\n        into grand prize and second place winners (the subslices).\n\n        Members of the population need not be hashable or unique.  If the\n        population contains repeats, then each occurrence is a possible\n        selection in the sample.\n\n        To choose a sample in a range of integers, use xrange as an argument.\n        This is especially fast and space efficient for sampling from a\n        large population:   sample(xrange(10000000), 60)\n        \"\"\"\n\n        # Sampling without replacement entails tracking either potential\n        # selections (the pool) in a list or previous selections in a set.\n\n        # When the number of selections is small compared to the\n        # population, then tracking selections is efficient, requiring\n        # only a small set and an occasional reselection.  For\n        # a larger number of selections, the pool tracking method is\n        # preferred since the list takes less space than the\n        # set and it doesn't suffer from frequent reselections.\n\n        n = len(population)\n        if not 0 <= k <= n:\n            raise ValueError(\"sample larger than population\")\n        random = self.random\n        _int = int\n        result = [None] * k\n        setsize = 21        # size of a small set minus size of an empty list\n        if k > 5:\n            setsize += 4 ** _ceil(_log(k * 3, 4)) # table size for big sets\n        if n <= setsize or hasattr(population, \"keys\"):\n            # An n-length list is smaller than a k-length set, or this is a\n            # mapping type so the other algorithm wouldn't work.\n            pool = list(population)\n            for i in xrange(k):         # invariant:  non-selected at [0,n-i)\n                j = _int(random() * (n-i))\n                result[i] = pool[j]\n                pool[j] = pool[n-i-1]   # move non-selected item into vacancy\n        else:\n            try:\n                selected = set()\n                selected_add = selected.add\n                for i in xrange(k):\n                    j = _int(random() * n)\n                    while j in selected:\n                        j = _int(random() * n)\n                    selected_add(j)\n                    result[i] = population[j]\n            except (TypeError, KeyError):   # handle (at least) sets\n                if isinstance(population, list):\n                    raise\n                return self.sample(tuple(population), k)\n        return result\n\n## -------------------- real-valued distributions  -------------------\n\n## -------------------- uniform distribution -------------------\n\n    def uniform(self, a, b):\n        \"Get a random number in the range [a, b) or [a, b] depending on rounding.\"\n        return a + (b-a) * self.random()\n\n## -------------------- triangular --------------------\n\n    def triangular(self, low=0.0, high=1.0, mode=None):\n        \"\"\"Triangular distribution.\n\n        Continuous distribution bounded by given lower and upper limits,\n        and having a given mode value in-between.\n\n        http://en.wikipedia.org/wiki/Triangular_distribution\n\n        \"\"\"\n        u = self.random()\n        try:\n            c = 0.5 if mode is None else (mode - low) / (high - low)\n        except ZeroDivisionError:\n            return low\n        if u > c:\n            u = 1.0 - u\n            c = 1.0 - c\n            low, high = high, low\n        return low + (high - low) * (u * c) ** 0.5\n\n## -------------------- normal distribution --------------------\n\n    def normalvariate(self, mu, sigma):\n        \"\"\"Normal distribution.\n\n        mu is the mean, and sigma is the standard deviation.\n\n        \"\"\"\n        # mu = mean, sigma = standard deviation\n\n        # Uses Kinderman and Monahan method. Reference: Kinderman,\n        # A.J. and Monahan, J.F., \"Computer generation of random\n        # variables using the ratio of uniform deviates\", ACM Trans\n        # Math Software, 3, (1977), pp257-260.\n\n        random = self.random\n        while 1:\n            u1 = random()\n            u2 = 1.0 - random()\n            z = NV_MAGICCONST*(u1-0.5)/u2\n            zz = z*z/4.0\n            if zz <= -_log(u2):\n                break\n        return mu + z*sigma\n\n## -------------------- lognormal distribution --------------------\n\n    def lognormvariate(self, mu, sigma):\n        \"\"\"Log normal distribution.\n\n        If you take the natural logarithm of this distribution, you'll get a\n        normal distribution with mean mu and standard deviation sigma.\n        mu can have any value, and sigma must be greater than zero.\n\n        \"\"\"\n        return _exp(self.normalvariate(mu, sigma))\n\n## -------------------- exponential distribution --------------------\n\n    def expovariate(self, lambd):\n        \"\"\"Exponential distribution.\n\n        lambd is 1.0 divided by the desired mean.  It should be\n        nonzero.  (The parameter would be called \"lambda\", but that is\n        a reserved word in Python.)  Returned values range from 0 to\n        positive infinity if lambd is positive, and from negative\n        infinity to 0 if lambd is negative.\n\n        \"\"\"\n        # lambd: rate lambd = 1/mean\n        # ('lambda' is a Python reserved word)\n\n        # we use 1-random() instead of random() to preclude the\n        # possibility of taking the log of zero.\n        return -_log(1.0 - self.random())/lambd\n\n## -------------------- von Mises distribution --------------------\n\n    def vonmisesvariate(self, mu, kappa):\n        \"\"\"Circular data distribution.\n\n        mu is the mean angle, expressed in radians between 0 and 2*pi, and\n        kappa is the concentration parameter, which must be greater than or\n        equal to zero.  If kappa is equal to zero, this distribution reduces\n        to a uniform random angle over the range 0 to 2*pi.\n\n        \"\"\"\n        # mu:    mean angle (in radians between 0 and 2*pi)\n        # kappa: concentration parameter kappa (>= 0)\n        # if kappa = 0 generate uniform random angle\n\n        # Based upon an algorithm published in: Fisher, N.I.,\n        # \"Statistical Analysis of Circular Data\", Cambridge\n        # University Press, 1993.\n\n        # Thanks to Magnus Kessler for a correction to the\n        # implementation of step 4.\n\n        random = self.random\n        if kappa <= 1e-6:\n            return TWOPI * random()\n\n        s = 0.5 / kappa\n        r = s + _sqrt(1.0 + s * s)\n\n        while 1:\n            u1 = random()\n            z = _cos(_pi * u1)\n\n            d = z / (r + z)\n            u2 = random()\n            if u2 < 1.0 - d * d or u2 <= (1.0 - d) * _exp(d):\n                break\n\n        q = 1.0 / r\n        f = (q + z) / (1.0 + q * z)\n        u3 = random()\n        if u3 > 0.5:\n            theta = (mu + _acos(f)) % TWOPI\n        else:\n            theta = (mu - _acos(f)) % TWOPI\n\n        return theta\n\n## -------------------- gamma distribution --------------------\n\n    def gammavariate(self, alpha, beta):\n        \"\"\"Gamma distribution.  Not the gamma function!\n\n        Conditions on the parameters are alpha > 0 and beta > 0.\n\n        The probability distribution function is:\n\n                    x ** (alpha - 1) * math.exp(-x / beta)\n          pdf(x) =  --------------------------------------\n                      math.gamma(alpha) * beta ** alpha\n\n        \"\"\"\n\n        # alpha > 0, beta > 0, mean is alpha*beta, variance is alpha*beta**2\n\n        # Warning: a few older sources define the gamma distribution in terms\n        # of alpha > -1.0\n        if alpha <= 0.0 or beta <= 0.0:\n            raise ValueError, 'gammavariate: alpha and beta must be > 0.0'\n\n        random = self.random\n        if alpha > 1.0:\n\n            # Uses R.C.H. Cheng, \"The generation of Gamma\n            # variables with non-integral shape parameters\",\n            # Applied Statistics, (1977), 26, No. 1, p71-74\n\n            ainv = _sqrt(2.0 * alpha - 1.0)\n            bbb = alpha - LOG4\n            ccc = alpha + ainv\n\n            while 1:\n                u1 = random()\n                if not 1e-7 < u1 < .9999999:\n                    continue\n                u2 = 1.0 - random()\n                v = _log(u1/(1.0-u1))/ainv\n                x = alpha*_exp(v)\n                z = u1*u1*u2\n                r = bbb+ccc*v-x\n                if r + SG_MAGICCONST - 4.5*z >= 0.0 or r >= _log(z):\n                    return x * beta\n\n        elif alpha == 1.0:\n            # expovariate(1)\n            u = random()\n            while u <= 1e-7:\n                u = random()\n            return -_log(u) * beta\n\n        else:   # alpha is between 0 and 1 (exclusive)\n\n            # Uses ALGORITHM GS of Statistical Computing - Kennedy & Gentle\n\n            while 1:\n                u = random()\n                b = (_e + alpha)/_e\n                p = b*u\n                if p <= 1.0:\n                    x = p ** (1.0/alpha)\n                else:\n                    x = -_log((b-p)/alpha)\n                u1 = random()\n                if p > 1.0:\n                    if u1 <= x ** (alpha - 1.0):\n                        break\n                elif u1 <= _exp(-x):\n                    break\n            return x * beta\n\n## -------------------- Gauss (faster alternative) --------------------\n\n    def gauss(self, mu, sigma):\n        \"\"\"Gaussian distribution.\n\n        mu is the mean, and sigma is the standard deviation.  This is\n        slightly faster than the normalvariate() function.\n\n        Not thread-safe without a lock around calls.\n\n        \"\"\"\n\n        # When x and y are two variables from [0, 1), uniformly\n        # distributed, then\n        #\n        #    cos(2*pi*x)*sqrt(-2*log(1-y))\n        #    sin(2*pi*x)*sqrt(-2*log(1-y))\n        #\n        # are two *independent* variables with normal distribution\n        # (mu = 0, sigma = 1).\n        # (Lambert Meertens)\n        # (corrected version; bug discovered by Mike Miller, fixed by LM)\n\n        # Multithreading note: When two threads call this function\n        # simultaneously, it is possible that they will receive the\n        # same return value.  The window is very small though.  To\n        # avoid this, you have to use a lock around all calls.  (I\n        # didn't want to slow this down in the serial case by using a\n        # lock here.)\n\n        random = self.random\n        z = self.gauss_next\n        self.gauss_next = None\n        if z is None:\n            x2pi = random() * TWOPI\n            g2rad = _sqrt(-2.0 * _log(1.0 - random()))\n            z = _cos(x2pi) * g2rad\n            self.gauss_next = _sin(x2pi) * g2rad\n\n        return mu + z*sigma\n\n## -------------------- beta --------------------\n## See\n## http://mail.python.org/pipermail/python-bugs-list/2001-January/003752.html\n## for Ivan Frohne's insightful analysis of why the original implementation:\n##\n##    def betavariate(self, alpha, beta):\n##        # Discrete Event Simulation in C, pp 87-88.\n##\n##        y = self.expovariate(alpha)\n##        z = self.expovariate(1.0/beta)\n##        return z/(y+z)\n##\n## was dead wrong, and how it probably got that way.\n\n    def betavariate(self, alpha, beta):\n        \"\"\"Beta distribution.\n\n        Conditions on the parameters are alpha > 0 and beta > 0.\n        Returned values range between 0 and 1.\n\n        \"\"\"\n\n        # This version due to Janne Sinkkonen, and matches all the std\n        # texts (e.g., Knuth Vol 2 Ed 3 pg 134 \"the beta distribution\").\n        y = self.gammavariate(alpha, 1.)\n        if y == 0:\n            return 0.0\n        else:\n            return y / (y + self.gammavariate(beta, 1.))\n\n## -------------------- Pareto --------------------\n\n    def paretovariate(self, alpha):\n        \"\"\"Pareto distribution.  alpha is the shape parameter.\"\"\"\n        # Jain, pg. 495\n\n        u = 1.0 - self.random()\n        return 1.0 / pow(u, 1.0/alpha)\n\n## -------------------- Weibull --------------------\n\n    def weibullvariate(self, alpha, beta):\n        \"\"\"Weibull distribution.\n\n        alpha is the scale parameter and beta is the shape parameter.\n\n        \"\"\"\n        # Jain, pg. 499; bug fix courtesy Bill Arms\n\n        u = 1.0 - self.random()\n        return alpha * pow(-_log(u), 1.0/beta)\n\n## -------------------- Wichmann-Hill -------------------\n\nclass WichmannHill(Random):\n\n    VERSION = 1     # used by getstate/setstate\n\n    def seed(self, a=None):\n        \"\"\"Initialize internal state from hashable object.\n\n        None or no argument seeds from current time or from an operating\n        system specific randomness source if available.\n\n        If a is not None or an int or long, hash(a) is used instead.\n\n        If a is an int or long, a is used directly.  Distinct values between\n        0 and 27814431486575L inclusive are guaranteed to yield distinct\n        internal states (this guarantee is specific to the default\n        Wichmann-Hill generator).\n        \"\"\"\n\n        if a is None:\n            try:\n                a = long(_hexlify(_urandom(16)), 16)\n            except NotImplementedError:\n                import time\n                a = long(time.time() * 256) # use fractional seconds\n\n        if not isinstance(a, (int, long)):\n            a = hash(a)\n\n        a, x = divmod(a, 30268)\n        a, y = divmod(a, 30306)\n        a, z = divmod(a, 30322)\n        self._seed = int(x)+1, int(y)+1, int(z)+1\n\n        self.gauss_next = None\n\n    def random(self):\n        \"\"\"Get the next random number in the range [0.0, 1.0).\"\"\"\n\n        # Wichman-Hill random number generator.\n        #\n        # Wichmann, B. A. & Hill, I. D. (1982)\n        # Algorithm AS 183:\n        # An efficient and portable pseudo-random number generator\n        # Applied Statistics 31 (1982) 188-190\n        #\n        # see also:\n        #        Correction to Algorithm AS 183\n        #        Applied Statistics 33 (1984) 123\n        #\n        #        McLeod, A. I. (1985)\n        #        A remark on Algorithm AS 183\n        #        Applied Statistics 34 (1985),198-200\n\n        # This part is thread-unsafe:\n        # BEGIN CRITICAL SECTION\n        x, y, z = self._seed\n        x = (171 * x) % 30269\n        y = (172 * y) % 30307\n        z = (170 * z) % 30323\n        self._seed = x, y, z\n        # END CRITICAL SECTION\n\n        # Note:  on a platform using IEEE-754 double arithmetic, this can\n        # never return 0.0 (asserted by Tim; proof too long for a comment).\n        return (x/30269.0 + y/30307.0 + z/30323.0) % 1.0\n\n    def getstate(self):\n        \"\"\"Return internal state; can be passed to setstate() later.\"\"\"\n        return self.VERSION, self._seed, self.gauss_next\n\n    def setstate(self, state):\n        \"\"\"Restore internal state from object returned by getstate().\"\"\"\n        version = state[0]\n        if version == 1:\n            version, self._seed, self.gauss_next = state\n        else:\n            raise ValueError(\"state with version %s passed to \"\n                             \"Random.setstate() of version %s\" %\n                             (version, self.VERSION))\n\n    def jumpahead(self, n):\n        \"\"\"Act as if n calls to random() were made, but quickly.\n\n        n is an int, greater than or equal to 0.\n\n        Example use:  If you have 2 threads and know that each will\n        consume no more than a million random numbers, create two Random\n        objects r1 and r2, then do\n            r2.setstate(r1.getstate())\n            r2.jumpahead(1000000)\n        Then r1 and r2 will use guaranteed-disjoint segments of the full\n        period.\n        \"\"\"\n\n        if not n >= 0:\n            raise ValueError(\"n must be >= 0\")\n        x, y, z = self._seed\n        x = int(x * pow(171, n, 30269)) % 30269\n        y = int(y * pow(172, n, 30307)) % 30307\n        z = int(z * pow(170, n, 30323)) % 30323\n        self._seed = x, y, z\n\n    def __whseed(self, x=0, y=0, z=0):\n        \"\"\"Set the Wichmann-Hill seed from (x, y, z).\n\n        These must be integers in the range [0, 256).\n        \"\"\"\n\n        if not type(x) == type(y) == type(z) == int:\n            raise TypeError('seeds must be integers')\n        if not (0 <= x < 256 and 0 <= y < 256 and 0 <= z < 256):\n            raise ValueError('seeds must be in range(0, 256)')\n        if 0 == x == y == z:\n            # Initialize from current time\n            import time\n            t = long(time.time() * 256)\n            t = int((t&0xffffff) ^ (t>>24))\n            t, x = divmod(t, 256)\n            t, y = divmod(t, 256)\n            t, z = divmod(t, 256)\n        # Zero is a poor seed, so substitute 1\n        self._seed = (x or 1, y or 1, z or 1)\n\n        self.gauss_next = None\n\n    def whseed(self, a=None):\n        \"\"\"Seed from hashable object's hash code.\n\n        None or no argument seeds from current time.  It is not guaranteed\n        that objects with distinct hash codes lead to distinct internal\n        states.\n\n        This is obsolete, provided for compatibility with the seed routine\n        used prior to Python 2.1.  Use the .seed() method instead.\n        \"\"\"\n\n        if a is None:\n            self.__whseed()\n            return\n        a = hash(a)\n        a, x = divmod(a, 256)\n        a, y = divmod(a, 256)\n        a, z = divmod(a, 256)\n        x = (x + a) % 256 or 1\n        y = (y + a) % 256 or 1\n        z = (z + a) % 256 or 1\n        self.__whseed(x, y, z)\n\n## --------------- Operating System Random Source  ------------------\n\nclass SystemRandom(Random):\n    \"\"\"Alternate random number generator using sources provided\n    by the operating system (such as /dev/urandom on Unix or\n    CryptGenRandom on Windows).\n\n     Not available on all systems (see os.urandom() for details).\n    \"\"\"\n\n    def random(self):\n        \"\"\"Get the next random number in the range [0.0, 1.0).\"\"\"\n        return (long(_hexlify(_urandom(7)), 16) >> 3) * RECIP_BPF\n\n    def getrandbits(self, k):\n        \"\"\"getrandbits(k) -> x.  Generates a long int with k random bits.\"\"\"\n        if k <= 0:\n            raise ValueError('number of bits must be greater than zero')\n        if k != int(k):\n            raise TypeError('number of bits should be an integer')\n        bytes = (k + 7) // 8                    # bits / 8 and rounded up\n        x = long(_hexlify(_urandom(bytes)), 16)\n        return x >> (bytes * 8 - k)             # trim excess bits\n\n    def _stub(self, *args, **kwds):\n        \"Stub method.  Not used for a system random number generator.\"\n        return None\n    seed = jumpahead = _stub\n\n    def _notimplemented(self, *args, **kwds):\n        \"Method should not be called for a system random number generator.\"\n        raise NotImplementedError('System entropy source does not have state.')\n    getstate = setstate = _notimplemented\n\n## -------------------- test program --------------------\n\ndef _test_generator(n, func, args):\n    import time\n    print n, 'times', func.__name__\n    total = 0.0\n    sqsum = 0.0\n    smallest = 1e10\n    largest = -1e10\n    t0 = time.time()\n    for i in range(n):\n        x = func(*args)\n        total += x\n        sqsum = sqsum + x*x\n        smallest = min(x, smallest)\n        largest = max(x, largest)\n    t1 = time.time()\n    print round(t1-t0, 3), 'sec,',\n    avg = total/n\n    stddev = _sqrt(sqsum/n - avg*avg)\n    print 'avg %g, stddev %g, min %g, max %g' % \\\n              (avg, stddev, smallest, largest)\n\n\ndef _test(N=2000):\n    _test_generator(N, random, ())\n    _test_generator(N, normalvariate, (0.0, 1.0))\n    _test_generator(N, lognormvariate, (0.0, 1.0))\n    _test_generator(N, vonmisesvariate, (0.0, 1.0))\n    _test_generator(N, gammavariate, (0.01, 1.0))\n    _test_generator(N, gammavariate, (0.1, 1.0))\n    _test_generator(N, gammavariate, (0.1, 2.0))\n    _test_generator(N, gammavariate, (0.5, 1.0))\n    _test_generator(N, gammavariate, (0.9, 1.0))\n    _test_generator(N, gammavariate, (1.0, 1.0))\n    _test_generator(N, gammavariate, (2.0, 1.0))\n    _test_generator(N, gammavariate, (20.0, 1.0))\n    _test_generator(N, gammavariate, (200.0, 1.0))\n    _test_generator(N, gauss, (0.0, 1.0))\n    _test_generator(N, betavariate, (3.0, 3.0))\n    _test_generator(N, triangular, (0.0, 1.0, 1.0/3.0))\n\n# Create one instance, seeded from current time, and export its methods\n# as module-level functions.  The functions share state across all uses\n#(both in the user's code and in the Python libraries), but that's fine\n# for most programs and is easier for the casual user than making them\n# instantiate their own Random() instance.\n\n_inst = Random()\nseed = _inst.seed\nrandom = _inst.random\nuniform = _inst.uniform\ntriangular = _inst.triangular\nrandint = _inst.randint\nchoice = _inst.choice\nrandrange = _inst.randrange\nsample = _inst.sample\nshuffle = _inst.shuffle\nnormalvariate = _inst.normalvariate\nlognormvariate = _inst.lognormvariate\nexpovariate = _inst.expovariate\nvonmisesvariate = _inst.vonmisesvariate\ngammavariate = _inst.gammavariate\ngauss = _inst.gauss\nbetavariate = _inst.betavariate\nparetovariate = _inst.paretovariate\nweibullvariate = _inst.weibullvariate\ngetstate = _inst.getstate\nsetstate = _inst.setstate\njumpahead = _inst.jumpahead\ngetrandbits = _inst.getrandbits\n\nif __name__ == '__main__':\n    _test()\n",
		"file_name": "random.py"
	},
	{
		"content": "# $Id$\n#\n#  Copyright (C) 2005   Gregory P. Smith (greg@krypto.org)\n#  Licensed to PSF under a Contributor Agreement.\n#\n\n__doc__ = \"\"\"hashlib module - A common interface to many hash functions.\n\nnew(name, string='') - returns a new hash object implementing the\n                       given hash function; initializing the hash\n                       using the given string data.\n\nNamed constructor functions are also available, these are much faster\nthan using new():\n\nmd5(), sha1(), sha224(), sha256(), sha384(), and sha512()\n\nMore algorithms may be available on your platform but the above are\nguaranteed to exist.\n\nNOTE: If you want the adler32 or crc32 hash functions they are available in\nthe zlib module.\n\nChoose your hash function wisely.  Some have known collision weaknesses.\nsha384 and sha512 will be slow on 32 bit platforms.\n\nHash objects have these methods:\n - update(arg): Update the hash object with the string arg. Repeated calls\n                are equivalent to a single call with the concatenation of all\n                the arguments.\n - digest():    Return the digest of the strings passed to the update() method\n                so far. This may contain non-ASCII characters, including\n                NUL bytes.\n - hexdigest(): Like digest() except the digest is returned as a string of\n                double length, containing only hexadecimal digits.\n - copy():      Return a copy (clone) of the hash object. This can be used to\n                efficiently compute the digests of strings that share a common\n                initial substring.\n\nFor example, to obtain the digest of the string 'Nobody inspects the\nspammish repetition':\n\n    >>> import hashlib\n    >>> m = hashlib.md5()\n    >>> m.update(\"Nobody inspects\")\n    >>> m.update(\" the spammish repetition\")\n    >>> m.digest()\n    '\\\\xbbd\\\\x9c\\\\x83\\\\xdd\\\\x1e\\\\xa5\\\\xc9\\\\xd9\\\\xde\\\\xc9\\\\xa1\\\\x8d\\\\xf0\\\\xff\\\\xe9'\n\nMore condensed:\n\n    >>> hashlib.sha224(\"Nobody inspects the spammish repetition\").hexdigest()\n    'a4337bc45a8fc544c03f52dc550cd6e1e87021bc896588bd79e901e2'\n\n\"\"\"\n\n# This tuple and __get_builtin_constructor() must be modified if a new\n# always available algorithm is added.\n__always_supported = ('md5', 'sha1', 'sha224', 'sha256', 'sha384', 'sha512')\n\nalgorithms = __always_supported\n\n__all__ = __always_supported + ('new', 'algorithms', 'pbkdf2_hmac')\n\n\ndef __get_builtin_constructor(name):\n    try:\n        if name in ('SHA1', 'sha1'):\n            import _sha\n            return _sha.new\n        elif name in ('MD5', 'md5'):\n            import _md5\n            return _md5.new\n        elif name in ('SHA256', 'sha256', 'SHA224', 'sha224'):\n            import _sha256\n            bs = name[3:]\n            if bs == '256':\n                return _sha256.sha256\n            elif bs == '224':\n                return _sha256.sha224\n        elif name in ('SHA512', 'sha512', 'SHA384', 'sha384'):\n            import _sha512\n            bs = name[3:]\n            if bs == '512':\n                return _sha512.sha512\n            elif bs == '384':\n                return _sha512.sha384\n    except ImportError:\n        pass  # no extension module, this hash is unsupported.\n\n    raise ValueError('unsupported hash type ' + name)\n\n\ndef __get_openssl_constructor(name):\n    try:\n        f = getattr(_hashlib, 'openssl_' + name)\n        # Allow the C module to raise ValueError.  The function will be\n        # defined but the hash not actually available thanks to OpenSSL.\n        f()\n        # Use the C function directly (very fast)\n        return f\n    except (AttributeError, ValueError):\n        return __get_builtin_constructor(name)\n\n\ndef __py_new(name, string=''):\n    \"\"\"new(name, string='') - Return a new hashing object using the named algorithm;\n    optionally initialized with a string.\n    \"\"\"\n    return __get_builtin_constructor(name)(string)\n\n\ndef __hash_new(name, string=''):\n    \"\"\"new(name, string='') - Return a new hashing object using the named algorithm;\n    optionally initialized with a string.\n    \"\"\"\n    try:\n        return _hashlib.new(name, string)\n    except ValueError:\n        # If the _hashlib module (OpenSSL) doesn't support the named\n        # hash, try using our builtin implementations.\n        # This allows for SHA224/256 and SHA384/512 support even though\n        # the OpenSSL library prior to 0.9.8 doesn't provide them.\n        return __get_builtin_constructor(name)(string)\n\n\ntry:\n    import _hashlib\n    new = __hash_new\n    __get_hash = __get_openssl_constructor\nexcept ImportError:\n    new = __py_new\n    __get_hash = __get_builtin_constructor\n\nfor __func_name in __always_supported:\n    # try them all, some may not work due to the OpenSSL\n    # version not supporting that algorithm.\n    try:\n        globals()[__func_name] = __get_hash(__func_name)\n    except ValueError:\n        import logging\n        logging.exception('code for hash %s was not found.', __func_name)\n\n\ntry:\n    # OpenSSL's PKCS5_PBKDF2_HMAC requires OpenSSL 1.0+ with HMAC and SHA\n    from _hashlib import pbkdf2_hmac\nexcept ImportError:\n    import binascii\n    import struct\n\n    _trans_5C = b\"\".join(chr(x ^ 0x5C) for x in range(256))\n    _trans_36 = b\"\".join(chr(x ^ 0x36) for x in range(256))\n\n    def pbkdf2_hmac(hash_name, password, salt, iterations, dklen=None):\n        \"\"\"Password based key derivation function 2 (PKCS #5 v2.0)\n\n        This Python implementations based on the hmac module about as fast\n        as OpenSSL's PKCS5_PBKDF2_HMAC for short passwords and much faster\n        for long passwords.\n        \"\"\"\n        if not isinstance(hash_name, str):\n            raise TypeError(hash_name)\n\n        if not isinstance(password, (bytes, bytearray)):\n            password = bytes(buffer(password))\n        if not isinstance(salt, (bytes, bytearray)):\n            salt = bytes(buffer(salt))\n\n        # Fast inline HMAC implementation\n        inner = new(hash_name)\n        outer = new(hash_name)\n        blocksize = getattr(inner, 'block_size', 64)\n        if len(password) > blocksize:\n            password = new(hash_name, password).digest()\n        password = password + b'\\x00' * (blocksize - len(password))\n        inner.update(password.translate(_trans_36))\n        outer.update(password.translate(_trans_5C))\n\n        def prf(msg, inner=inner, outer=outer):\n            # PBKDF2_HMAC uses the password as key. We can re-use the same\n            # digest objects and and just update copies to skip initialization.\n            icpy = inner.copy()\n            ocpy = outer.copy()\n            icpy.update(msg)\n            ocpy.update(icpy.digest())\n            return ocpy.digest()\n\n        if iterations < 1:\n            raise ValueError(iterations)\n        if dklen is None:\n            dklen = outer.digest_size\n        if dklen < 1:\n            raise ValueError(dklen)\n\n        hex_format_string = \"%%0%ix\" % (new(hash_name).digest_size * 2)\n\n        dkey = b''\n        loop = 1\n        while len(dkey) < dklen:\n            prev = prf(salt + struct.pack(b'>I', loop))\n            rkey = int(binascii.hexlify(prev), 16)\n            for i in xrange(iterations - 1):\n                prev = prf(prev)\n                rkey ^= int(binascii.hexlify(prev), 16)\n            loop += 1\n            dkey += binascii.unhexlify(hex_format_string % rkey)\n\n        return dkey[:dklen]\n\n# Cleanup locals()\ndel __always_supported, __func_name, __get_hash\ndel __py_new, __hash_new, __get_openssl_constructor\n",
		"file_name": "hashlib.py"
	},
	{
		"content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note that PyPy contains also a built-in module 'md5' which will hide\n# this one if compiled in.\n\n\"\"\"A sample implementation of MD5 in pure Python.\n\nThis is an implementation of the MD5 hash function, as specified by\nRFC 1321, in pure Python. It was implemented using Bruce Schneier's\nexcellent book \"Applied Cryptography\", 2nd ed., 1996.\n\nSurely this is not meant to compete with the existing implementation\nof the Python standard library (written in C). Rather, it should be\nseen as a Python complement that is more readable than C and can be\nused more conveniently for learning and experimenting purposes in\nthe field of cryptography.\n\nThis module tries very hard to follow the API of the existing Python\nstandard library's \"md5\" module, but although it seems to work fine,\nit has not been extensively tested! (But note that there is a test\nmodule, test_md5py.py, that compares this Python implementation with\nthe C one of the Python standard library.\n\nBEWARE: this comes with no guarantee whatsoever about fitness and/or\nother properties! Specifically, do not use this in any production\ncode! License is Python License!\n\nSpecial thanks to Aurelian Coman who fixed some nasty bugs!\n\nDinu C. Gherman\n\"\"\"\n\n\n__date__    = '2004-11-17'\n__version__ = 0.91 # Modernised by J. Hall\u00e9n and L. Creighton for Pypy\n\n__metaclass__ = type # or genrpy won't work\n\nimport struct, copy\n\n\n# ======================================================================\n# Bit-Manipulation helpers\n# ======================================================================\n\ndef _bytelist2long(list):\n    \"Transform a list of characters into a list of longs.\"\n\n    imax = len(list) // 4\n    hl = [0] * imax\n\n    j = 0\n    i = 0\n    while i < imax:\n        b0 = ord(list[j])\n        b1 = ord(list[j+1]) << 8\n        b2 = ord(list[j+2]) << 16\n        b3 = ord(list[j+3]) << 24\n        hl[i] = b0 | b1 |b2 | b3\n        i = i+1\n        j = j+4\n\n    return hl\n\n\ndef _rotateLeft(x, n):\n    \"Rotate x (32 bit) left n bits circularly.\"\n\n    return (x << n) | (x >> (32-n))\n\n\n# ======================================================================\n# The real MD5 meat...\n#\n#   Implemented after \"Applied Cryptography\", 2nd ed., 1996,\n#   pp. 436-441 by Bruce Schneier.\n# ======================================================================\n\n# F, G, H and I are basic MD5 functions.\n\ndef F(x, y, z):\n    return (x & y) | ((~x) & z)\n\ndef G(x, y, z):\n    return (x & z) | (y & (~z))\n\ndef H(x, y, z):\n    return x ^ y ^ z\n\ndef I(x, y, z):\n    return y ^ (x | (~z))\n\n\ndef XX(func, a, b, c, d, x, s, ac):\n    \"\"\"Wrapper for call distribution to functions F, G, H and I.\n\n    This replaces functions FF, GG, HH and II from \"Appl. Crypto.\"\n    Rotation is separate from addition to prevent recomputation\n    (now summed-up in one function).\n    \"\"\"\n\n    res = 0\n    res = res + a + func(b, c, d)\n    res = res + x \n    res = res + ac\n    res = res & 0xffffffff\n    res = _rotateLeft(res, s)\n    res = res & 0xffffffff\n    res = res + b\n\n    return res & 0xffffffff\n\n\nclass MD5Type:\n    \"An implementation of the MD5 hash function in pure Python.\"\n\n    digest_size = digestsize = 16\n    block_size = 64\n\n    def __init__(self):\n        \"Initialisation.\"\n        \n        # Initial message length in bits(!).\n        self.length = 0\n        self.count = [0, 0]\n\n        # Initial empty message as a sequence of bytes (8 bit characters).\n        self.input = []\n\n        # Call a separate init function, that can be used repeatedly\n        # to start from scratch on the same object.\n        self.init()\n\n\n    def init(self):\n        \"Initialize the message-digest and set all fields to zero.\"\n\n        self.length = 0\n        self.count = [0, 0]\n        self.input = []\n\n        # Load magic initialization constants.\n        self.A = 0x67452301\n        self.B = 0xefcdab89\n        self.C = 0x98badcfe\n        self.D = 0x10325476\n\n\n    def _transform(self, inp):\n        \"\"\"Basic MD5 step transforming the digest based on the input.\n\n        Note that if the Mysterious Constants are arranged backwards\n        in little-endian order and decrypted with the DES they produce\n        OCCULT MESSAGES!\n        \"\"\"\n\n        a, b, c, d = A, B, C, D = self.A, self.B, self.C, self.D\n\n        # Round 1.\n\n        S11, S12, S13, S14 = 7, 12, 17, 22\n\n        a = XX(F, a, b, c, d, inp[ 0], S11, 0xD76AA478) # 1 \n        d = XX(F, d, a, b, c, inp[ 1], S12, 0xE8C7B756) # 2 \n        c = XX(F, c, d, a, b, inp[ 2], S13, 0x242070DB) # 3 \n        b = XX(F, b, c, d, a, inp[ 3], S14, 0xC1BDCEEE) # 4 \n        a = XX(F, a, b, c, d, inp[ 4], S11, 0xF57C0FAF) # 5 \n        d = XX(F, d, a, b, c, inp[ 5], S12, 0x4787C62A) # 6 \n        c = XX(F, c, d, a, b, inp[ 6], S13, 0xA8304613) # 7 \n        b = XX(F, b, c, d, a, inp[ 7], S14, 0xFD469501) # 8 \n        a = XX(F, a, b, c, d, inp[ 8], S11, 0x698098D8) # 9 \n        d = XX(F, d, a, b, c, inp[ 9], S12, 0x8B44F7AF) # 10 \n        c = XX(F, c, d, a, b, inp[10], S13, 0xFFFF5BB1) # 11 \n        b = XX(F, b, c, d, a, inp[11], S14, 0x895CD7BE) # 12 \n        a = XX(F, a, b, c, d, inp[12], S11, 0x6B901122) # 13 \n        d = XX(F, d, a, b, c, inp[13], S12, 0xFD987193) # 14 \n        c = XX(F, c, d, a, b, inp[14], S13, 0xA679438E) # 15 \n        b = XX(F, b, c, d, a, inp[15], S14, 0x49B40821) # 16 \n\n        # Round 2.\n\n        S21, S22, S23, S24 = 5, 9, 14, 20\n\n        a = XX(G, a, b, c, d, inp[ 1], S21, 0xF61E2562) # 17 \n        d = XX(G, d, a, b, c, inp[ 6], S22, 0xC040B340) # 18 \n        c = XX(G, c, d, a, b, inp[11], S23, 0x265E5A51) # 19 \n        b = XX(G, b, c, d, a, inp[ 0], S24, 0xE9B6C7AA) # 20 \n        a = XX(G, a, b, c, d, inp[ 5], S21, 0xD62F105D) # 21 \n        d = XX(G, d, a, b, c, inp[10], S22, 0x02441453) # 22 \n        c = XX(G, c, d, a, b, inp[15], S23, 0xD8A1E681) # 23 \n        b = XX(G, b, c, d, a, inp[ 4], S24, 0xE7D3FBC8) # 24 \n        a = XX(G, a, b, c, d, inp[ 9], S21, 0x21E1CDE6) # 25 \n        d = XX(G, d, a, b, c, inp[14], S22, 0xC33707D6) # 26 \n        c = XX(G, c, d, a, b, inp[ 3], S23, 0xF4D50D87) # 27 \n        b = XX(G, b, c, d, a, inp[ 8], S24, 0x455A14ED) # 28 \n        a = XX(G, a, b, c, d, inp[13], S21, 0xA9E3E905) # 29 \n        d = XX(G, d, a, b, c, inp[ 2], S22, 0xFCEFA3F8) # 30 \n        c = XX(G, c, d, a, b, inp[ 7], S23, 0x676F02D9) # 31 \n        b = XX(G, b, c, d, a, inp[12], S24, 0x8D2A4C8A) # 32 \n\n        # Round 3.\n\n        S31, S32, S33, S34 = 4, 11, 16, 23\n\n        a = XX(H, a, b, c, d, inp[ 5], S31, 0xFFFA3942) # 33 \n        d = XX(H, d, a, b, c, inp[ 8], S32, 0x8771F681) # 34 \n        c = XX(H, c, d, a, b, inp[11], S33, 0x6D9D6122) # 35 \n        b = XX(H, b, c, d, a, inp[14], S34, 0xFDE5380C) # 36 \n        a = XX(H, a, b, c, d, inp[ 1], S31, 0xA4BEEA44) # 37 \n        d = XX(H, d, a, b, c, inp[ 4], S32, 0x4BDECFA9) # 38 \n        c = XX(H, c, d, a, b, inp[ 7], S33, 0xF6BB4B60) # 39 \n        b = XX(H, b, c, d, a, inp[10], S34, 0xBEBFBC70) # 40 \n        a = XX(H, a, b, c, d, inp[13], S31, 0x289B7EC6) # 41 \n        d = XX(H, d, a, b, c, inp[ 0], S32, 0xEAA127FA) # 42 \n        c = XX(H, c, d, a, b, inp[ 3], S33, 0xD4EF3085) # 43 \n        b = XX(H, b, c, d, a, inp[ 6], S34, 0x04881D05) # 44 \n        a = XX(H, a, b, c, d, inp[ 9], S31, 0xD9D4D039) # 45 \n        d = XX(H, d, a, b, c, inp[12], S32, 0xE6DB99E5) # 46 \n        c = XX(H, c, d, a, b, inp[15], S33, 0x1FA27CF8) # 47 \n        b = XX(H, b, c, d, a, inp[ 2], S34, 0xC4AC5665) # 48 \n\n        # Round 4.\n\n        S41, S42, S43, S44 = 6, 10, 15, 21\n\n        a = XX(I, a, b, c, d, inp[ 0], S41, 0xF4292244) # 49 \n        d = XX(I, d, a, b, c, inp[ 7], S42, 0x432AFF97) # 50 \n        c = XX(I, c, d, a, b, inp[14], S43, 0xAB9423A7) # 51 \n        b = XX(I, b, c, d, a, inp[ 5], S44, 0xFC93A039) # 52 \n        a = XX(I, a, b, c, d, inp[12], S41, 0x655B59C3) # 53 \n        d = XX(I, d, a, b, c, inp[ 3], S42, 0x8F0CCC92) # 54 \n        c = XX(I, c, d, a, b, inp[10], S43, 0xFFEFF47D) # 55 \n        b = XX(I, b, c, d, a, inp[ 1], S44, 0x85845DD1) # 56 \n        a = XX(I, a, b, c, d, inp[ 8], S41, 0x6FA87E4F) # 57 \n        d = XX(I, d, a, b, c, inp[15], S42, 0xFE2CE6E0) # 58 \n        c = XX(I, c, d, a, b, inp[ 6], S43, 0xA3014314) # 59 \n        b = XX(I, b, c, d, a, inp[13], S44, 0x4E0811A1) # 60 \n        a = XX(I, a, b, c, d, inp[ 4], S41, 0xF7537E82) # 61 \n        d = XX(I, d, a, b, c, inp[11], S42, 0xBD3AF235) # 62 \n        c = XX(I, c, d, a, b, inp[ 2], S43, 0x2AD7D2BB) # 63 \n        b = XX(I, b, c, d, a, inp[ 9], S44, 0xEB86D391) # 64 \n\n        A = (A + a) & 0xffffffff\n        B = (B + b) & 0xffffffff\n        C = (C + c) & 0xffffffff\n        D = (D + d) & 0xffffffff\n\n        self.A, self.B, self.C, self.D = A, B, C, D\n\n\n    # Down from here all methods follow the Python Standard Library\n    # API of the md5 module.\n\n    def update(self, inBuf):\n        \"\"\"Add to the current message.\n\n        Update the md5 object with the string arg. Repeated calls\n        are equivalent to a single call with the concatenation of all\n        the arguments, i.e. m.update(a); m.update(b) is equivalent\n        to m.update(a+b).\n\n        The hash is immediately calculated for all full blocks. The final\n        calculation is made in digest(). This allows us to keep an\n        intermediate value for the hash, so that we only need to make\n        minimal recalculation if we call update() to add moredata to\n        the hashed string.\n        \"\"\"\n\n        leninBuf = len(inBuf)\n\n        # Compute number of bytes mod 64.\n        index = (self.count[0] >> 3) & 0x3F\n\n        # Update number of bits.\n        self.count[0] = self.count[0] + (leninBuf << 3)\n        if self.count[0] < (leninBuf << 3):\n            self.count[1] = self.count[1] + 1\n        self.count[1] = self.count[1] + (leninBuf >> 29)\n\n        partLen = 64 - index\n\n        if leninBuf >= partLen:\n            self.input[index:] = list(inBuf[:partLen])\n            self._transform(_bytelist2long(self.input))\n            i = partLen\n            while i + 63 < leninBuf:\n                self._transform(_bytelist2long(list(inBuf[i:i+64])))\n                i = i + 64\n            else:\n                self.input = list(inBuf[i:leninBuf])\n        else:\n            i = 0\n            self.input = self.input + list(inBuf)\n\n\n    def digest(self):\n        \"\"\"Terminate the message-digest computation and return digest.\n\n        Return the digest of the strings passed to the update()\n        method so far. This is a 16-byte string which may contain\n        non-ASCII characters, including null bytes.\n        \"\"\"\n\n        A = self.A\n        B = self.B\n        C = self.C\n        D = self.D\n        input = [] + self.input\n        count = [] + self.count\n\n        index = (self.count[0] >> 3) & 0x3f\n\n        if index < 56:\n            padLen = 56 - index\n        else:\n            padLen = 120 - index\n\n        padding = [b'\\200'] + [b'\\000'] * 63\n        self.update(padding[:padLen])\n\n        # Append length (before padding).\n        bits = _bytelist2long(self.input[:56]) + count\n\n        self._transform(bits)\n\n        # Store state in digest.\n        digest = struct.pack(\"<IIII\", self.A, self.B, self.C, self.D)\n\n        self.A = A \n        self.B = B\n        self.C = C\n        self.D = D\n        self.input = input \n        self.count = count \n\n        return digest\n\n\n    def hexdigest(self):\n        \"\"\"Terminate and return digest in HEX form.\n\n        Like digest() except the digest is returned as a string of\n        length 32, containing only hexadecimal digits. This may be\n        used to exchange the value safely in email or other non-\n        binary environments.\n        \"\"\"\n\n        return ''.join(['%02x' % ord(c) for c in self.digest()])\n\n    def copy(self):\n        \"\"\"Return a clone object.\n\n        Return a copy ('clone') of the md5 object. This can be used\n        to efficiently compute the digests of strings that share\n        a common initial substring.\n        \"\"\"\n        if 0: # set this to 1 to make the flow space crash\n            return copy.deepcopy(self)\n        clone = self.__class__()\n        clone.length = self.length\n        clone.count  = [] + self.count[:]\n        clone.input  = [] + self.input\n        clone.A = self.A\n        clone.B = self.B\n        clone.C = self.C\n        clone.D = self.D\n        return clone\n\n\n# ======================================================================\n# Mimic Python top-level functions from standard library API\n# for consistency with the _md5 module of the standard library.\n# ======================================================================\n\ndigest_size = 16\n\ndef new(arg=None):\n    \"\"\"Return a new md5 crypto object.\n    If arg is present, the method call update(arg) is made.\n    \"\"\"\n\n    crypto = MD5Type()\n    if arg:\n        crypto.update(arg)\n\n    return crypto\n\n",
		"file_name": "_md5.py"
	},
	{
		"content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note that PyPy contains also a built-in module 'sha' which will hide\n# this one if compiled in.\n\n\"\"\"A sample implementation of SHA-1 in pure Python.\n\n   Framework adapted from Dinu Gherman's MD5 implementation by\n   J. Hall\u00e9n and L. Creighton. SHA-1 implementation based directly on\n   the text of the NIST standard FIPS PUB 180-1.\n\"\"\"\n\n\n__date__    = '2004-11-17'\n__version__ = 0.91 # Modernised by J. Hall\u00e9n and L. Creighton for Pypy\n\n\nimport struct, copy\n\n\n# ======================================================================\n# Bit-Manipulation helpers\n#\n#   _long2bytes() was contributed by Barry Warsaw\n#   and is reused here with tiny modifications.\n# ======================================================================\n\ndef _long2bytesBigEndian(n, blocksize=0):\n    \"\"\"Convert a long integer to a byte string.\n\n    If optional blocksize is given and greater than zero, pad the front\n    of the byte string with binary zeros so that the length is a multiple\n    of blocksize.\n    \"\"\"\n\n    # After much testing, this algorithm was deemed to be the fastest.\n    s = b''\n    pack = struct.pack\n    while n > 0:\n        s = pack('>I', n & 0xffffffff) + s\n        n = n >> 32\n\n    # Strip off leading zeros.\n    for i in range(len(s)):\n        if s[i] != '\\000':\n            break\n    else:\n        # Only happens when n == 0.\n        s = '\\000'\n        i = 0\n\n    s = s[i:]\n\n    # Add back some pad bytes. This could be done more efficiently\n    # w.r.t. the de-padding being done above, but sigh...\n    if blocksize > 0 and len(s) % blocksize:\n        s = (blocksize - len(s) % blocksize) * '\\000' + s\n\n    return s\n\n\ndef _bytelist2longBigEndian(list):\n    \"Transform a list of characters into a list of longs.\"\n\n    imax = len(list) // 4\n    hl = [0] * imax\n\n    j = 0\n    i = 0\n    while i < imax:\n        b0 = ord(list[j]) << 24\n        b1 = ord(list[j+1]) << 16\n        b2 = ord(list[j+2]) << 8\n        b3 = ord(list[j+3])\n        hl[i] = b0 | b1 | b2 | b3\n        i = i+1\n        j = j+4\n\n    return hl\n\n\ndef _rotateLeft(x, n):\n    \"Rotate x (32 bit) left n bits circularly.\"\n\n    return (x << n) | (x >> (32-n))\n\n\n# ======================================================================\n# The SHA transformation functions\n#\n# ======================================================================\n\ndef f0_19(B, C, D):\n    return (B & C) | ((~ B) & D)\n\ndef f20_39(B, C, D):\n    return B ^ C ^ D\n\ndef f40_59(B, C, D):\n    return (B & C) | (B & D) | (C & D)\n\ndef f60_79(B, C, D):\n    return B ^ C ^ D\n\n\nf = [f0_19, f20_39, f40_59, f60_79]\n\n# Constants to be used\nK = [\n    0x5A827999, # ( 0 <= t <= 19)\n    0x6ED9EBA1, # (20 <= t <= 39)\n    0x8F1BBCDC, # (40 <= t <= 59)\n    0xCA62C1D6  # (60 <= t <= 79)\n    ]\n\nclass sha:\n    \"An implementation of the SHA hash function in pure Python.\"\n\n    digest_size = digestsize = 20\n    block_size = 512 // 8\n\n    def __init__(self):\n        \"Initialisation.\"\n\n        # Initial message length in bits(!).\n        self.length = 0\n        self.count = [0, 0]\n\n        # Initial empty message as a sequence of bytes (8 bit characters).\n        self.input = []\n\n        # Call a separate init function, that can be used repeatedly\n        # to start from scratch on the same object.\n        self.init()\n\n\n    def init(self):\n        \"Initialize the message-digest and set all fields to zero.\"\n\n        self.length = 0\n        self.input = []\n\n        # Initial 160 bit message digest (5 times 32 bit).\n        self.H0 = 0x67452301\n        self.H1 = 0xEFCDAB89\n        self.H2 = 0x98BADCFE\n        self.H3 = 0x10325476\n        self.H4 = 0xC3D2E1F0\n\n    def _transform(self, W):\n\n        for t in range(16, 80):\n            W.append(_rotateLeft(\n                W[t-3] ^ W[t-8] ^ W[t-14] ^ W[t-16], 1) & 0xffffffff)\n\n        A = self.H0\n        B = self.H1\n        C = self.H2\n        D = self.H3\n        E = self.H4\n\n        \"\"\"\n        This loop was unrolled to gain about 10% in speed\n        for t in range(0, 80):\n            TEMP = _rotateLeft(A, 5) + f[t/20] + E + W[t] + K[t/20]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n        \"\"\"\n\n        for t in range(0, 20):\n            TEMP = _rotateLeft(A, 5) + ((B & C) | ((~ B) & D)) + E + W[t] + K[0]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n        for t in range(20, 40):\n            TEMP = _rotateLeft(A, 5) + (B ^ C ^ D) + E + W[t] + K[1]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n        for t in range(40, 60):\n            TEMP = _rotateLeft(A, 5) + ((B & C) | (B & D) | (C & D)) + E + W[t] + K[2]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n        for t in range(60, 80):\n            TEMP = _rotateLeft(A, 5) + (B ^ C ^ D)  + E + W[t] + K[3]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n\n        self.H0 = (self.H0 + A) & 0xffffffff\n        self.H1 = (self.H1 + B) & 0xffffffff\n        self.H2 = (self.H2 + C) & 0xffffffff\n        self.H3 = (self.H3 + D) & 0xffffffff\n        self.H4 = (self.H4 + E) & 0xffffffff\n\n\n    # Down from here all methods follow the Python Standard Library\n    # API of the sha module.\n\n    def update(self, inBuf):\n        \"\"\"Add to the current message.\n\n        Update the md5 object with the string arg. Repeated calls\n        are equivalent to a single call with the concatenation of all\n        the arguments, i.e. m.update(a); m.update(b) is equivalent\n        to m.update(a+b).\n\n        The hash is immediately calculated for all full blocks. The final\n        calculation is made in digest(). It will calculate 1-2 blocks,\n        depending on how much padding we have to add. This allows us to\n        keep an intermediate value for the hash, so that we only need to\n        make minimal recalculation if we call update() to add more data\n        to the hashed string.\n        \"\"\"\n\n        leninBuf = len(inBuf)\n\n        # Compute number of bytes mod 64.\n        index = (self.count[1] >> 3) & 0x3F\n\n        # Update number of bits.\n        self.count[1] = self.count[1] + (leninBuf << 3)\n        if self.count[1] < (leninBuf << 3):\n            self.count[0] = self.count[0] + 1\n        self.count[0] = self.count[0] + (leninBuf >> 29)\n\n        partLen = 64 - index\n\n        if leninBuf >= partLen:\n            self.input[index:] = list(inBuf[:partLen])\n            self._transform(_bytelist2longBigEndian(self.input))\n            i = partLen\n            while i + 63 < leninBuf:\n                self._transform(_bytelist2longBigEndian(list(inBuf[i:i+64])))\n                i = i + 64\n            else:\n                self.input = list(inBuf[i:leninBuf])\n        else:\n            i = 0\n            self.input = self.input + list(inBuf)\n\n\n    def digest(self):\n        \"\"\"Terminate the message-digest computation and return digest.\n\n        Return the digest of the strings passed to the update()\n        method so far. This is a 16-byte string which may contain\n        non-ASCII characters, including null bytes.\n        \"\"\"\n\n        H0 = self.H0\n        H1 = self.H1\n        H2 = self.H2\n        H3 = self.H3\n        H4 = self.H4\n        input = [] + self.input\n        count = [] + self.count\n\n        index = (self.count[1] >> 3) & 0x3f\n\n        if index < 56:\n            padLen = 56 - index\n        else:\n            padLen = 120 - index\n\n        padding = ['\\200'] + ['\\000'] * 63\n        self.update(padding[:padLen])\n\n        # Append length (before padding).\n        bits = _bytelist2longBigEndian(self.input[:56]) + count\n\n        self._transform(bits)\n\n        # Store state in digest.\n        digest = _long2bytesBigEndian(self.H0, 4) + \\\n                 _long2bytesBigEndian(self.H1, 4) + \\\n                 _long2bytesBigEndian(self.H2, 4) + \\\n                 _long2bytesBigEndian(self.H3, 4) + \\\n                 _long2bytesBigEndian(self.H4, 4)\n\n        self.H0 = H0\n        self.H1 = H1\n        self.H2 = H2\n        self.H3 = H3\n        self.H4 = H4\n        self.input = input\n        self.count = count\n\n        return digest\n\n\n    def hexdigest(self):\n        \"\"\"Terminate and return digest in HEX form.\n\n        Like digest() except the digest is returned as a string of\n        length 32, containing only hexadecimal digits. This may be\n        used to exchange the value safely in email or other non-\n        binary environments.\n        \"\"\"\n        return ''.join(['%02x' % ord(c) for c in self.digest()])\n\n    def copy(self):\n        \"\"\"Return a clone object.\n\n        Return a copy ('clone') of the md5 object. This can be used\n        to efficiently compute the digests of strings that share\n        a common initial substring.\n        \"\"\"\n\n        return copy.deepcopy(self)\n\n\n# ======================================================================\n# Mimic Python top-level functions from standard library API\n# for consistency with the _sha module of the standard library.\n# ======================================================================\n\n# These are mandatory variables in the module. They have constant values\n# in the SHA standard.\n\ndigest_size = 20\ndigestsize = 20\nblocksize = 1\n\ndef new(arg=None):\n    \"\"\"Return a new sha crypto object.\n\n    If arg is present, the method call update(arg) is made.\n    \"\"\"\n\n    crypto = sha()\n    if arg:\n        crypto.update(arg)\n\n    return crypto\n",
		"file_name": "_sha.py"
	},
	{
		"content": "# Copyright 2001-2014 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\"\"\"\nLogging package for Python. Based on PEP 282 and comments thereto in\ncomp.lang.python.\n\nCopyright (C) 2001-2014 Vinay Sajip. All Rights Reserved.\n\nTo use, simply 'import logging' and log away!\n\"\"\"\n\nimport sys, os, time, cStringIO, traceback, warnings, weakref, collections\n\n__all__ = ['BASIC_FORMAT', 'BufferingFormatter', 'CRITICAL', 'DEBUG', 'ERROR',\n           'FATAL', 'FileHandler', 'Filter', 'Formatter', 'Handler', 'INFO',\n           'LogRecord', 'Logger', 'LoggerAdapter', 'NOTSET', 'NullHandler',\n           'StreamHandler', 'WARN', 'WARNING', 'addLevelName', 'basicConfig',\n           'captureWarnings', 'critical', 'debug', 'disable', 'error',\n           'exception', 'fatal', 'getLevelName', 'getLogger', 'getLoggerClass',\n           'info', 'log', 'makeLogRecord', 'setLoggerClass', 'warn', 'warning']\n\ntry:\n    import codecs\nexcept ImportError:\n    codecs = None\n\ntry:\n    import thread\n    import threading\nexcept ImportError:\n    thread = None\n\n__author__  = \"Vinay Sajip <vinay_sajip@red-dove.com>\"\n__status__  = \"production\"\n# Note: the attributes below are no longer maintained.\n__version__ = \"0.5.1.2\"\n__date__    = \"07 February 2010\"\n\n#---------------------------------------------------------------------------\n#   Miscellaneous module data\n#---------------------------------------------------------------------------\ntry:\n    unicode\n    _unicode = True\nexcept NameError:\n    _unicode = False\n\n#\n# _srcfile is used when walking the stack to check when we've got the first\n# caller stack frame.\n#\nif hasattr(sys, 'frozen'): #support for py2exe\n    _srcfile = \"logging%s__init__%s\" % (os.sep, __file__[-4:])\nelif __file__[-4:].lower() in ['.pyc', '.pyo']:\n    _srcfile = __file__[:-4] + '.py'\nelse:\n    _srcfile = __file__\n_srcfile = os.path.normcase(_srcfile)\n\n# next bit filched from 1.5.2's inspect.py\ndef currentframe():\n    \"\"\"Return the frame object for the caller's stack frame.\"\"\"\n    try:\n        raise Exception\n    except:\n        return sys.exc_info()[2].tb_frame.f_back\n\nif hasattr(sys, '_getframe'): currentframe = lambda: sys._getframe(3)\n# done filching\n\n# _srcfile is only used in conjunction with sys._getframe().\n# To provide compatibility with older versions of Python, set _srcfile\n# to None if _getframe() is not available; this value will prevent\n# findCaller() from being called.\n#if not hasattr(sys, \"_getframe\"):\n#    _srcfile = None\n\n#\n#_startTime is used as the base when calculating the relative time of events\n#\n_startTime = time.time()\n\n#\n#raiseExceptions is used to see if exceptions during handling should be\n#propagated\n#\nraiseExceptions = 1\n\n#\n# If you don't want threading information in the log, set this to zero\n#\nlogThreads = 1\n\n#\n# If you don't want multiprocessing information in the log, set this to zero\n#\nlogMultiprocessing = 1\n\n#\n# If you don't want process information in the log, set this to zero\n#\nlogProcesses = 1\n\n#---------------------------------------------------------------------------\n#   Level related stuff\n#---------------------------------------------------------------------------\n#\n# Default levels and level names, these can be replaced with any positive set\n# of values having corresponding names. There is a pseudo-level, NOTSET, which\n# is only really there as a lower limit for user-defined levels. Handlers and\n# loggers are initialized with NOTSET so that they will log all messages, even\n# at user-defined levels.\n#\n\nCRITICAL = 50\nFATAL = CRITICAL\nERROR = 40\nWARNING = 30\nWARN = WARNING\nINFO = 20\nDEBUG = 10\nNOTSET = 0\n\n# NOTE(flaper87): This is different from\n# python's stdlib module since pypy's\n# dicts are much faster when their\n# keys are all of the same type.\n# Introduced in commit 9de7b40c586f\n_levelToName = {\n    CRITICAL: 'CRITICAL',\n    ERROR: 'ERROR',\n    WARNING: 'WARNING',\n    INFO: 'INFO',\n    DEBUG: 'DEBUG',\n    NOTSET: 'NOTSET',\n}\n_nameToLevel = {\n    'CRITICAL': CRITICAL,\n    'ERROR': ERROR,\n    'WARN': WARNING,\n    'WARNING': WARNING,\n    'INFO': INFO,\n    'DEBUG': DEBUG,\n    'NOTSET': NOTSET,\n}\n_levelNames = dict(_levelToName)\n_levelNames.update(_nameToLevel)   # backward compatibility\n\ndef getLevelName(level):\n    \"\"\"\n    Return the textual representation of logging level 'level'.\n\n    If the level is one of the predefined levels (CRITICAL, ERROR, WARNING,\n    INFO, DEBUG) then you get the corresponding string. If you have\n    associated levels with names using addLevelName then the name you have\n    associated with 'level' is returned.\n\n    If a numeric value corresponding to one of the defined levels is passed\n    in, the corresponding string representation is returned.\n\n    Otherwise, the string \"Level %s\" % level is returned.\n    \"\"\"\n\n    # NOTE(flaper87): Check also in _nameToLevel\n    # if value is None.\n    return (_levelToName.get(level) or\n            _nameToLevel.get(level, (\"Level %s\" % level)))\n\ndef addLevelName(level, levelName):\n    \"\"\"\n    Associate 'levelName' with 'level'.\n\n    This is used when converting levels to text during message formatting.\n    \"\"\"\n    _acquireLock()\n    try:    #unlikely to cause an exception, but you never know...\n        _levelToName[level] = levelName\n        _nameToLevel[levelName] = level\n    finally:\n        _releaseLock()\n\ndef _checkLevel(level):\n    if isinstance(level, (int, long)):\n        rv = level\n    elif str(level) == level:\n        if level not in _nameToLevel:\n            raise ValueError(\"Unknown level: %r\" % level)\n        rv = _nameToLevel[level]\n    else:\n        raise TypeError(\"Level not an integer or a valid string: %r\" % level)\n    return rv\n\n#---------------------------------------------------------------------------\n#   Thread-related stuff\n#---------------------------------------------------------------------------\n\n#\n#_lock is used to serialize access to shared data structures in this module.\n#This needs to be an RLock because fileConfig() creates and configures\n#Handlers, and so might arbitrary user threads. Since Handler code updates the\n#shared dictionary _handlers, it needs to acquire the lock. But if configuring,\n#the lock would already have been acquired - so we need an RLock.\n#The same argument applies to Loggers and Manager.loggerDict.\n#\nif thread:\n    _lock = threading.RLock()\nelse:\n    _lock = None\n\ndef _acquireLock():\n    \"\"\"\n    Acquire the module-level lock for serializing access to shared data.\n\n    This should be released with _releaseLock().\n    \"\"\"\n    if _lock:\n        _lock.acquire()\n\ndef _releaseLock():\n    \"\"\"\n    Release the module-level lock acquired by calling _acquireLock().\n    \"\"\"\n    if _lock:\n        _lock.release()\n\n#---------------------------------------------------------------------------\n#   The logging record\n#---------------------------------------------------------------------------\n\nclass LogRecord(object):\n    \"\"\"\n    A LogRecord instance represents an event being logged.\n\n    LogRecord instances are created every time something is logged. They\n    contain all the information pertinent to the event being logged. The\n    main information passed in is in msg and args, which are combined\n    using str(msg) % args to create the message field of the record. The\n    record also includes information such as when the record was created,\n    the source line where the logging call was made, and any exception\n    information to be logged.\n    \"\"\"\n    def __init__(self, name, level, pathname, lineno,\n                 msg, args, exc_info, func=None):\n        \"\"\"\n        Initialize a logging record with interesting information.\n        \"\"\"\n        ct = time.time()\n        self.name = name\n        self.msg = msg\n        #\n        # The following statement allows passing of a dictionary as a sole\n        # argument, so that you can do something like\n        #  logging.debug(\"a %(a)d b %(b)s\", {'a':1, 'b':2})\n        # Suggested by Stefan Behnel.\n        # Note that without the test for args[0], we get a problem because\n        # during formatting, we test to see if the arg is present using\n        # 'if self.args:'. If the event being logged is e.g. 'Value is %d'\n        # and if the passed arg fails 'if self.args:' then no formatting\n        # is done. For example, logger.warn('Value is %d', 0) would log\n        # 'Value is %d' instead of 'Value is 0'.\n        # For the use case of passing a dictionary, this should not be a\n        # problem.\n        # Issue #21172: a request was made to relax the isinstance check\n        # to hasattr(args[0], '__getitem__'). However, the docs on string\n        # formatting still seem to suggest a mapping object is required.\n        # Thus, while not removing the isinstance check, it does now look\n        # for collections.Mapping rather than, as before, dict.\n        if (args and len(args) == 1 and isinstance(args[0], collections.Mapping)\n            and args[0]):\n            args = args[0]\n        self.args = args\n        self.levelname = getLevelName(level)\n        self.levelno = level\n        self.pathname = pathname\n        try:\n            self.filename = os.path.basename(pathname)\n            self.module = os.path.splitext(self.filename)[0]\n        except (TypeError, ValueError, AttributeError):\n            self.filename = pathname\n            self.module = \"Unknown module\"\n        self.exc_info = exc_info\n        self.exc_text = None      # used to cache the traceback text\n        self.lineno = lineno\n        self.funcName = func\n        self.created = ct\n        self.msecs = (ct - int(ct)) * 1000\n        self.relativeCreated = (self.created - _startTime) * 1000\n        if logThreads and thread:\n            self.thread = thread.get_ident()\n            self.threadName = threading.current_thread().name\n        else:\n            self.thread = None\n            self.threadName = None\n        if not logMultiprocessing:\n            self.processName = None\n        else:\n            self.processName = 'MainProcess'\n            mp = sys.modules.get('multiprocessing')\n            if mp is not None:\n                # Errors may occur if multiprocessing has not finished loading\n                # yet - e.g. if a custom import hook causes third-party code\n                # to run when multiprocessing calls import. See issue 8200\n                # for an example\n                try:\n                    self.processName = mp.current_process().name\n                except StandardError:\n                    pass\n        if logProcesses and hasattr(os, 'getpid'):\n            self.process = os.getpid()\n        else:\n            self.process = None\n\n    def __str__(self):\n        return '<LogRecord: %s, %s, %s, %s, \"%s\">'%(self.name, self.levelno,\n            self.pathname, self.lineno, self.msg)\n\n    def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        if not _unicode: #if no unicode support...\n            msg = str(self.msg)\n        else:\n            msg = self.msg\n            if not isinstance(msg, basestring):\n                try:\n                    msg = str(self.msg)\n                except UnicodeError:\n                    msg = self.msg      #Defer encoding till later\n        if self.args:\n            msg = msg % self.args\n        return msg\n\ndef makeLogRecord(dict):\n    \"\"\"\n    Make a LogRecord whose attributes are defined by the specified dictionary,\n    This function is useful for converting a logging event received over\n    a socket connection (which is sent as a dictionary) into a LogRecord\n    instance.\n    \"\"\"\n    rv = LogRecord(None, None, \"\", 0, \"\", (), None, None)\n    rv.__dict__.update(dict)\n    return rv\n\n#---------------------------------------------------------------------------\n#   Formatter classes and functions\n#---------------------------------------------------------------------------\n\nclass Formatter(object):\n    \"\"\"\n    Formatter instances are used to convert a LogRecord to text.\n\n    Formatters need to know how a LogRecord is constructed. They are\n    responsible for converting a LogRecord to (usually) a string which can\n    be interpreted by either a human or an external system. The base Formatter\n    allows a formatting string to be specified. If none is supplied, the\n    default value of \"%s(message)\\\\n\" is used.\n\n    The Formatter can be initialized with a format string which makes use of\n    knowledge of the LogRecord attributes - e.g. the default value mentioned\n    above makes use of the fact that the user's message and arguments are pre-\n    formatted into a LogRecord's message attribute. Currently, the useful\n    attributes in a LogRecord are described by:\n\n    %(name)s            Name of the logger (logging channel)\n    %(levelno)s         Numeric logging level for the message (DEBUG, INFO,\n                        WARNING, ERROR, CRITICAL)\n    %(levelname)s       Text logging level for the message (\"DEBUG\", \"INFO\",\n                        \"WARNING\", \"ERROR\", \"CRITICAL\")\n    %(pathname)s        Full pathname of the source file where the logging\n                        call was issued (if available)\n    %(filename)s        Filename portion of pathname\n    %(module)s          Module (name portion of filename)\n    %(lineno)d          Source line number where the logging call was issued\n                        (if available)\n    %(funcName)s        Function name\n    %(created)f         Time when the LogRecord was created (time.time()\n                        return value)\n    %(asctime)s         Textual time when the LogRecord was created\n    %(msecs)d           Millisecond portion of the creation time\n    %(relativeCreated)d Time in milliseconds when the LogRecord was created,\n                        relative to the time the logging module was loaded\n                        (typically at application startup time)\n    %(thread)d          Thread ID (if available)\n    %(threadName)s      Thread name (if available)\n    %(process)d         Process ID (if available)\n    %(message)s         The result of record.getMessage(), computed just as\n                        the record is emitted\n    \"\"\"\n\n    converter = time.localtime\n\n    def __init__(self, fmt=None, datefmt=None):\n        \"\"\"\n        Initialize the formatter with specified format strings.\n\n        Initialize the formatter either with the specified format string, or a\n        default as described above. Allow for specialized date formatting with\n        the optional datefmt argument (if omitted, you get the ISO8601 format).\n        \"\"\"\n        if fmt:\n            self._fmt = fmt\n        else:\n            self._fmt = \"%(message)s\"\n        self.datefmt = datefmt\n\n    def formatTime(self, record, datefmt=None):\n        \"\"\"\n        Return the creation time of the specified LogRecord as formatted text.\n\n        This method should be called from format() by a formatter which\n        wants to make use of a formatted time. This method can be overridden\n        in formatters to provide for any specific requirement, but the\n        basic behaviour is as follows: if datefmt (a string) is specified,\n        it is used with time.strftime() to format the creation time of the\n        record. Otherwise, the ISO8601 format is used. The resulting\n        string is returned. This function uses a user-configurable function\n        to convert the creation time to a tuple. By default, time.localtime()\n        is used; to change this for a particular formatter instance, set the\n        'converter' attribute to a function with the same signature as\n        time.localtime() or time.gmtime(). To change it for all formatters,\n        for example if you want all logging times to be shown in GMT,\n        set the 'converter' attribute in the Formatter class.\n        \"\"\"\n        ct = self.converter(record.created)\n        if datefmt:\n            s = time.strftime(datefmt, ct)\n        else:\n            t = time.strftime(\"%Y-%m-%d %H:%M:%S\", ct)\n            s = \"%s,%03d\" % (t, record.msecs)\n        return s\n\n    def formatException(self, ei):\n        \"\"\"\n        Format and return the specified exception information as a string.\n\n        This default implementation just uses\n        traceback.print_exception()\n        \"\"\"\n        sio = cStringIO.StringIO()\n        traceback.print_exception(ei[0], ei[1], ei[2], None, sio)\n        s = sio.getvalue()\n        sio.close()\n        if s[-1:] == \"\\n\":\n            s = s[:-1]\n        return s\n\n    def usesTime(self):\n        \"\"\"\n        Check if the format uses the creation time of the record.\n        \"\"\"\n        return self._fmt.find(\"%(asctime)\") >= 0\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record as text.\n\n        The record's attribute dictionary is used as the operand to a\n        string formatting operation which yields the returned string.\n        Before formatting the dictionary, a couple of preparatory steps\n        are carried out. The message attribute of the record is computed\n        using LogRecord.getMessage(). If the formatting string uses the\n        time (as determined by a call to usesTime(), formatTime() is\n        called to format the event time. If there is exception information,\n        it is formatted using formatException() and appended to the message.\n        \"\"\"\n        record.message = record.getMessage()\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n        s = self._fmt % record.__dict__\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            try:\n                s = s + record.exc_text\n            except UnicodeError:\n                # Sometimes filenames have non-ASCII chars, which can lead\n                # to errors when s is Unicode and record.exc_text is str\n                # See issue 8924.\n                # We also use replace for when there are multiple\n                # encodings, e.g. UTF-8 for the filesystem and latin-1\n                # for a script. See issue 13232.\n                s = s + record.exc_text.decode(sys.getfilesystemencoding(),\n                                               'replace')\n        return s\n\n#\n#   The default formatter to use when no other is specified\n#\n_defaultFormatter = Formatter()\n\nclass BufferingFormatter(object):\n    \"\"\"\n    A formatter suitable for formatting a number of records.\n    \"\"\"\n    def __init__(self, linefmt=None):\n        \"\"\"\n        Optionally specify a formatter which will be used to format each\n        individual record.\n        \"\"\"\n        if linefmt:\n            self.linefmt = linefmt\n        else:\n            self.linefmt = _defaultFormatter\n\n    def formatHeader(self, records):\n        \"\"\"\n        Return the header string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def formatFooter(self, records):\n        \"\"\"\n        Return the footer string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def format(self, records):\n        \"\"\"\n        Format the specified records and return the result as a string.\n        \"\"\"\n        rv = \"\"\n        if len(records) > 0:\n            rv = rv + self.formatHeader(records)\n            for record in records:\n                rv = rv + self.linefmt.format(record)\n            rv = rv + self.formatFooter(records)\n        return rv\n\n#---------------------------------------------------------------------------\n#   Filter classes and functions\n#---------------------------------------------------------------------------\n\nclass Filter(object):\n    \"\"\"\n    Filter instances are used to perform arbitrary filtering of LogRecords.\n\n    Loggers and Handlers can optionally use Filter instances to filter\n    records as desired. The base filter class only allows events which are\n    below a certain point in the logger hierarchy. For example, a filter\n    initialized with \"A.B\" will allow events logged by loggers \"A.B\",\n    \"A.B.C\", \"A.B.C.D\", \"A.B.D\" etc. but not \"A.BB\", \"B.A.B\" etc. If\n    initialized with the empty string, all events are passed.\n    \"\"\"\n    def __init__(self, name=''):\n        \"\"\"\n        Initialize a filter.\n\n        Initialize with the name of the logger which, together with its\n        children, will have its events allowed through the filter. If no\n        name is specified, allow every event.\n        \"\"\"\n        self.name = name\n        self.nlen = len(name)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if the specified record is to be logged.\n\n        Is the specified record to be logged? Returns 0 for no, nonzero for\n        yes. If deemed appropriate, the record may be modified in-place.\n        \"\"\"\n        if self.nlen == 0:\n            return 1\n        elif self.name == record.name:\n            return 1\n        elif record.name.find(self.name, 0, self.nlen) != 0:\n            return 0\n        return (record.name[self.nlen] == \".\")\n\nclass Filterer(object):\n    \"\"\"\n    A base class for loggers and handlers which allows them to share\n    common code.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the list of filters to be an empty list.\n        \"\"\"\n        self.filters = []\n\n    def addFilter(self, filter):\n        \"\"\"\n        Add the specified filter to this handler.\n        \"\"\"\n        if not (filter in self.filters):\n            self.filters.append(filter)\n\n    def removeFilter(self, filter):\n        \"\"\"\n        Remove the specified filter from this handler.\n        \"\"\"\n        if filter in self.filters:\n            self.filters.remove(filter)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if a record is loggable by consulting all the filters.\n\n        The default is to allow the record to be logged; any filter can veto\n        this and the record is then dropped. Returns a zero value if a record\n        is to be dropped, else non-zero.\n        \"\"\"\n        rv = 1\n        for f in self.filters:\n            if not f.filter(record):\n                rv = 0\n                break\n        return rv\n\n#---------------------------------------------------------------------------\n#   Handler classes and functions\n#---------------------------------------------------------------------------\n\n_handlers = weakref.WeakValueDictionary()  #map of handler names to handlers\n_handlerList = [] # added to allow handlers to be removed in reverse of order initialized\n\ndef _removeHandlerRef(wr):\n    \"\"\"\n    Remove a handler reference from the internal cleanup list.\n    \"\"\"\n    # This function can be called during module teardown, when globals are\n    # set to None. It can also be called from another thread. So we need to\n    # pre-emptively grab the necessary globals and check if they're None,\n    # to prevent race conditions and failures during interpreter shutdown.\n    acquire, release, handlers = _acquireLock, _releaseLock, _handlerList\n    if acquire and release and handlers:\n        acquire()\n        try:\n            if wr in handlers:\n                handlers.remove(wr)\n        finally:\n            release()\n\ndef _addHandlerRef(handler):\n    \"\"\"\n    Add a handler to the internal cleanup list using a weak reference.\n    \"\"\"\n    _acquireLock()\n    try:\n        _handlerList.append(weakref.ref(handler, _removeHandlerRef))\n    finally:\n        _releaseLock()\n\nclass Handler(Filterer):\n    \"\"\"\n    Handler instances dispatch logging events to specific destinations.\n\n    The base handler class. Acts as a placeholder which defines the Handler\n    interface. Handlers can optionally use Formatter instances to format\n    records as desired. By default, no formatter is specified; in this case,\n    the 'raw' message as determined by record.message is logged.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initializes the instance - basically setting the formatter to None\n        and the filter list to empty.\n        \"\"\"\n        Filterer.__init__(self)\n        self._name = None\n        self.level = _checkLevel(level)\n        self.formatter = None\n        # Add the handler to the global _handlerList (for cleanup on shutdown)\n        _addHandlerRef(self)\n        self.createLock()\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, name):\n        _acquireLock()\n        try:\n            if self._name in _handlers:\n                del _handlers[self._name]\n            self._name = name\n            if name:\n                _handlers[name] = self\n        finally:\n            _releaseLock()\n\n    name = property(get_name, set_name)\n\n    def createLock(self):\n        \"\"\"\n        Acquire a thread lock for serializing access to the underlying I/O.\n        \"\"\"\n        if thread:\n            self.lock = threading.RLock()\n        else:\n            self.lock = None\n\n    def acquire(self):\n        \"\"\"\n        Acquire the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.acquire()\n\n    def release(self):\n        \"\"\"\n        Release the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.release()\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this handler.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record.\n\n        If a formatter is set, use it. Otherwise, use the default formatter\n        for the module.\n        \"\"\"\n        if self.formatter:\n            fmt = self.formatter\n        else:\n            fmt = _defaultFormatter\n        return fmt.format(record)\n\n    def emit(self, record):\n        \"\"\"\n        Do whatever it takes to actually log the specified logging record.\n\n        This version is intended to be implemented by subclasses and so\n        raises a NotImplementedError.\n        \"\"\"\n        raise NotImplementedError('emit must be implemented '\n                                  'by Handler subclasses')\n\n    def handle(self, record):\n        \"\"\"\n        Conditionally emit the specified logging record.\n\n        Emission depends on filters which may have been added to the handler.\n        Wrap the actual emission of the record with acquisition/release of\n        the I/O thread lock. Returns whether the filter passed the record for\n        emission.\n        \"\"\"\n        rv = self.filter(record)\n        if rv:\n            self.acquire()\n            try:\n                self.emit(record)\n            finally:\n                self.release()\n        return rv\n\n    def setFormatter(self, fmt):\n        \"\"\"\n        Set the formatter for this handler.\n        \"\"\"\n        self.formatter = fmt\n\n    def flush(self):\n        \"\"\"\n        Ensure all logging output has been flushed.\n\n        This version does nothing and is intended to be implemented by\n        subclasses.\n        \"\"\"\n        pass\n\n    def close(self):\n        \"\"\"\n        Tidy up any resources used by the handler.\n\n        This version removes the handler from an internal map of handlers,\n        _handlers, which is used for handler lookup by name. Subclasses\n        should ensure that this gets called from overridden close()\n        methods.\n        \"\"\"\n        #get the module data lock, as we're updating a shared structure.\n        _acquireLock()\n        try:    #unlikely to raise an exception, but you never know...\n            if self._name and self._name in _handlers:\n                del _handlers[self._name]\n        finally:\n            _releaseLock()\n\n    def handleError(self, record):\n        \"\"\"\n        Handle errors which occur during an emit() call.\n\n        This method should be called from handlers when an exception is\n        encountered during an emit() call. If raiseExceptions is false,\n        exceptions get silently ignored. This is what is mostly wanted\n        for a logging system - most users will not care about errors in\n        the logging system, they are more interested in application errors.\n        You could, however, replace this with a custom handler if you wish.\n        The record which was being processed is passed in to this method.\n        \"\"\"\n        if raiseExceptions and sys.stderr:  # see issue 13807\n            ei = sys.exc_info()\n            try:\n                traceback.print_exception(ei[0], ei[1], ei[2],\n                                          None, sys.stderr)\n                sys.stderr.write('Logged from file %s, line %s\\n' % (\n                                 record.filename, record.lineno))\n            except IOError:\n                pass    # see issue 5971\n            finally:\n                del ei\n\nclass StreamHandler(Handler):\n    \"\"\"\n    A handler class which writes logging records, appropriately formatted,\n    to a stream. Note that this class does not close the stream, as\n    sys.stdout or sys.stderr may be used.\n    \"\"\"\n\n    def __init__(self, stream=None):\n        \"\"\"\n        Initialize the handler.\n\n        If stream is not specified, sys.stderr is used.\n        \"\"\"\n        Handler.__init__(self)\n        if stream is None:\n            stream = sys.stderr\n        self.stream = stream\n\n    def flush(self):\n        \"\"\"\n        Flushes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream and hasattr(self.stream, \"flush\"):\n                self.stream.flush()\n        finally:\n            self.release()\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If a formatter is specified, it is used to format the record.\n        The record is then written to the stream with a trailing newline.  If\n        exception information is present, it is formatted using\n        traceback.print_exception and appended to the stream.  If the stream\n        has an 'encoding' attribute, it is used to determine how to do the\n        output to the stream.\n        \"\"\"\n        try:\n            msg = self.format(record)\n            stream = self.stream\n            fs = \"%s\\n\"\n            if not _unicode: #if no unicode support...\n                stream.write(fs % msg)\n            else:\n                try:\n                    if (isinstance(msg, unicode) and\n                        getattr(stream, 'encoding', None)):\n                        ufs = u'%s\\n'\n                        try:\n                            stream.write(ufs % msg)\n                        except UnicodeEncodeError:\n                            #Printing to terminals sometimes fails. For example,\n                            #with an encoding of 'cp1251', the above write will\n                            #work if written to a stream opened or wrapped by\n                            #the codecs module, but fail when writing to a\n                            #terminal even when the codepage is set to cp1251.\n                            #An extra encoding step seems to be needed.\n                            stream.write((ufs % msg).encode(stream.encoding))\n                    else:\n                        stream.write(fs % msg)\n                except UnicodeError:\n                    stream.write(fs % msg.encode(\"UTF-8\"))\n            self.flush()\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.handleError(record)\n\nclass FileHandler(StreamHandler):\n    \"\"\"\n    A handler class which writes formatted logging records to disk files.\n    \"\"\"\n    def __init__(self, filename, mode='a', encoding=None, delay=0):\n        \"\"\"\n        Open the specified file and use it as the stream for logging.\n        \"\"\"\n        #keep the absolute path, otherwise derived classes which use this\n        #may come a cropper when the current directory changes\n        if codecs is None:\n            encoding = None\n        self.baseFilename = os.path.abspath(filename)\n        self.mode = mode\n        self.encoding = encoding\n        self.delay = delay\n        if delay:\n            #We don't open the stream, but we still need to call the\n            #Handler constructor to set level, formatter, lock etc.\n            Handler.__init__(self)\n            self.stream = None\n        else:\n            StreamHandler.__init__(self, self._open())\n\n    def close(self):\n        \"\"\"\n        Closes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream:\n                self.flush()\n                if hasattr(self.stream, \"close\"):\n                    self.stream.close()\n                self.stream = None\n            # Issue #19523: call unconditionally to\n            # prevent a handler leak when delay is set\n            StreamHandler.close(self)\n        finally:\n            self.release()\n\n    def _open(self):\n        \"\"\"\n        Open the current base file with the (original) mode and encoding.\n        Return the resulting stream.\n        \"\"\"\n        if self.encoding is None:\n            stream = open(self.baseFilename, self.mode)\n        else:\n            stream = codecs.open(self.baseFilename, self.mode, self.encoding)\n        return stream\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If the stream was not opened because 'delay' was specified in the\n        constructor, open it before calling the superclass's emit.\n        \"\"\"\n        if self.stream is None:\n            self.stream = self._open()\n        StreamHandler.emit(self, record)\n\n#---------------------------------------------------------------------------\n#   Manager classes and functions\n#---------------------------------------------------------------------------\n\nclass PlaceHolder(object):\n    \"\"\"\n    PlaceHolder instances are used in the Manager logger hierarchy to take\n    the place of nodes for which no loggers have been defined. This class is\n    intended for internal use only and not as part of the public API.\n    \"\"\"\n    def __init__(self, alogger):\n        \"\"\"\n        Initialize with the specified logger being a child of this placeholder.\n        \"\"\"\n        #self.loggers = [alogger]\n        self.loggerMap = { alogger : None }\n\n    def append(self, alogger):\n        \"\"\"\n        Add the specified logger as a child of this placeholder.\n        \"\"\"\n        #if alogger not in self.loggers:\n        if alogger not in self.loggerMap:\n            #self.loggers.append(alogger)\n            self.loggerMap[alogger] = None\n\n#\n#   Determine which class to use when instantiating loggers.\n#\n_loggerClass = None\n\ndef setLoggerClass(klass):\n    \"\"\"\n    Set the class to be used when instantiating a logger. The class should\n    define __init__() such that only a name argument is required, and the\n    __init__() should call Logger.__init__()\n    \"\"\"\n    if klass != Logger:\n        if not issubclass(klass, Logger):\n            raise TypeError(\"logger not derived from logging.Logger: \"\n                            + klass.__name__)\n    global _loggerClass\n    _loggerClass = klass\n\ndef getLoggerClass():\n    \"\"\"\n    Return the class to be used when instantiating a logger.\n    \"\"\"\n\n    return _loggerClass\n\nclass Manager(object):\n    \"\"\"\n    There is [under normal circumstances] just one Manager instance, which\n    holds the hierarchy of loggers.\n    \"\"\"\n    def __init__(self, rootnode):\n        \"\"\"\n        Initialize the manager with the root node of the logger hierarchy.\n        \"\"\"\n        self.root = rootnode\n        self.disable = 0\n        self.emittedNoHandlerWarning = 0\n        self.loggerDict = {}\n        self.loggerClass = None\n\n    def getLogger(self, name):\n        \"\"\"\n        Get a logger with the specified name (channel name), creating it\n        if it doesn't yet exist. This name is a dot-separated hierarchical\n        name, such as \"a\", \"a.b\", \"a.b.c\" or similar.\n\n        If a PlaceHolder existed for the specified name [i.e. the logger\n        didn't exist but a child of it did], replace it with the created\n        logger and fix up the parent/child references which pointed to the\n        placeholder to now point to the logger.\n        \"\"\"\n        rv = None\n        if not isinstance(name, basestring):\n            raise TypeError('A logger name must be string or Unicode')\n        if isinstance(name, unicode):\n            name = name.encode('utf-8')\n        _acquireLock()\n        try:\n            if name in self.loggerDict:\n                rv = self.loggerDict[name]\n                if isinstance(rv, PlaceHolder):\n                    ph = rv\n                    rv = (self.loggerClass or _loggerClass)(name)\n                    rv.manager = self\n                    self.loggerDict[name] = rv\n                    self._fixupChildren(ph, rv)\n                    self._fixupParents(rv)\n            else:\n                rv = (self.loggerClass or _loggerClass)(name)\n                rv.manager = self\n                self.loggerDict[name] = rv\n                self._fixupParents(rv)\n        finally:\n            _releaseLock()\n        return rv\n\n    def setLoggerClass(self, klass):\n        \"\"\"\n        Set the class to be used when instantiating a logger with this Manager.\n        \"\"\"\n        if klass != Logger:\n            if not issubclass(klass, Logger):\n                raise TypeError(\"logger not derived from logging.Logger: \"\n                                + klass.__name__)\n        self.loggerClass = klass\n\n    def _fixupParents(self, alogger):\n        \"\"\"\n        Ensure that there are either loggers or placeholders all the way\n        from the specified logger to the root of the logger hierarchy.\n        \"\"\"\n        name = alogger.name\n        i = name.rfind(\".\")\n        rv = None\n        while (i > 0) and not rv:\n            substr = name[:i]\n            if substr not in self.loggerDict:\n                self.loggerDict[substr] = PlaceHolder(alogger)\n            else:\n                obj = self.loggerDict[substr]\n                if isinstance(obj, Logger):\n                    rv = obj\n                else:\n                    assert isinstance(obj, PlaceHolder)\n                    obj.append(alogger)\n            i = name.rfind(\".\", 0, i - 1)\n        if not rv:\n            rv = self.root\n        alogger.parent = rv\n\n    def _fixupChildren(self, ph, alogger):\n        \"\"\"\n        Ensure that children of the placeholder ph are connected to the\n        specified logger.\n        \"\"\"\n        name = alogger.name\n        namelen = len(name)\n        for c in ph.loggerMap.keys():\n            #The if means ... if not c.parent.name.startswith(nm)\n            if c.parent.name[:namelen] != name:\n                alogger.parent = c.parent\n                c.parent = alogger\n\n#---------------------------------------------------------------------------\n#   Logger classes and functions\n#---------------------------------------------------------------------------\n\nclass Logger(Filterer):\n    \"\"\"\n    Instances of the Logger class represent a single logging channel. A\n    \"logging channel\" indicates an area of an application. Exactly how an\n    \"area\" is defined is up to the application developer. Since an\n    application can have any number of areas, logging channels are identified\n    by a unique string. Application areas can be nested (e.g. an area\n    of \"input processing\" might include sub-areas \"read CSV files\", \"read\n    XLS files\" and \"read Gnumeric files\"). To cater for this natural nesting,\n    channel names are organized into a namespace hierarchy where levels are\n    separated by periods, much like the Java or Python package namespace. So\n    in the instance given above, channel names might be \"input\" for the upper\n    level, and \"input.csv\", \"input.xls\" and \"input.gnu\" for the sub-levels.\n    There is no arbitrary limit to the depth of nesting.\n    \"\"\"\n    def __init__(self, name, level=NOTSET):\n        \"\"\"\n        Initialize the logger with a name and an optional level.\n        \"\"\"\n        Filterer.__init__(self)\n        self.name = name\n        self.level = _checkLevel(level)\n        self.parent = None\n        self.propagate = 1\n        self.handlers = []\n        self.disabled = 0\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this logger.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'DEBUG'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.debug(\"Houston, we have a %s\", \"thorny problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(DEBUG):\n            self._log(DEBUG, msg, args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'INFO'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(INFO):\n            self._log(INFO, msg, args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'WARNING'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.warning(\"Houston, we have a %s\", \"bit of a problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(WARNING):\n            self._log(WARNING, msg, args, **kwargs)\n\n    warn = warning\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'ERROR'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.error(\"Houston, we have a %s\", \"major problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(ERROR):\n            self._log(ERROR, msg, args, **kwargs)\n\n    def exception(self, msg, *args, **kwargs):\n        \"\"\"\n        Convenience method for logging an ERROR with exception information.\n        \"\"\"\n        kwargs['exc_info'] = 1\n        self.error(msg, *args, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'CRITICAL'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.critical(\"Houston, we have a %s\", \"major disaster\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(CRITICAL):\n            self._log(CRITICAL, msg, args, **kwargs)\n\n    fatal = critical\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with the integer severity 'level'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.log(level, \"We have a %s\", \"mysterious problem\", exc_info=1)\n        \"\"\"\n        if not isinstance(level, int):\n            if raiseExceptions:\n                raise TypeError(\"level must be an integer\")\n            else:\n                return\n        if self.isEnabledFor(level):\n            self._log(level, msg, args, **kwargs)\n\n    def findCaller(self):\n        \"\"\"\n        Find the stack frame of the caller so that we can note the source\n        file name, line number and function name.\n        \"\"\"\n        f = currentframe()\n        #On some versions of IronPython, currentframe() returns None if\n        #IronPython isn't run with -X:Frames.\n        if f is not None:\n            f = f.f_back\n        rv = \"(unknown file)\", 0, \"(unknown function)\"\n        while hasattr(f, \"f_code\"):\n            co = f.f_code\n            filename = os.path.normcase(co.co_filename)\n            if filename == _srcfile:\n                f = f.f_back\n                continue\n            rv = (co.co_filename, f.f_lineno, co.co_name)\n            break\n        return rv\n\n    def makeRecord(self, name, level, fn, lno, msg, args, exc_info, func=None, extra=None):\n        \"\"\"\n        A factory method which can be overridden in subclasses to create\n        specialized LogRecords.\n        \"\"\"\n        rv = LogRecord(name, level, fn, lno, msg, args, exc_info, func)\n        if extra is not None:\n            for key in extra:\n                if (key in [\"message\", \"asctime\"]) or (key in rv.__dict__):\n                    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\n                rv.__dict__[key] = extra[key]\n        return rv\n\n    def _log(self, level, msg, args, exc_info=None, extra=None):\n        \"\"\"\n        Low-level logging routine which creates a LogRecord and then calls\n        all the handlers of this logger to handle the record.\n        \"\"\"\n        if _srcfile:\n            #IronPython doesn't track Python frames, so findCaller raises an\n            #exception on some versions of IronPython. We trap it here so that\n            #IronPython can use logging.\n            try:\n                fn, lno, func = self.findCaller()\n            except ValueError:\n                fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        else:\n            fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        if exc_info:\n            if not isinstance(exc_info, tuple):\n                exc_info = sys.exc_info()\n        record = self.makeRecord(self.name, level, fn, lno, msg, args, exc_info, func, extra)\n        self.handle(record)\n\n    def handle(self, record):\n        \"\"\"\n        Call the handlers for the specified record.\n\n        This method is used for unpickled records received from a socket, as\n        well as those created locally. Logger-level filtering is applied.\n        \"\"\"\n        if (not self.disabled) and self.filter(record):\n            self.callHandlers(record)\n\n    def addHandler(self, hdlr):\n        \"\"\"\n        Add the specified handler to this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if not (hdlr in self.handlers):\n                self.handlers.append(hdlr)\n        finally:\n            _releaseLock()\n\n    def removeHandler(self, hdlr):\n        \"\"\"\n        Remove the specified handler from this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if hdlr in self.handlers:\n                self.handlers.remove(hdlr)\n        finally:\n            _releaseLock()\n\n    def callHandlers(self, record):\n        \"\"\"\n        Pass a record to all relevant handlers.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. If no handler was found, output a one-off error\n        message to sys.stderr. Stop searching up the hierarchy whenever a\n        logger with the \"propagate\" attribute set to zero is found - that\n        will be the last logger whose handlers are called.\n        \"\"\"\n        c = self\n        found = 0\n        while c:\n            for hdlr in c.handlers:\n                found = found + 1\n                if record.levelno >= hdlr.level:\n                    hdlr.handle(record)\n            if not c.propagate:\n                c = None    #break out\n            else:\n                c = c.parent\n        if (found == 0) and raiseExceptions and not self.manager.emittedNoHandlerWarning:\n            sys.stderr.write(\"No handlers could be found for logger\"\n                             \" \\\"%s\\\"\\n\" % self.name)\n            self.manager.emittedNoHandlerWarning = 1\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for this logger.\n\n        Loop through this logger and its parents in the logger hierarchy,\n        looking for a non-zero logging level. Return the first one found.\n        \"\"\"\n        logger = self\n        while logger:\n            if logger.level:\n                return logger.level\n            logger = logger.parent\n        return NOTSET\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        if self.manager.disable >= level:\n            return 0\n        return level >= self.getEffectiveLevel()\n\n    def getChild(self, suffix):\n        \"\"\"\n        Get a logger which is a descendant to this one.\n\n        This is a convenience method, such that\n\n        logging.getLogger('abc').getChild('def.ghi')\n\n        is the same as\n\n        logging.getLogger('abc.def.ghi')\n\n        It's useful, for example, when the parent logger is named using\n        __name__ rather than a literal string.\n        \"\"\"\n        if self.root is not self:\n            suffix = '.'.join((self.name, suffix))\n        return self.manager.getLogger(suffix)\n\nclass RootLogger(Logger):\n    \"\"\"\n    A root logger is not that different to any other logger, except that\n    it must have a logging level and there is only one instance of it in\n    the hierarchy.\n    \"\"\"\n    def __init__(self, level):\n        \"\"\"\n        Initialize the logger with the name \"root\".\n        \"\"\"\n        Logger.__init__(self, \"root\", level)\n\n_loggerClass = Logger\n\nclass LoggerAdapter(object):\n    \"\"\"\n    An adapter for loggers which makes it easier to specify contextual\n    information in logging output.\n    \"\"\"\n\n    def __init__(self, logger, extra):\n        \"\"\"\n        Initialize the adapter with a logger and a dict-like object which\n        provides contextual information. This constructor signature allows\n        easy stacking of LoggerAdapters, if so desired.\n\n        You can effectively pass keyword arguments as shown in the\n        following example:\n\n        adapter = LoggerAdapter(someLogger, dict(p1=v1, p2=\"v2\"))\n        \"\"\"\n        self.logger = logger\n        self.extra = extra\n\n    def process(self, msg, kwargs):\n        \"\"\"\n        Process the logging message and keyword arguments passed in to\n        a logging call to insert contextual information. You can either\n        manipulate the message itself, the keyword args or both. Return\n        the message and kwargs modified (or not) to suit your needs.\n\n        Normally, you'll only need to override this one method in a\n        LoggerAdapter subclass for your specific needs.\n        \"\"\"\n        kwargs[\"extra\"] = self.extra\n        return msg, kwargs\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a debug call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.debug(msg, *args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an info call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.info(msg, *args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a warning call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an error call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.error(msg, *args, **kwargs)\n\n    def exception(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an exception call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        kwargs[\"exc_info\"] = 1\n        self.logger.error(msg, *args, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a critical call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.critical(msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a log call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        See if the underlying logger is enabled for the specified level.\n        \"\"\"\n        return self.logger.isEnabledFor(level)\n\nroot = RootLogger(WARNING)\nLogger.root = root\nLogger.manager = Manager(Logger.root)\n\n#---------------------------------------------------------------------------\n# Configuration classes and functions\n#---------------------------------------------------------------------------\n\nBASIC_FORMAT = \"%(levelname)s:%(name)s:%(message)s\"\n\ndef basicConfig(**kwargs):\n    \"\"\"\n    Do basic configuration for the logging system.\n\n    This function does nothing if the root logger already has handlers\n    configured. It is a convenience method intended for use by simple scripts\n    to do one-shot configuration of the logging package.\n\n    The default behaviour is to create a StreamHandler which writes to\n    sys.stderr, set a formatter using the BASIC_FORMAT format string, and\n    add the handler to the root logger.\n\n    A number of optional keyword arguments may be specified, which can alter\n    the default behaviour.\n\n    filename  Specifies that a FileHandler be created, using the specified\n              filename, rather than a StreamHandler.\n    filemode  Specifies the mode to open the file, if filename is specified\n              (if filemode is unspecified, it defaults to 'a').\n    format    Use the specified format string for the handler.\n    datefmt   Use the specified date/time format.\n    level     Set the root logger level to the specified level.\n    stream    Use the specified stream to initialize the StreamHandler. Note\n              that this argument is incompatible with 'filename' - if both\n              are present, 'stream' is ignored.\n\n    Note that you could specify a stream created using open(filename, mode)\n    rather than passing the filename and mode in. However, it should be\n    remembered that StreamHandler does not close its stream (since it may be\n    using sys.stdout or sys.stderr), whereas FileHandler closes its stream\n    when the handler is closed.\n    \"\"\"\n    # Add thread safety in case someone mistakenly calls\n    # basicConfig() from multiple threads\n    _acquireLock()\n    try:\n        if len(root.handlers) == 0:\n            filename = kwargs.get(\"filename\")\n            if filename:\n                mode = kwargs.get(\"filemode\", 'a')\n                hdlr = FileHandler(filename, mode)\n            else:\n                stream = kwargs.get(\"stream\")\n                hdlr = StreamHandler(stream)\n            fs = kwargs.get(\"format\", BASIC_FORMAT)\n            dfs = kwargs.get(\"datefmt\", None)\n            fmt = Formatter(fs, dfs)\n            hdlr.setFormatter(fmt)\n            root.addHandler(hdlr)\n            level = kwargs.get(\"level\")\n            if level is not None:\n                root.setLevel(level)\n    finally:\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n# Utility functions at module level.\n# Basically delegate everything to the root logger.\n#---------------------------------------------------------------------------\n\ndef getLogger(name=None):\n    \"\"\"\n    Return a logger with the specified name, creating it if necessary.\n\n    If no name is specified, return the root logger.\n    \"\"\"\n    if name:\n        return Logger.manager.getLogger(name)\n    else:\n        return root\n\n#def getRootLogger():\n#    \"\"\"\n#    Return the root logger.\n#\n#    Note that getLogger('') now does the same thing, so this function is\n#    deprecated and may disappear in the future.\n#    \"\"\"\n#    return root\n\ndef critical(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'CRITICAL' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.critical(msg, *args, **kwargs)\n\nfatal = critical\n\ndef error(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.error(msg, *args, **kwargs)\n\ndef exception(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger,\n    with exception information.\n    \"\"\"\n    kwargs['exc_info'] = 1\n    error(msg, *args, **kwargs)\n\ndef warning(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'WARNING' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.warning(msg, *args, **kwargs)\n\nwarn = warning\n\ndef info(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'INFO' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.info(msg, *args, **kwargs)\n\ndef debug(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'DEBUG' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.debug(msg, *args, **kwargs)\n\ndef log(level, msg, *args, **kwargs):\n    \"\"\"\n    Log 'msg % args' with the integer severity 'level' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.log(level, msg, *args, **kwargs)\n\ndef disable(level):\n    \"\"\"\n    Disable all logging calls of severity 'level' and below.\n    \"\"\"\n    root.manager.disable = level\n\ndef shutdown(handlerList=_handlerList):\n    \"\"\"\n    Perform any cleanup actions in the logging system (e.g. flushing\n    buffers).\n\n    Should be called at application exit.\n    \"\"\"\n    for wr in reversed(handlerList[:]):\n        #errors might occur, for example, if files are locked\n        #we just ignore them if raiseExceptions is not set\n        try:\n            h = wr()\n            if h:\n                try:\n                    h.acquire()\n                    h.flush()\n                    h.close()\n                except (IOError, ValueError):\n                    # Ignore errors which might be caused\n                    # because handlers have been closed but\n                    # references to them are still around at\n                    # application exit.\n                    pass\n                finally:\n                    h.release()\n        except:\n            if raiseExceptions:\n                raise\n            #else, swallow\n\n#Let's try and shutdown automatically on application exit...\nimport atexit\natexit.register(shutdown)\n\n# Null handler\n\nclass NullHandler(Handler):\n    \"\"\"\n    This handler does nothing. It's intended to be used to avoid the\n    \"No handlers could be found for logger XXX\" one-off warning. This is\n    important for library code, which may contain code to log events. If a user\n    of the library does not configure logging, the one-off warning might be\n    produced; to avoid this, the library developer simply needs to instantiate\n    a NullHandler and add it to the top-level logger of the library module or\n    package.\n    \"\"\"\n    def handle(self, record):\n        pass\n\n    def emit(self, record):\n        pass\n\n    def createLock(self):\n        self.lock = None\n\n# Warnings integration\n\n_warnings_showwarning = None\n\ndef _showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"\n    Implementation of showwarnings which redirects to logging, which will first\n    check to see if the file parameter is None. If a file is specified, it will\n    delegate to the original warnings implementation of showwarning. Otherwise,\n    it will call warnings.formatwarning and will log the resulting string to a\n    warnings logger named \"py.warnings\" with level logging.WARNING.\n    \"\"\"\n    if file is not None:\n        if _warnings_showwarning is not None:\n            _warnings_showwarning(message, category, filename, lineno, file, line)\n    else:\n        s = warnings.formatwarning(message, category, filename, lineno, line)\n        logger = getLogger(\"py.warnings\")\n        if not logger.handlers:\n            logger.addHandler(NullHandler())\n        logger.warning(\"%s\", s)\n\ndef captureWarnings(capture):\n    \"\"\"\n    If capture is true, redirect all warnings to the logging package.\n    If capture is False, ensure that warnings are not redirected to logging\n    but to their original destinations.\n    \"\"\"\n    global _warnings_showwarning\n    if capture:\n        if _warnings_showwarning is None:\n            _warnings_showwarning = warnings.showwarning\n            warnings.showwarning = _showwarning\n    else:\n        if _warnings_showwarning is not None:\n            warnings.showwarning = _warnings_showwarning\n            _warnings_showwarning = None\n",
		"file_name": "logging/__init__.py"
	},
	{
		"content": "\"\"\"\natexit.py - allow programmer to define multiple exit functions to be executed\nupon normal program termination.\n\nOne public function, register, is defined.\n\"\"\"\n\n__all__ = [\"register\"]\n\nimport sys\n\n_exithandlers = []\ndef _run_exitfuncs():\n    \"\"\"run any registered exit functions\n\n    _exithandlers is traversed in reverse order so functions are executed\n    last in, first out.\n    \"\"\"\n\n    exc_info = None\n    while _exithandlers:\n        func, targs, kargs = _exithandlers.pop()\n        try:\n            func(*targs, **kargs)\n        except SystemExit:\n            exc_info = sys.exc_info()\n        except:\n            import traceback\n            print >> sys.stderr, \"Error in atexit._run_exitfuncs:\"\n            traceback.print_exc()\n            exc_info = sys.exc_info()\n\n    if exc_info is not None:\n        raise exc_info[0], exc_info[1], exc_info[2]\n\n\ndef register(func, *targs, **kargs):\n    \"\"\"register a function to be executed upon normal program termination\n\n    func - function to be called at exit\n    targs - optional arguments to pass to func\n    kargs - optional keyword arguments to pass to func\n\n    func is returned to facilitate usage as a decorator.\n    \"\"\"\n    _exithandlers.append((func, targs, kargs))\n    return func\n\nif hasattr(sys, \"exitfunc\"):\n    # Assume it's another registered exit function - append it to our list\n    register(sys.exitfunc)\nsys.exitfunc = _run_exitfuncs\n\nif __name__ == \"__main__\":\n    def x1():\n        print \"running x1\"\n    def x2(n):\n        print \"running x2(%r)\" % (n,)\n    def x3(n, kwd=None):\n        print \"running x3(%r, kwd=%r)\" % (n, kwd)\n\n    register(x1)\n    register(x2, 12)\n    register(x3, 5, \"bar\")\n    register(x3, \"no kwd args\")\n",
		"file_name": "atexit.py"
	},
	{
		"content": "#\n# StringIO-based cStringIO implementation.\n#\n\n# Note that PyPy also contains a built-in module 'cStringIO' which will hide\n# this one if compiled in.\n\nfrom StringIO import *\nfrom StringIO import __doc__\n\nclass StringIO(StringIO):\n    def reset(self):\n        \"\"\"\n        reset() -- Reset the file position to the beginning\n        \"\"\"\n        self.seek(0, 0)\n",
		"file_name": "cStringIO.py"
	},
	{
		"content": "__all__ = ['Counter', 'deque', 'defaultdict', 'namedtuple', 'OrderedDict']\n# For bootstrapping reasons, the collection ABCs are defined in _abcoll.py.\n# They should however be considered an integral part of collections.py.\nfrom _abcoll import *\nimport _abcoll\n__all__ += _abcoll.__all__\n\nfrom _collections import deque, defaultdict\nfrom operator import itemgetter as _itemgetter, eq as _eq\nfrom keyword import iskeyword as _iskeyword\nimport sys as _sys\nimport heapq as _heapq\nfrom itertools import repeat as _repeat, chain as _chain, starmap as _starmap\nfrom itertools import imap as _imap\ntry:\n    from __pypy__ import newdict\nexcept ImportError:\n    assert '__pypy__' not in _sys.builtin_module_names\n    newdict = lambda _ : {}\ntry:\n    from __pypy__ import reversed_dict\nexcept ImportError:\n    reversed_dict = lambda d: reversed(d.keys())\n\ntry:\n    from thread import get_ident as _get_ident\nexcept ImportError:\n    from dummy_thread import get_ident as _get_ident\n\n\n################################################################################\n### OrderedDict\n################################################################################\n\nclass OrderedDict(dict):\n    '''Dictionary that remembers insertion order.\n\n    In PyPy all dicts are ordered anyway.  This is mostly useful as a\n    placeholder to mean \"this dict must be ordered even on CPython\".\n\n    Known difference: iterating over an OrderedDict which is being\n    concurrently modified raises RuntimeError in PyPy.  In CPython\n    instead we get some behavior that appears reasonable in some\n    cases but is nonsensical in other cases.  This is officially\n    forbidden by the CPython docs, so we forbid it explicitly for now.\n    '''\n\n    def __reversed__(self):\n        return reversed_dict(self)\n\n    def popitem(self, last=True):\n        '''od.popitem() -> (k, v), return and remove a (key, value) pair.\n        Pairs are returned in LIFO order if last is true or FIFO order if false.\n\n        '''\n        if last:\n            return dict.popitem(self)\n        else:\n            it = dict.__iter__(self)\n            try:\n                k = it.next()\n            except StopIteration:\n                raise KeyError('dictionary is empty')\n            return (k, self.pop(k))\n\n    def __repr__(self, _repr_running={}):\n        'od.__repr__() <==> repr(od)'\n        call_key = id(self), _get_ident()\n        if call_key in _repr_running:\n            return '...'\n        _repr_running[call_key] = 1\n        try:\n            if not self:\n                return '%s()' % (self.__class__.__name__,)\n            return '%s(%r)' % (self.__class__.__name__, self.items())\n        finally:\n            del _repr_running[call_key]\n\n    def __reduce__(self):\n        'Return state information for pickling'\n        items = [[k, self[k]] for k in self]\n        inst_dict = vars(self).copy()\n        if inst_dict:\n            return (self.__class__, (items,), inst_dict)\n        return self.__class__, (items,)\n\n    def copy(self):\n        'od.copy() -> a shallow copy of od'\n        return self.__class__(self)\n\n    def __eq__(self, other):\n        '''od.__eq__(y) <==> od==y.  Comparison to another OD is order-sensitive\n        while comparison to a regular mapping is order-insensitive.\n\n        '''\n        if isinstance(other, OrderedDict):\n            return dict.__eq__(self, other) and all(_imap(_eq, self, other))\n        return dict.__eq__(self, other)\n\n    def __ne__(self, other):\n        'od.__ne__(y) <==> od!=y'\n        return not self == other\n\n    # -- the following methods support python 3.x style dictionary views --\n\n    def viewkeys(self):\n        \"od.viewkeys() -> a set-like object providing a view on od's keys\"\n        return KeysView(self)\n\n    def viewvalues(self):\n        \"od.viewvalues() -> an object providing a view on od's values\"\n        return ValuesView(self)\n\n    def viewitems(self):\n        \"od.viewitems() -> a set-like object providing a view on od's items\"\n        return ItemsView(self)\n\n\n################################################################################\n### namedtuple\n################################################################################\n\n_class_template = '''\\\nclass {typename}(tuple):\n    '{typename}({arg_list})'\n\n    __slots__ = ()\n\n    _fields = {field_names!r}\n\n    def __new__(_cls, {arg_list}):\n        'Create new instance of {typename}({arg_list})'\n        return _tuple.__new__(_cls, ({arg_list}))\n\n    @classmethod\n    def _make(cls, iterable, new=tuple.__new__, len=len):\n        'Make a new {typename} object from a sequence or iterable'\n        result = new(cls, iterable)\n        if len(result) != {num_fields:d}:\n            raise TypeError('Expected {num_fields:d} arguments, got %d' % len(result))\n        return result\n\n    def __repr__(self):\n        'Return a nicely formatted representation string'\n        return '{typename}({repr_fmt})' % self\n\n    def _asdict(self):\n        'Return a new OrderedDict which maps field names to their values'\n        return OrderedDict(zip(self._fields, self))\n\n    def _replace(_self, **kwds):\n        'Return a new {typename} object replacing specified fields with new values'\n        result = _self._make(map(kwds.pop, {field_names!r}, _self))\n        if kwds:\n            raise ValueError('Got unexpected field names: %r' % kwds.keys())\n        return result\n\n    def __getnewargs__(self):\n        'Return self as a plain tuple.  Used by copy and pickle.'\n        return tuple(self)\n\n    __dict__ = _property(_asdict)\n\n    def __getstate__(self):\n        'Exclude the OrderedDict from pickling'\n        pass\n\n{field_defs}\n'''\n\n_repr_template = '{name}=%r'\n\n_field_template = '''\\\n    {name} = _property(lambda self: self[{index:d}], doc='Alias for field number {index:d}')\n'''\n\ndef namedtuple(typename, field_names, verbose=False, rename=False):\n    \"\"\"Returns a new subclass of tuple with named fields.\n\n    >>> Point = namedtuple('Point', ['x', 'y'])\n    >>> Point.__doc__                   # docstring for the new class\n    'Point(x, y)'\n    >>> p = Point(11, y=22)             # instantiate with positional args or keywords\n    >>> p[0] + p[1]                     # indexable like a plain tuple\n    33\n    >>> x, y = p                        # unpack like a regular tuple\n    >>> x, y\n    (11, 22)\n    >>> p.x + p.y                       # fields also accessable by name\n    33\n    >>> d = p._asdict()                 # convert to a dictionary\n    >>> d['x']\n    11\n    >>> Point(**d)                      # convert from a dictionary\n    Point(x=11, y=22)\n    >>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields\n    Point(x=100, y=22)\n\n    \"\"\"\n\n    # Validate the field names.  At the user's option, either generate an error\n    # message or automatically replace the field name with a valid name.\n    if isinstance(field_names, basestring):\n        field_names = field_names.replace(',', ' ').split()\n    field_names = map(str, field_names)\n    typename = str(typename)\n    if rename:\n        seen = set()\n        for index, name in enumerate(field_names):\n            if (not all(c.isalnum() or c=='_' for c in name)\n                or _iskeyword(name)\n                or not name\n                or name[0].isdigit()\n                or name.startswith('_')\n                or name in seen):\n                field_names[index] = '_%d' % index\n            seen.add(name)\n    for name in [typename] + field_names:\n        if type(name) != str:\n            raise TypeError('Type names and field names must be strings')\n        if not all(c.isalnum() or c=='_' for c in name):\n            raise ValueError('Type names and field names can only contain '\n                             'alphanumeric characters and underscores: %r' % name)\n        if _iskeyword(name):\n            raise ValueError('Type names and field names cannot be a '\n                             'keyword: %r' % name)\n        if name[0].isdigit():\n            raise ValueError('Type names and field names cannot start with '\n                             'a number: %r' % name)\n    seen = set()\n    for name in field_names:\n        if name.startswith('_') and not rename:\n            raise ValueError('Field names cannot start with an underscore: '\n                             '%r' % name)\n        if name in seen:\n            raise ValueError('Encountered duplicate field name: %r' % name)\n        seen.add(name)\n\n    # Fill-in the class template\n    class_definition = _class_template.format(\n        typename = typename,\n        field_names = tuple(field_names),\n        num_fields = len(field_names),\n        arg_list = repr(tuple(field_names)).replace(\"'\", \"\")[1:-1],\n        repr_fmt = ', '.join(_repr_template.format(name=name)\n                             for name in field_names),\n        field_defs = '\\n'.join(_field_template.format(index=index, name=name)\n                               for index, name in enumerate(field_names))\n    )\n    if verbose:\n        print class_definition\n\n    # Execute the template string in a temporary namespace and support\n    # tracing utilities by setting a value for frame.f_globals['__name__']\n    namespace = newdict('module')\n    namespace['__name__'] = 'namedtuple_%s' % typename\n    namespace['OrderedDict'] = OrderedDict\n    namespace['_property'] = property\n    namespace['_tuple'] = tuple\n    try:\n        exec class_definition in namespace\n    except SyntaxError as e:\n        raise SyntaxError(e.message + ':\\n' + class_definition)\n    result = namespace[typename]\n\n    # For pickling to work, the __module__ variable needs to be set to the frame\n    # where the named tuple is created.  Bypass this step in environments where\n    # sys._getframe is not defined (Jython for example) or sys._getframe is not\n    # defined for arguments greater than 0 (IronPython).\n    try:\n        result.__module__ = _sys._getframe(1).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):\n        pass\n\n    return result\n\n\n########################################################################\n###  Counter\n########################################################################\n\nclass Counter(dict):\n    '''Dict subclass for counting hashable items.  Sometimes called a bag\n    or multiset.  Elements are stored as dictionary keys and their counts\n    are stored as dictionary values.\n\n    >>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n    >>> c.most_common(3)                # three most common elements\n    [('a', 5), ('b', 4), ('c', 3)]\n    >>> sorted(c)                       # list all unique elements\n    ['a', 'b', 'c', 'd', 'e']\n    >>> ''.join(sorted(c.elements()))   # list elements with repetitions\n    'aaaaabbbbcccdde'\n    >>> sum(c.values())                 # total of all counts\n    15\n\n    >>> c['a']                          # count of letter 'a'\n    5\n    >>> for elem in 'shazam':           # update counts from an iterable\n    ...     c[elem] += 1                # by adding 1 to each element's count\n    >>> c['a']                          # now there are seven 'a'\n    7\n    >>> del c['b']                      # remove all 'b'\n    >>> c['b']                          # now there are zero 'b'\n    0\n\n    >>> d = Counter('simsalabim')       # make another counter\n    >>> c.update(d)                     # add in the second counter\n    >>> c['a']                          # now there are nine 'a'\n    9\n\n    >>> c.clear()                       # empty the counter\n    >>> c\n    Counter()\n\n    Note:  If a count is set to zero or reduced to zero, it will remain\n    in the counter until the entry is deleted or the counter is cleared:\n\n    >>> c = Counter('aaabbc')\n    >>> c['b'] -= 2                     # reduce the count of 'b' by two\n    >>> c.most_common()                 # 'b' is still in, but its count is zero\n    [('a', 3), ('c', 1), ('b', 0)]\n\n    '''\n    # References:\n    #   http://en.wikipedia.org/wiki/Multiset\n    #   http://www.gnu.org/software/smalltalk/manual-base/html_node/Bag.html\n    #   http://www.demo2s.com/Tutorial/Cpp/0380__set-multiset/Catalog0380__set-multiset.htm\n    #   http://code.activestate.com/recipes/259174/\n    #   Knuth, TAOCP Vol. II section 4.6.3\n\n    def __init__(self, iterable=None, **kwds):\n        '''Create a new, empty Counter object.  And if given, count elements\n        from an input iterable.  Or, initialize the count from another mapping\n        of elements to their counts.\n\n        >>> c = Counter()                           # a new, empty counter\n        >>> c = Counter('gallahad')                 # a new counter from an iterable\n        >>> c = Counter({'a': 4, 'b': 2})           # a new counter from a mapping\n        >>> c = Counter(a=4, b=2)                   # a new counter from keyword args\n\n        '''\n        super(Counter, self).__init__()\n        self.update(iterable, **kwds)\n\n    def __missing__(self, key):\n        'The count of elements not in the Counter is zero.'\n        # Needed so that self[missing_item] does not raise KeyError\n        return 0\n\n    def most_common(self, n=None):\n        '''List the n most common elements and their counts from the most\n        common to the least.  If n is None, then list all element counts.\n\n        >>> Counter('abcdeabcdabcaba').most_common(3)\n        [('a', 5), ('b', 4), ('c', 3)]\n\n        '''\n        # Emulate Bag.sortedByCount from Smalltalk\n        if n is None:\n            return sorted(self.iteritems(), key=_itemgetter(1), reverse=True)\n        return _heapq.nlargest(n, self.iteritems(), key=_itemgetter(1))\n\n    def elements(self):\n        '''Iterator over elements repeating each as many times as its count.\n\n        >>> c = Counter('ABCABC')\n        >>> sorted(c.elements())\n        ['A', 'A', 'B', 'B', 'C', 'C']\n\n        # Knuth's example for prime factors of 1836:  2**2 * 3**3 * 17**1\n        >>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n        >>> product = 1\n        >>> for factor in prime_factors.elements():     # loop over factors\n        ...     product *= factor                       # and multiply them\n        >>> product\n        1836\n\n        Note, if an element's count has been set to zero or is a negative\n        number, elements() will ignore it.\n\n        '''\n        # Emulate Bag.do from Smalltalk and Multiset.begin from C++.\n        return _chain.from_iterable(_starmap(_repeat, self.iteritems()))\n\n    # Override dict methods where necessary\n\n    @classmethod\n    def fromkeys(cls, iterable, v=None):\n        # There is no equivalent method for counters because setting v=1\n        # means that no element can have a count greater than one.\n        raise NotImplementedError(\n            'Counter.fromkeys() is undefined.  Use Counter(iterable) instead.')\n\n    def update(self, iterable=None, **kwds):\n        '''Like dict.update() but add counts instead of replacing them.\n\n        Source can be an iterable, a dictionary, or another Counter instance.\n\n        >>> c = Counter('which')\n        >>> c.update('witch')           # add elements from another iterable\n        >>> d = Counter('watch')\n        >>> c.update(d)                 # add elements from another counter\n        >>> c['h']                      # four 'h' in which, witch, and watch\n        4\n\n        '''\n        # The regular dict.update() operation makes no sense here because the\n        # replace behavior results in the some of original untouched counts\n        # being mixed-in with all of the other counts for a mismash that\n        # doesn't have a straight-forward interpretation in most counting\n        # contexts.  Instead, we implement straight-addition.  Both the inputs\n        # and outputs are allowed to contain zero and negative counts.\n\n        if iterable is not None:\n            if isinstance(iterable, Mapping):\n                if self:\n                    self_get = self.get\n                    for elem, count in iterable.iteritems():\n                        self[elem] = self_get(elem, 0) + count\n                else:\n                    super(Counter, self).update(iterable) # fast path when counter is empty\n            else:\n                self_get = self.get\n                for elem in iterable:\n                    self[elem] = self_get(elem, 0) + 1\n        if kwds:\n            self.update(kwds)\n\n    def subtract(self, iterable=None, **kwds):\n        '''Like dict.update() but subtracts counts instead of replacing them.\n        Counts can be reduced below zero.  Both the inputs and outputs are\n        allowed to contain zero and negative counts.\n\n        Source can be an iterable, a dictionary, or another Counter instance.\n\n        >>> c = Counter('which')\n        >>> c.subtract('witch')             # subtract elements from another iterable\n        >>> c.subtract(Counter('watch'))    # subtract elements from another counter\n        >>> c['h']                          # 2 in which, minus 1 in witch, minus 1 in watch\n        0\n        >>> c['w']                          # 1 in which, minus 1 in witch, minus 1 in watch\n        -1\n\n        '''\n        if iterable is not None:\n            self_get = self.get\n            if isinstance(iterable, Mapping):\n                for elem, count in iterable.items():\n                    self[elem] = self_get(elem, 0) - count\n            else:\n                for elem in iterable:\n                    self[elem] = self_get(elem, 0) - 1\n        if kwds:\n            self.subtract(kwds)\n\n    def copy(self):\n        'Return a shallow copy.'\n        return self.__class__(self)\n\n    def __reduce__(self):\n        return self.__class__, (dict(self),)\n\n    def __delitem__(self, elem):\n        'Like dict.__delitem__() but does not raise KeyError for missing values.'\n        if elem in self:\n            super(Counter, self).__delitem__(elem)\n\n    def __repr__(self):\n        if not self:\n            return '%s()' % self.__class__.__name__\n        items = ', '.join(map('%r: %r'.__mod__, self.most_common()))\n        return '%s({%s})' % (self.__class__.__name__, items)\n\n    # Multiset-style mathematical operations discussed in:\n    #       Knuth TAOCP Volume II section 4.6.3 exercise 19\n    #       and at http://en.wikipedia.org/wiki/Multiset\n    #\n    # Outputs guaranteed to only include positive counts.\n    #\n    # To strip negative and zero counts, add-in an empty counter:\n    #       c += Counter()\n\n    def __add__(self, other):\n        '''Add counts from two counters.\n\n        >>> Counter('abbb') + Counter('bcc')\n        Counter({'b': 4, 'c': 2, 'a': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            newcount = count + other[elem]\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count > 0:\n                result[elem] = count\n        return result\n\n    def __sub__(self, other):\n        ''' Subtract count, but keep only results with positive counts.\n\n        >>> Counter('abbbc') - Counter('bccd')\n        Counter({'b': 2, 'a': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            newcount = count - other[elem]\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count < 0:\n                result[elem] = 0 - count\n        return result\n\n    def __or__(self, other):\n        '''Union is the maximum of value in either of the input counters.\n\n        >>> Counter('abbb') | Counter('bcc')\n        Counter({'b': 3, 'c': 2, 'a': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            other_count = other[elem]\n            newcount = other_count if count < other_count else count\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count > 0:\n                result[elem] = count\n        return result\n\n    def __and__(self, other):\n        ''' Intersection is the minimum of corresponding counts.\n\n        >>> Counter('abbb') & Counter('bcc')\n        Counter({'b': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            other_count = other[elem]\n            newcount = count if count < other_count else other_count\n            if newcount > 0:\n                result[elem] = newcount\n        return result\n\n\nif __name__ == '__main__':\n    # verify that instances can be pickled\n    from cPickle import loads, dumps\n    Point = namedtuple('Point', 'x, y', True)\n    p = Point(x=10, y=20)\n    assert p == loads(dumps(p))\n\n    # test and demonstrate ability to override methods\n    class Point(namedtuple('Point', 'x y')):\n        __slots__ = ()\n        @property\n        def hypot(self):\n            return (self.x ** 2 + self.y ** 2) ** 0.5\n        def __str__(self):\n            return 'Point: x=%6.3f  y=%6.3f  hypot=%6.3f' % (self.x, self.y, self.hypot)\n\n    for p in Point(3, 4), Point(14, 5/7.):\n        print p\n\n    class Point(namedtuple('Point', 'x y')):\n        'Point class with optimized _make() and _replace() without error-checking'\n        __slots__ = ()\n        _make = classmethod(tuple.__new__)\n        def _replace(self, _map=map, **kwds):\n            return self._make(_map(kwds.get, ('x', 'y'), self))\n\n    print Point(11, 22)._replace(x=100)\n\n    Point3D = namedtuple('Point3D', Point._fields + ('z',))\n    print Point3D.__doc__\n\n    import doctest\n    TestResults = namedtuple('TestResults', 'failed attempted')\n    print TestResults(*doctest.testmod())\n",
		"file_name": "collections.py"
	},
	{
		"content": "# Module doctest.\n# Released to the public domain 16-Jan-2001, by Tim Peters (tim@python.org).\n# Major enhancements and refactoring by:\n#     Jim Fulton\n#     Edward Loper\n\n# Provided as-is; use at your own risk; no warranty; no promises; enjoy!\n\nr\"\"\"Module doctest -- a framework for running examples in docstrings.\n\nIn simplest use, end each module M to be tested with:\n\ndef _test():\n    import doctest\n    doctest.testmod()\n\nif __name__ == \"__main__\":\n    _test()\n\nThen running the module as a script will cause the examples in the\ndocstrings to get executed and verified:\n\npython M.py\n\nThis won't display anything unless an example fails, in which case the\nfailing example(s) and the cause(s) of the failure(s) are printed to stdout\n(why not stderr? because stderr is a lame hack <0.2 wink>), and the final\nline of output is \"Test failed.\".\n\nRun it with the -v switch instead:\n\npython M.py -v\n\nand a detailed report of all examples tried is printed to stdout, along\nwith assorted summaries at the end.\n\nYou can force verbose mode by passing \"verbose=True\" to testmod, or prohibit\nit by passing \"verbose=False\".  In either of those cases, sys.argv is not\nexamined by testmod.\n\nThere are a variety of other ways to run doctests, including integration\nwith the unittest framework, and support for running non-Python text\nfiles containing doctests.  There are also many ways to override parts\nof doctest's default behaviors.  See the Library Reference Manual for\ndetails.\n\"\"\"\n\n__docformat__ = 'reStructuredText en'\n\n__all__ = [\n    # 0, Option Flags\n    'register_optionflag',\n    'DONT_ACCEPT_TRUE_FOR_1',\n    'DONT_ACCEPT_BLANKLINE',\n    'NORMALIZE_WHITESPACE',\n    'ELLIPSIS',\n    'SKIP',\n    'IGNORE_EXCEPTION_DETAIL',\n    'COMPARISON_FLAGS',\n    'REPORT_UDIFF',\n    'REPORT_CDIFF',\n    'REPORT_NDIFF',\n    'REPORT_ONLY_FIRST_FAILURE',\n    'REPORTING_FLAGS',\n    # 1. Utility Functions\n    # 2. Example & DocTest\n    'Example',\n    'DocTest',\n    # 3. Doctest Parser\n    'DocTestParser',\n    # 4. Doctest Finder\n    'DocTestFinder',\n    # 5. Doctest Runner\n    'DocTestRunner',\n    'OutputChecker',\n    'DocTestFailure',\n    'UnexpectedException',\n    'DebugRunner',\n    # 6. Test Functions\n    'testmod',\n    'testfile',\n    'run_docstring_examples',\n    # 7. Tester\n    'Tester',\n    # 8. Unittest Support\n    'DocTestSuite',\n    'DocFileSuite',\n    'set_unittest_reportflags',\n    # 9. Debugging Support\n    'script_from_examples',\n    'testsource',\n    'debug_src',\n    'debug',\n]\n\nimport __future__\n\nimport sys, traceback, inspect, linecache, os, re\nimport unittest, difflib, pdb, tempfile\nimport warnings\nfrom StringIO import StringIO\nfrom collections import namedtuple\n\nTestResults = namedtuple('TestResults', 'failed attempted')\n\n# There are 4 basic classes:\n#  - Example: a <source, want> pair, plus an intra-docstring line number.\n#  - DocTest: a collection of examples, parsed from a docstring, plus\n#    info about where the docstring came from (name, filename, lineno).\n#  - DocTestFinder: extracts DocTests from a given object's docstring and\n#    its contained objects' docstrings.\n#  - DocTestRunner: runs DocTest cases, and accumulates statistics.\n#\n# So the basic picture is:\n#\n#                             list of:\n# +------+                   +---------+                   +-------+\n# |object| --DocTestFinder-> | DocTest | --DocTestRunner-> |results|\n# +------+                   +---------+                   +-------+\n#                            | Example |\n#                            |   ...   |\n#                            | Example |\n#                            +---------+\n\n# Option constants.\n\nOPTIONFLAGS_BY_NAME = {}\ndef register_optionflag(name):\n    # Create a new flag unless `name` is already known.\n    return OPTIONFLAGS_BY_NAME.setdefault(name, 1 << len(OPTIONFLAGS_BY_NAME))\n\nDONT_ACCEPT_TRUE_FOR_1 = register_optionflag('DONT_ACCEPT_TRUE_FOR_1')\nDONT_ACCEPT_BLANKLINE = register_optionflag('DONT_ACCEPT_BLANKLINE')\nNORMALIZE_WHITESPACE = register_optionflag('NORMALIZE_WHITESPACE')\nELLIPSIS = register_optionflag('ELLIPSIS')\nSKIP = register_optionflag('SKIP')\nIGNORE_EXCEPTION_DETAIL = register_optionflag('IGNORE_EXCEPTION_DETAIL')\n\nCOMPARISON_FLAGS = (DONT_ACCEPT_TRUE_FOR_1 |\n                    DONT_ACCEPT_BLANKLINE |\n                    NORMALIZE_WHITESPACE |\n                    ELLIPSIS |\n                    SKIP |\n                    IGNORE_EXCEPTION_DETAIL)\n\nREPORT_UDIFF = register_optionflag('REPORT_UDIFF')\nREPORT_CDIFF = register_optionflag('REPORT_CDIFF')\nREPORT_NDIFF = register_optionflag('REPORT_NDIFF')\nREPORT_ONLY_FIRST_FAILURE = register_optionflag('REPORT_ONLY_FIRST_FAILURE')\n\nREPORTING_FLAGS = (REPORT_UDIFF |\n                   REPORT_CDIFF |\n                   REPORT_NDIFF |\n                   REPORT_ONLY_FIRST_FAILURE)\n\n# Special string markers for use in `want` strings:\nBLANKLINE_MARKER = '<BLANKLINE>'\nELLIPSIS_MARKER = '...'\n\n######################################################################\n## Table of Contents\n######################################################################\n#  1. Utility Functions\n#  2. Example & DocTest -- store test cases\n#  3. DocTest Parser -- extracts examples from strings\n#  4. DocTest Finder -- extracts test cases from objects\n#  5. DocTest Runner -- runs test cases\n#  6. Test Functions -- convenient wrappers for testing\n#  7. Tester Class -- for backwards compatibility\n#  8. Unittest Support\n#  9. Debugging Support\n# 10. Example Usage\n\n######################################################################\n## 1. Utility Functions\n######################################################################\n\ndef _extract_future_flags(globs):\n    \"\"\"\n    Return the compiler-flags associated with the future features that\n    have been imported into the given namespace (globs).\n    \"\"\"\n    flags = 0\n    for fname in __future__.all_feature_names:\n        feature = globs.get(fname, None)\n        if feature is getattr(__future__, fname):\n            flags |= feature.compiler_flag\n    return flags\n\ndef _normalize_module(module, depth=2):\n    \"\"\"\n    Return the module specified by `module`.  In particular:\n      - If `module` is a module, then return module.\n      - If `module` is a string, then import and return the\n        module with that name.\n      - If `module` is None, then return the calling module.\n        The calling module is assumed to be the module of\n        the stack frame at the given depth in the call stack.\n    \"\"\"\n    if inspect.ismodule(module):\n        return module\n    elif isinstance(module, (str, unicode)):\n        return __import__(module, globals(), locals(), [\"*\"])\n    elif module is None:\n        return sys.modules[sys._getframe(depth).f_globals['__name__']]\n    else:\n        raise TypeError(\"Expected a module, string, or None\")\n\ndef _load_testfile(filename, package, module_relative):\n    if module_relative:\n        package = _normalize_module(package, 3)\n        filename = _module_relative_path(package, filename)\n        if hasattr(package, '__loader__'):\n            if hasattr(package.__loader__, 'get_data'):\n                file_contents = package.__loader__.get_data(filename)\n                # get_data() opens files as 'rb', so one must do the equivalent\n                # conversion as universal newlines would do.\n                return file_contents.replace(os.linesep, '\\n'), filename\n    with open(filename) as f:\n        return f.read(), filename\n\n# Use sys.stdout encoding for ouput.\n_encoding = getattr(sys.__stdout__, 'encoding', None) or 'utf-8'\n\ndef _indent(s, indent=4):\n    \"\"\"\n    Add the given number of space characters to the beginning of\n    every non-blank line in `s`, and return the result.\n    If the string `s` is Unicode, it is encoded using the stdout\n    encoding and the `backslashreplace` error handler.\n    \"\"\"\n    if isinstance(s, unicode):\n        s = s.encode(_encoding, 'backslashreplace')\n    # This regexp matches the start of non-blank lines:\n    return re.sub('(?m)^(?!$)', indent*' ', s)\n\ndef _exception_traceback(exc_info):\n    \"\"\"\n    Return a string containing a traceback message for the given\n    exc_info tuple (as returned by sys.exc_info()).\n    \"\"\"\n    # Get a traceback message.\n    excout = StringIO()\n    exc_type, exc_val, exc_tb = exc_info\n    traceback.print_exception(exc_type, exc_val, exc_tb, file=excout)\n    return excout.getvalue()\n\n# Override some StringIO methods.\nclass _SpoofOut(StringIO):\n    def getvalue(self):\n        result = StringIO.getvalue(self)\n        # If anything at all was written, make sure there's a trailing\n        # newline.  There's no way for the expected output to indicate\n        # that a trailing newline is missing.\n        if result and not result.endswith(\"\\n\"):\n            result += \"\\n\"\n        # Prevent softspace from screwing up the next test case, in\n        # case they used print with a trailing comma in an example.\n        if hasattr(self, \"softspace\"):\n            del self.softspace\n        return result\n\n    def truncate(self,   size=None):\n        StringIO.truncate(self, size)\n        if hasattr(self, \"softspace\"):\n            del self.softspace\n        if not self.buf:\n            # Reset it to an empty string, to make sure it's not unicode.\n            self.buf = ''\n\n# Worst-case linear-time ellipsis matching.\ndef _ellipsis_match(want, got):\n    \"\"\"\n    Essentially the only subtle case:\n    >>> _ellipsis_match('aa...aa', 'aaa')\n    False\n    \"\"\"\n    if ELLIPSIS_MARKER not in want:\n        return want == got\n\n    # Find \"the real\" strings.\n    ws = want.split(ELLIPSIS_MARKER)\n    assert len(ws) >= 2\n\n    # Deal with exact matches possibly needed at one or both ends.\n    startpos, endpos = 0, len(got)\n    w = ws[0]\n    if w:   # starts with exact match\n        if got.startswith(w):\n            startpos = len(w)\n            del ws[0]\n        else:\n            return False\n    w = ws[-1]\n    if w:   # ends with exact match\n        if got.endswith(w):\n            endpos -= len(w)\n            del ws[-1]\n        else:\n            return False\n\n    if startpos > endpos:\n        # Exact end matches required more characters than we have, as in\n        # _ellipsis_match('aa...aa', 'aaa')\n        return False\n\n    # For the rest, we only need to find the leftmost non-overlapping\n    # match for each piece.  If there's no overall match that way alone,\n    # there's no overall match period.\n    for w in ws:\n        # w may be '' at times, if there are consecutive ellipses, or\n        # due to an ellipsis at the start or end of `want`.  That's OK.\n        # Search for an empty string succeeds, and doesn't change startpos.\n        startpos = got.find(w, startpos, endpos)\n        if startpos < 0:\n            return False\n        startpos += len(w)\n\n    return True\n\ndef _comment_line(line):\n    \"Return a commented form of the given line\"\n    line = line.rstrip()\n    if line:\n        return '# '+line\n    else:\n        return '#'\n\ndef _strip_exception_details(msg):\n    # Support for IGNORE_EXCEPTION_DETAIL.\n    # Get rid of everything except the exception name; in particular, drop\n    # the possibly dotted module path (if any) and the exception message (if\n    # any).  We assume that a colon is never part of a dotted name, or of an\n    # exception name.\n    # E.g., given\n    #    \"foo.bar.MyError: la di da\"\n    # return \"MyError\"\n    # Or for \"abc.def\" or \"abc.def:\\n\" return \"def\".\n\n    start, end = 0, len(msg)\n    # The exception name must appear on the first line.\n    i = msg.find(\"\\n\")\n    if i >= 0:\n        end = i\n    # retain up to the first colon (if any)\n    i = msg.find(':', 0, end)\n    if i >= 0:\n        end = i\n    # retain just the exception name\n    i = msg.rfind('.', 0, end)\n    if i >= 0:\n        start = i+1\n    return msg[start: end]\n\nclass _OutputRedirectingPdb(pdb.Pdb):\n    \"\"\"\n    A specialized version of the python debugger that redirects stdout\n    to a given stream when interacting with the user.  Stdout is *not*\n    redirected when traced code is executed.\n    \"\"\"\n    def __init__(self, out):\n        self.__out = out\n        self.__debugger_used = False\n        pdb.Pdb.__init__(self, stdout=out)\n        # still use input() to get user input\n        self.use_rawinput = 1\n\n    def set_trace(self, frame=None):\n        self.__debugger_used = True\n        if frame is None:\n            frame = sys._getframe().f_back\n        pdb.Pdb.set_trace(self, frame)\n\n    def set_continue(self):\n        # Calling set_continue unconditionally would break unit test\n        # coverage reporting, as Bdb.set_continue calls sys.settrace(None).\n        if self.__debugger_used:\n            pdb.Pdb.set_continue(self)\n\n    def trace_dispatch(self, *args):\n        # Redirect stdout to the given stream.\n        save_stdout = sys.stdout\n        sys.stdout = self.__out\n        # Call Pdb's trace dispatch method.\n        try:\n            return pdb.Pdb.trace_dispatch(self, *args)\n        finally:\n            sys.stdout = save_stdout\n\n# [XX] Normalize with respect to os.path.pardir?\ndef _module_relative_path(module, path):\n    if not inspect.ismodule(module):\n        raise TypeError, 'Expected a module: %r' % module\n    if path.startswith('/'):\n        raise ValueError, 'Module-relative files may not have absolute paths'\n\n    # Find the base directory for the path.\n    if hasattr(module, '__file__'):\n        # A normal module/package\n        basedir = os.path.split(module.__file__)[0]\n    elif module.__name__ == '__main__':\n        # An interactive session.\n        if len(sys.argv)>0 and sys.argv[0] != '':\n            basedir = os.path.split(sys.argv[0])[0]\n        else:\n            basedir = os.curdir\n    else:\n        # A module w/o __file__ (this includes builtins)\n        raise ValueError(\"Can't resolve paths relative to the module \" +\n                         module + \" (it has no __file__)\")\n\n    # Combine the base directory and the path.\n    return os.path.join(basedir, *(path.split('/')))\n\n######################################################################\n## 2. Example & DocTest\n######################################################################\n## - An \"example\" is a <source, want> pair, where \"source\" is a\n##   fragment of source code, and \"want\" is the expected output for\n##   \"source.\"  The Example class also includes information about\n##   where the example was extracted from.\n##\n## - A \"doctest\" is a collection of examples, typically extracted from\n##   a string (such as an object's docstring).  The DocTest class also\n##   includes information about where the string was extracted from.\n\nclass Example:\n    \"\"\"\n    A single doctest example, consisting of source code and expected\n    output.  `Example` defines the following attributes:\n\n      - source: A single Python statement, always ending with a newline.\n        The constructor adds a newline if needed.\n\n      - want: The expected output from running the source code (either\n        from stdout, or a traceback in case of exception).  `want` ends\n        with a newline unless it's empty, in which case it's an empty\n        string.  The constructor adds a newline if needed.\n\n      - exc_msg: The exception message generated by the example, if\n        the example is expected to generate an exception; or `None` if\n        it is not expected to generate an exception.  This exception\n        message is compared against the return value of\n        `traceback.format_exception_only()`.  `exc_msg` ends with a\n        newline unless it's `None`.  The constructor adds a newline\n        if needed.\n\n      - lineno: The line number within the DocTest string containing\n        this Example where the Example begins.  This line number is\n        zero-based, with respect to the beginning of the DocTest.\n\n      - indent: The example's indentation in the DocTest string.\n        I.e., the number of space characters that precede the\n        example's first prompt.\n\n      - options: A dictionary mapping from option flags to True or\n        False, which is used to override default options for this\n        example.  Any option flags not contained in this dictionary\n        are left at their default value (as specified by the\n        DocTestRunner's optionflags).  By default, no options are set.\n    \"\"\"\n    def __init__(self, source, want, exc_msg=None, lineno=0, indent=0,\n                 options=None):\n        # Normalize inputs.\n        if not source.endswith('\\n'):\n            source += '\\n'\n        if want and not want.endswith('\\n'):\n            want += '\\n'\n        if exc_msg is not None and not exc_msg.endswith('\\n'):\n            exc_msg += '\\n'\n        # Store properties.\n        self.source = source\n        self.want = want\n        self.lineno = lineno\n        self.indent = indent\n        if options is None: options = {}\n        self.options = options\n        self.exc_msg = exc_msg\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self.source == other.source and \\\n               self.want == other.want and \\\n               self.lineno == other.lineno and \\\n               self.indent == other.indent and \\\n               self.options == other.options and \\\n               self.exc_msg == other.exc_msg\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self.source, self.want, self.lineno, self.indent,\n                     self.exc_msg))\n\n\nclass DocTest:\n    \"\"\"\n    A collection of doctest examples that should be run in a single\n    namespace.  Each `DocTest` defines the following attributes:\n\n      - examples: the list of examples.\n\n      - globs: The namespace (aka globals) that the examples should\n        be run in.\n\n      - name: A name identifying the DocTest (typically, the name of\n        the object whose docstring this DocTest was extracted from).\n\n      - filename: The name of the file that this DocTest was extracted\n        from, or `None` if the filename is unknown.\n\n      - lineno: The line number within filename where this DocTest\n        begins, or `None` if the line number is unavailable.  This\n        line number is zero-based, with respect to the beginning of\n        the file.\n\n      - docstring: The string that the examples were extracted from,\n        or `None` if the string is unavailable.\n    \"\"\"\n    def __init__(self, examples, globs, name, filename, lineno, docstring):\n        \"\"\"\n        Create a new DocTest containing the given examples.  The\n        DocTest's globals are initialized with a copy of `globs`.\n        \"\"\"\n        assert not isinstance(examples, basestring), \\\n               \"DocTest no longer accepts str; use DocTestParser instead\"\n        self.examples = examples\n        self.docstring = docstring\n        self.globs = globs.copy()\n        self.name = name\n        self.filename = filename\n        self.lineno = lineno\n\n    def __repr__(self):\n        if len(self.examples) == 0:\n            examples = 'no examples'\n        elif len(self.examples) == 1:\n            examples = '1 example'\n        else:\n            examples = '%d examples' % len(self.examples)\n        return ('<DocTest %s from %s:%s (%s)>' %\n                (self.name, self.filename, self.lineno, examples))\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self.examples == other.examples and \\\n               self.docstring == other.docstring and \\\n               self.globs == other.globs and \\\n               self.name == other.name and \\\n               self.filename == other.filename and \\\n               self.lineno == other.lineno\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self.docstring, self.name, self.filename, self.lineno))\n\n    # This lets us sort tests by name:\n    def __cmp__(self, other):\n        if not isinstance(other, DocTest):\n            return -1\n        return cmp((self.name, self.filename, self.lineno, id(self)),\n                   (other.name, other.filename, other.lineno, id(other)))\n\n######################################################################\n## 3. DocTestParser\n######################################################################\n\nclass DocTestParser:\n    \"\"\"\n    A class used to parse strings containing doctest examples.\n    \"\"\"\n    # This regular expression is used to find doctest examples in a\n    # string.  It defines three groups: `source` is the source code\n    # (including leading indentation and prompts); `indent` is the\n    # indentation of the first (PS1) line of the source code; and\n    # `want` is the expected output (including leading indentation).\n    _EXAMPLE_RE = re.compile(r'''\n        # Source consists of a PS1 line followed by zero or more PS2 lines.\n        (?P<source>\n            (?:^(?P<indent> [ ]*) >>>    .*)    # PS1 line\n            (?:\\n           [ ]*  \\.\\.\\. .*)*)  # PS2 lines\n        \\n?\n        # Want consists of any non-blank lines that do not start with PS1.\n        (?P<want> (?:(?![ ]*$)    # Not a blank line\n                     (?![ ]*>>>)  # Not a line starting with PS1\n                     .+$\\n?       # But any other line\n                  )*)\n        ''', re.MULTILINE | re.VERBOSE)\n\n    # A regular expression for handling `want` strings that contain\n    # expected exceptions.  It divides `want` into three pieces:\n    #    - the traceback header line (`hdr`)\n    #    - the traceback stack (`stack`)\n    #    - the exception message (`msg`), as generated by\n    #      traceback.format_exception_only()\n    # `msg` may have multiple lines.  We assume/require that the\n    # exception message is the first non-indented line starting with a word\n    # character following the traceback header line.\n    _EXCEPTION_RE = re.compile(r\"\"\"\n        # Grab the traceback header.  Different versions of Python have\n        # said different things on the first traceback line.\n        ^(?P<hdr> Traceback\\ \\(\n            (?: most\\ recent\\ call\\ last\n            |   innermost\\ last\n            ) \\) :\n        )\n        \\s* $                # toss trailing whitespace on the header.\n        (?P<stack> .*?)      # don't blink: absorb stuff until...\n        ^ (?P<msg> \\w+ .*)   #     a line *starts* with alphanum.\n        \"\"\", re.VERBOSE | re.MULTILINE | re.DOTALL)\n\n    # A callable returning a true value iff its argument is a blank line\n    # or contains a single comment.\n    _IS_BLANK_OR_COMMENT = re.compile(r'^[ ]*(#.*)?$').match\n\n    def parse(self, string, name='<string>'):\n        \"\"\"\n        Divide the given string into examples and intervening text,\n        and return them as a list of alternating Examples and strings.\n        Line numbers for the Examples are 0-based.  The optional\n        argument `name` is a name identifying this string, and is only\n        used for error messages.\n        \"\"\"\n        string = string.expandtabs()\n        # If all lines begin with the same indentation, then strip it.\n        min_indent = self._min_indent(string)\n        if min_indent > 0:\n            string = '\\n'.join([l[min_indent:] for l in string.split('\\n')])\n\n        output = []\n        charno, lineno = 0, 0\n        # Find all doctest examples in the string:\n        for m in self._EXAMPLE_RE.finditer(string):\n            # Add the pre-example text to `output`.\n            output.append(string[charno:m.start()])\n            # Update lineno (lines before this example)\n            lineno += string.count('\\n', charno, m.start())\n            # Extract info from the regexp match.\n            (source, options, want, exc_msg) = \\\n                     self._parse_example(m, name, lineno)\n            # Create an Example, and add it to the list.\n            if not self._IS_BLANK_OR_COMMENT(source):\n                output.append( Example(source, want, exc_msg,\n                                    lineno=lineno,\n                                    indent=min_indent+len(m.group('indent')),\n                                    options=options) )\n            # Update lineno (lines inside this example)\n            lineno += string.count('\\n', m.start(), m.end())\n            # Update charno.\n            charno = m.end()\n        # Add any remaining post-example text to `output`.\n        output.append(string[charno:])\n        return output\n\n    def get_doctest(self, string, globs, name, filename, lineno):\n        \"\"\"\n        Extract all doctest examples from the given string, and\n        collect them into a `DocTest` object.\n\n        `globs`, `name`, `filename`, and `lineno` are attributes for\n        the new `DocTest` object.  See the documentation for `DocTest`\n        for more information.\n        \"\"\"\n        return DocTest(self.get_examples(string, name), globs,\n                       name, filename, lineno, string)\n\n    def get_examples(self, string, name='<string>'):\n        \"\"\"\n        Extract all doctest examples from the given string, and return\n        them as a list of `Example` objects.  Line numbers are\n        0-based, because it's most common in doctests that nothing\n        interesting appears on the same line as opening triple-quote,\n        and so the first interesting line is called \\\"line 1\\\" then.\n\n        The optional argument `name` is a name identifying this\n        string, and is only used for error messages.\n        \"\"\"\n        return [x for x in self.parse(string, name)\n                if isinstance(x, Example)]\n\n    def _parse_example(self, m, name, lineno):\n        \"\"\"\n        Given a regular expression match from `_EXAMPLE_RE` (`m`),\n        return a pair `(source, want)`, where `source` is the matched\n        example's source code (with prompts and indentation stripped);\n        and `want` is the example's expected output (with indentation\n        stripped).\n\n        `name` is the string's name, and `lineno` is the line number\n        where the example starts; both are used for error messages.\n        \"\"\"\n        # Get the example's indentation level.\n        indent = len(m.group('indent'))\n\n        # Divide source into lines; check that they're properly\n        # indented; and then strip their indentation & prompts.\n        source_lines = m.group('source').split('\\n')\n        self._check_prompt_blank(source_lines, indent, name, lineno)\n        self._check_prefix(source_lines[1:], ' '*indent + '.', name, lineno)\n        source = '\\n'.join([sl[indent+4:] for sl in source_lines])\n\n        # Divide want into lines; check that it's properly indented; and\n        # then strip the indentation.  Spaces before the last newline should\n        # be preserved, so plain rstrip() isn't good enough.\n        want = m.group('want')\n        want_lines = want.split('\\n')\n        if len(want_lines) > 1 and re.match(r' *$', want_lines[-1]):\n            del want_lines[-1]  # forget final newline & spaces after it\n        self._check_prefix(want_lines, ' '*indent, name,\n                           lineno + len(source_lines))\n        want = '\\n'.join([wl[indent:] for wl in want_lines])\n\n        # If `want` contains a traceback message, then extract it.\n        m = self._EXCEPTION_RE.match(want)\n        if m:\n            exc_msg = m.group('msg')\n        else:\n            exc_msg = None\n\n        # Extract options from the source.\n        options = self._find_options(source, name, lineno)\n\n        return source, options, want, exc_msg\n\n    # This regular expression looks for option directives in the\n    # source code of an example.  Option directives are comments\n    # starting with \"doctest:\".  Warning: this may give false\n    # positives for string-literals that contain the string\n    # \"#doctest:\".  Eliminating these false positives would require\n    # actually parsing the string; but we limit them by ignoring any\n    # line containing \"#doctest:\" that is *followed* by a quote mark.\n    _OPTION_DIRECTIVE_RE = re.compile(r'#\\s*doctest:\\s*([^\\n\\'\"]*)$',\n                                      re.MULTILINE)\n\n    def _find_options(self, source, name, lineno):\n        \"\"\"\n        Return a dictionary containing option overrides extracted from\n        option directives in the given source string.\n\n        `name` is the string's name, and `lineno` is the line number\n        where the example starts; both are used for error messages.\n        \"\"\"\n        options = {}\n        # (note: with the current regexp, this will match at most once:)\n        for m in self._OPTION_DIRECTIVE_RE.finditer(source):\n            option_strings = m.group(1).replace(',', ' ').split()\n            for option in option_strings:\n                if (option[0] not in '+-' or\n                    option[1:] not in OPTIONFLAGS_BY_NAME):\n                    raise ValueError('line %r of the doctest for %s '\n                                     'has an invalid option: %r' %\n                                     (lineno+1, name, option))\n                flag = OPTIONFLAGS_BY_NAME[option[1:]]\n                options[flag] = (option[0] == '+')\n        if options and self._IS_BLANK_OR_COMMENT(source):\n            raise ValueError('line %r of the doctest for %s has an option '\n                             'directive on a line with no example: %r' %\n                             (lineno, name, source))\n        return options\n\n    # This regular expression finds the indentation of every non-blank\n    # line in a string.\n    _INDENT_RE = re.compile('^([ ]*)(?=\\S)', re.MULTILINE)\n\n    def _min_indent(self, s):\n        \"Return the minimum indentation of any non-blank line in `s`\"\n        indents = [len(indent) for indent in self._INDENT_RE.findall(s)]\n        if len(indents) > 0:\n            return min(indents)\n        else:\n            return 0\n\n    def _check_prompt_blank(self, lines, indent, name, lineno):\n        \"\"\"\n        Given the lines of a source string (including prompts and\n        leading indentation), check to make sure that every prompt is\n        followed by a space character.  If any line is not followed by\n        a space character, then raise ValueError.\n        \"\"\"\n        for i, line in enumerate(lines):\n            if len(line) >= indent+4 and line[indent+3] != ' ':\n                raise ValueError('line %r of the docstring for %s '\n                                 'lacks blank after %s: %r' %\n                                 (lineno+i+1, name,\n                                  line[indent:indent+3], line))\n\n    def _check_prefix(self, lines, prefix, name, lineno):\n        \"\"\"\n        Check that every line in the given list starts with the given\n        prefix; if any line does not, then raise a ValueError.\n        \"\"\"\n        for i, line in enumerate(lines):\n            if line and not line.startswith(prefix):\n                raise ValueError('line %r of the docstring for %s has '\n                                 'inconsistent leading whitespace: %r' %\n                                 (lineno+i+1, name, line))\n\n\n######################################################################\n## 4. DocTest Finder\n######################################################################\n\nclass DocTestFinder:\n    \"\"\"\n    A class used to extract the DocTests that are relevant to a given\n    object, from its docstring and the docstrings of its contained\n    objects.  Doctests can currently be extracted from the following\n    object types: modules, functions, classes, methods, staticmethods,\n    classmethods, and properties.\n    \"\"\"\n\n    def __init__(self, verbose=False, parser=DocTestParser(),\n                 recurse=True, exclude_empty=True):\n        \"\"\"\n        Create a new doctest finder.\n\n        The optional argument `parser` specifies a class or\n        function that should be used to create new DocTest objects (or\n        objects that implement the same interface as DocTest).  The\n        signature for this factory function should match the signature\n        of the DocTest constructor.\n\n        If the optional argument `recurse` is false, then `find` will\n        only examine the given object, and not any contained objects.\n\n        If the optional argument `exclude_empty` is false, then `find`\n        will include tests for objects with empty docstrings.\n        \"\"\"\n        self._parser = parser\n        self._verbose = verbose\n        self._recurse = recurse\n        self._exclude_empty = exclude_empty\n\n    def find(self, obj, name=None, module=None, globs=None, extraglobs=None):\n        \"\"\"\n        Return a list of the DocTests that are defined by the given\n        object's docstring, or by any of its contained objects'\n        docstrings.\n\n        The optional parameter `module` is the module that contains\n        the given object.  If the module is not specified or is None, then\n        the test finder will attempt to automatically determine the\n        correct module.  The object's module is used:\n\n            - As a default namespace, if `globs` is not specified.\n            - To prevent the DocTestFinder from extracting DocTests\n              from objects that are imported from other modules.\n            - To find the name of the file containing the object.\n            - To help find the line number of the object within its\n              file.\n\n        Contained objects whose module does not match `module` are ignored.\n\n        If `module` is False, no attempt to find the module will be made.\n        This is obscure, of use mostly in tests:  if `module` is False, or\n        is None but cannot be found automatically, then all objects are\n        considered to belong to the (non-existent) module, so all contained\n        objects will (recursively) be searched for doctests.\n\n        The globals for each DocTest is formed by combining `globs`\n        and `extraglobs` (bindings in `extraglobs` override bindings\n        in `globs`).  A new copy of the globals dictionary is created\n        for each DocTest.  If `globs` is not specified, then it\n        defaults to the module's `__dict__`, if specified, or {}\n        otherwise.  If `extraglobs` is not specified, then it defaults\n        to {}.\n\n        \"\"\"\n        # If name was not specified, then extract it from the object.\n        if name is None:\n            name = getattr(obj, '__name__', None)\n            if name is None:\n                raise ValueError(\"DocTestFinder.find: name must be given \"\n                        \"when obj.__name__ doesn't exist: %r\" %\n                                 (type(obj),))\n\n        # Find the module that contains the given object (if obj is\n        # a module, then module=obj.).  Note: this may fail, in which\n        # case module will be None.\n        if module is False:\n            module = None\n        elif module is None:\n            module = inspect.getmodule(obj)\n\n        # Read the module's source code.  This is used by\n        # DocTestFinder._find_lineno to find the line number for a\n        # given object's docstring.\n        try:\n            file = inspect.getsourcefile(obj) or inspect.getfile(obj)\n            if module is not None:\n                # Supply the module globals in case the module was\n                # originally loaded via a PEP 302 loader and\n                # file is not a valid filesystem path\n                source_lines = linecache.getlines(file, module.__dict__)\n            else:\n                # No access to a loader, so assume it's a normal\n                # filesystem path\n                source_lines = linecache.getlines(file)\n            if not source_lines:\n                source_lines = None\n        except TypeError:\n            source_lines = None\n\n        # Initialize globals, and merge in extraglobs.\n        if globs is None:\n            if module is None:\n                globs = {}\n            else:\n                globs = module.__dict__.copy()\n        else:\n            globs = globs.copy()\n        if extraglobs is not None:\n            globs.update(extraglobs)\n        if '__name__' not in globs:\n            globs['__name__'] = '__main__'  # provide a default module name\n\n        # Recursively explore `obj`, extracting DocTests.\n        tests = []\n        self._find(tests, obj, name, module, source_lines, globs, {})\n        # Sort the tests by alpha order of names, for consistency in\n        # verbose-mode output.  This was a feature of doctest in Pythons\n        # <= 2.3 that got lost by accident in 2.4.  It was repaired in\n        # 2.4.4 and 2.5.\n        tests.sort()\n        return tests\n\n    def _from_module(self, module, object):\n        \"\"\"\n        Return true if the given object is defined in the given\n        module.\n        \"\"\"\n        if module is None:\n            return True\n        elif inspect.getmodule(object) is not None:\n            return module is inspect.getmodule(object)\n        elif inspect.isfunction(object):\n            return module.__dict__ is object.func_globals\n        elif inspect.isclass(object):\n            return module.__name__ == object.__module__\n        elif hasattr(object, '__module__'):\n            return module.__name__ == object.__module__\n        elif isinstance(object, property):\n            return True # [XX] no way not be sure.\n        else:\n            raise ValueError(\"object must be a class or function\")\n\n    def _find(self, tests, obj, name, module, source_lines, globs, seen):\n        \"\"\"\n        Find tests for the given object and any contained objects, and\n        add them to `tests`.\n        \"\"\"\n        if self._verbose:\n            print 'Finding tests in %s' % name\n\n        # If we've already processed this object, then ignore it.\n        if id(obj) in seen:\n            return\n        seen[id(obj)] = 1\n\n        # Find a test for this object, and add it to the list of tests.\n        test = self._get_test(obj, name, module, globs, source_lines)\n        if test is not None:\n            tests.append(test)\n\n        # Look for tests in a module's contained objects.\n        if inspect.ismodule(obj) and self._recurse:\n            for valname, val in obj.__dict__.items():\n                valname = '%s.%s' % (name, valname)\n                # Recurse to functions & classes.\n                if ((inspect.isfunction(val) or inspect.isclass(val)) and\n                    self._from_module(module, val)):\n                    self._find(tests, val, valname, module, source_lines,\n                               globs, seen)\n\n        # Look for tests in a module's __test__ dictionary.\n        if inspect.ismodule(obj) and self._recurse:\n            for valname, val in getattr(obj, '__test__', {}).items():\n                if not isinstance(valname, basestring):\n                    raise ValueError(\"DocTestFinder.find: __test__ keys \"\n                                     \"must be strings: %r\" %\n                                     (type(valname),))\n                if not (inspect.isfunction(val) or inspect.isclass(val) or\n                        inspect.ismethod(val) or inspect.ismodule(val) or\n                        isinstance(val, basestring)):\n                    raise ValueError(\"DocTestFinder.find: __test__ values \"\n                                     \"must be strings, functions, methods, \"\n                                     \"classes, or modules: %r\" %\n                                     (type(val),))\n                valname = '%s.__test__.%s' % (name, valname)\n                self._find(tests, val, valname, module, source_lines,\n                           globs, seen)\n\n        # Look for tests in a class's contained objects.\n        if inspect.isclass(obj) and self._recurse:\n            for valname, val in obj.__dict__.items():\n                # Special handling for staticmethod/classmethod.\n                if isinstance(val, staticmethod):\n                    val = getattr(obj, valname)\n                if isinstance(val, classmethod):\n                    val = getattr(obj, valname).im_func\n\n                # Recurse to methods, properties, and nested classes.\n                if ((inspect.isfunction(val) or inspect.isclass(val) or\n                      isinstance(val, property)) and\n                      self._from_module(module, val)):\n                    valname = '%s.%s' % (name, valname)\n                    self._find(tests, val, valname, module, source_lines,\n                               globs, seen)\n\n    def _get_test(self, obj, name, module, globs, source_lines):\n        \"\"\"\n        Return a DocTest for the given object, if it defines a docstring;\n        otherwise, return None.\n        \"\"\"\n        # Extract the object's docstring.  If it doesn't have one,\n        # then return None (no test for this object).\n        if isinstance(obj, basestring):\n            docstring = obj\n        else:\n            try:\n                if obj.__doc__ is None:\n                    docstring = ''\n                else:\n                    docstring = obj.__doc__\n                    if not isinstance(docstring, basestring):\n                        docstring = str(docstring)\n            except (TypeError, AttributeError):\n                docstring = ''\n\n        # Find the docstring's location in the file.\n        lineno = self._find_lineno(obj, source_lines)\n\n        # Don't bother if the docstring is empty.\n        if self._exclude_empty and not docstring:\n            return None\n\n        # Return a DocTest for this object.\n        if module is None:\n            filename = None\n        else:\n            filename = getattr(module, '__file__', module.__name__)\n            if filename[-4:] in (\".pyc\", \".pyo\"):\n                filename = filename[:-1]\n        return self._parser.get_doctest(docstring, globs, name,\n                                        filename, lineno)\n\n    def _find_lineno(self, obj, source_lines):\n        \"\"\"\n        Return a line number of the given object's docstring.  Note:\n        this method assumes that the object has a docstring.\n        \"\"\"\n        lineno = None\n\n        # Find the line number for modules.\n        if inspect.ismodule(obj):\n            lineno = 0\n\n        # Find the line number for classes.\n        # Note: this could be fooled if a class is defined multiple\n        # times in a single file.\n        if inspect.isclass(obj):\n            if source_lines is None:\n                return None\n            pat = re.compile(r'^\\s*class\\s*%s\\b' %\n                             getattr(obj, '__name__', '-'))\n            for i, line in enumerate(source_lines):\n                if pat.match(line):\n                    lineno = i\n                    break\n\n        # Find the line number for functions & methods.\n        if inspect.ismethod(obj): obj = obj.im_func\n        if inspect.isfunction(obj): obj = obj.func_code\n        if inspect.istraceback(obj): obj = obj.tb_frame\n        if inspect.isframe(obj): obj = obj.f_code\n        if inspect.iscode(obj):\n            lineno = getattr(obj, 'co_firstlineno', None)-1\n\n        # Find the line number where the docstring starts.  Assume\n        # that it's the first line that begins with a quote mark.\n        # Note: this could be fooled by a multiline function\n        # signature, where a continuation line begins with a quote\n        # mark.\n        if lineno is not None:\n            if source_lines is None:\n                return lineno+1\n            pat = re.compile('(^|.*:)\\s*\\w*(\"|\\')')\n            for lineno in range(lineno, len(source_lines)):\n                if pat.match(source_lines[lineno]):\n                    return lineno\n\n        # We couldn't find the line number.\n        return None\n\n######################################################################\n## 5. DocTest Runner\n######################################################################\n\nclass DocTestRunner:\n    \"\"\"\n    A class used to run DocTest test cases, and accumulate statistics.\n    The `run` method is used to process a single DocTest case.  It\n    returns a tuple `(f, t)`, where `t` is the number of test cases\n    tried, and `f` is the number of test cases that failed.\n\n        >>> tests = DocTestFinder().find(_TestClass)\n        >>> runner = DocTestRunner(verbose=False)\n        >>> tests.sort(key = lambda test: test.name)\n        >>> for test in tests:\n        ...     print test.name, '->', runner.run(test)\n        _TestClass -> TestResults(failed=0, attempted=2)\n        _TestClass.__init__ -> TestResults(failed=0, attempted=2)\n        _TestClass.get -> TestResults(failed=0, attempted=2)\n        _TestClass.square -> TestResults(failed=0, attempted=1)\n\n    The `summarize` method prints a summary of all the test cases that\n    have been run by the runner, and returns an aggregated `(f, t)`\n    tuple:\n\n        >>> runner.summarize(verbose=1)\n        4 items passed all tests:\n           2 tests in _TestClass\n           2 tests in _TestClass.__init__\n           2 tests in _TestClass.get\n           1 tests in _TestClass.square\n        7 tests in 4 items.\n        7 passed and 0 failed.\n        Test passed.\n        TestResults(failed=0, attempted=7)\n\n    The aggregated number of tried examples and failed examples is\n    also available via the `tries` and `failures` attributes:\n\n        >>> runner.tries\n        7\n        >>> runner.failures\n        0\n\n    The comparison between expected outputs and actual outputs is done\n    by an `OutputChecker`.  This comparison may be customized with a\n    number of option flags; see the documentation for `testmod` for\n    more information.  If the option flags are insufficient, then the\n    comparison may also be customized by passing a subclass of\n    `OutputChecker` to the constructor.\n\n    The test runner's display output can be controlled in two ways.\n    First, an output function (`out) can be passed to\n    `TestRunner.run`; this function will be called with strings that\n    should be displayed.  It defaults to `sys.stdout.write`.  If\n    capturing the output is not sufficient, then the display output\n    can be also customized by subclassing DocTestRunner, and\n    overriding the methods `report_start`, `report_success`,\n    `report_unexpected_exception`, and `report_failure`.\n    \"\"\"\n    # This divider string is used to separate failure messages, and to\n    # separate sections of the summary.\n    DIVIDER = \"*\" * 70\n\n    def __init__(self, checker=None, verbose=None, optionflags=0):\n        \"\"\"\n        Create a new test runner.\n\n        Optional keyword arg `checker` is the `OutputChecker` that\n        should be used to compare the expected outputs and actual\n        outputs of doctest examples.\n\n        Optional keyword arg 'verbose' prints lots of stuff if true,\n        only failures if false; by default, it's true iff '-v' is in\n        sys.argv.\n\n        Optional argument `optionflags` can be used to control how the\n        test runner compares expected output to actual output, and how\n        it displays failures.  See the documentation for `testmod` for\n        more information.\n        \"\"\"\n        self._checker = checker or OutputChecker()\n        if verbose is None:\n            verbose = '-v' in sys.argv\n        self._verbose = verbose\n        self.optionflags = optionflags\n        self.original_optionflags = optionflags\n\n        # Keep track of the examples we've run.\n        self.tries = 0\n        self.failures = 0\n        self._name2ft = {}\n\n        # Create a fake output target for capturing doctest output.\n        self._fakeout = _SpoofOut()\n\n    #/////////////////////////////////////////////////////////////////\n    # Reporting methods\n    #/////////////////////////////////////////////////////////////////\n\n    def report_start(self, out, test, example):\n        \"\"\"\n        Report that the test runner is about to process the given\n        example.  (Only displays a message if verbose=True)\n        \"\"\"\n        if self._verbose:\n            if example.want:\n                out('Trying:\\n' + _indent(example.source) +\n                    'Expecting:\\n' + _indent(example.want))\n            else:\n                out('Trying:\\n' + _indent(example.source) +\n                    'Expecting nothing\\n')\n\n    def report_success(self, out, test, example, got):\n        \"\"\"\n        Report that the given example ran successfully.  (Only\n        displays a message if verbose=True)\n        \"\"\"\n        if self._verbose:\n            out(\"ok\\n\")\n\n    def report_failure(self, out, test, example, got):\n        \"\"\"\n        Report that the given example failed.\n        \"\"\"\n        out(self._failure_header(test, example) +\n            self._checker.output_difference(example, got, self.optionflags))\n\n    def report_unexpected_exception(self, out, test, example, exc_info):\n        \"\"\"\n        Report that the given example raised an unexpected exception.\n        \"\"\"\n        out(self._failure_header(test, example) +\n            'Exception raised:\\n' + _indent(_exception_traceback(exc_info)))\n\n    def _failure_header(self, test, example):\n        out = [self.DIVIDER]\n        if test.filename:\n            if test.lineno is not None and example.lineno is not None:\n                lineno = test.lineno + example.lineno + 1\n            else:\n                lineno = '?'\n            out.append('File \"%s\", line %s, in %s' %\n                       (test.filename, lineno, test.name))\n        else:\n            out.append('Line %s, in %s' % (example.lineno+1, test.name))\n        out.append('Failed example:')\n        source = example.source\n        out.append(_indent(source))\n        return '\\n'.join(out)\n\n    #/////////////////////////////////////////////////////////////////\n    # DocTest Running\n    #/////////////////////////////////////////////////////////////////\n\n    def __run(self, test, compileflags, out):\n        \"\"\"\n        Run the examples in `test`.  Write the outcome of each example\n        with one of the `DocTestRunner.report_*` methods, using the\n        writer function `out`.  `compileflags` is the set of compiler\n        flags that should be used to execute examples.  Return a tuple\n        `(f, t)`, where `t` is the number of examples tried, and `f`\n        is the number of examples that failed.  The examples are run\n        in the namespace `test.globs`.\n        \"\"\"\n        # Keep track of the number of failures and tries.\n        failures = tries = 0\n\n        # Save the option flags (since option directives can be used\n        # to modify them).\n        original_optionflags = self.optionflags\n\n        SUCCESS, FAILURE, BOOM = range(3) # `outcome` state\n\n        check = self._checker.check_output\n\n        # Process each example.\n        for examplenum, example in enumerate(test.examples):\n\n            # If REPORT_ONLY_FIRST_FAILURE is set, then suppress\n            # reporting after the first failure.\n            quiet = (self.optionflags & REPORT_ONLY_FIRST_FAILURE and\n                     failures > 0)\n\n            # Merge in the example's options.\n            self.optionflags = original_optionflags\n            if example.options:\n                for (optionflag, val) in example.options.items():\n                    if val:\n                        self.optionflags |= optionflag\n                    else:\n                        self.optionflags &= ~optionflag\n\n            # If 'SKIP' is set, then skip this example.\n            if self.optionflags & SKIP:\n                continue\n\n            # Record that we started this example.\n            tries += 1\n            if not quiet:\n                self.report_start(out, test, example)\n\n            # Use a special filename for compile(), so we can retrieve\n            # the source code during interactive debugging (see\n            # __patched_linecache_getlines).\n            filename = '<doctest %s[%d]>' % (test.name, examplenum)\n\n            # Run the example in the given context (globs), and record\n            # any exception that gets raised.  (But don't intercept\n            # keyboard interrupts.)\n            try:\n                # Don't blink!  This is where the user's code gets run.\n                exec compile(example.source, filename, \"single\",\n                             compileflags, 1) in test.globs\n                self.debugger.set_continue() # ==== Example Finished ====\n                exception = None\n            except KeyboardInterrupt:\n                raise\n            except:\n                exception = sys.exc_info()\n                self.debugger.set_continue() # ==== Example Finished ====\n\n            got = self._fakeout.getvalue()  # the actual output\n            self._fakeout.truncate(0)\n            outcome = FAILURE   # guilty until proved innocent or insane\n\n            # If the example executed without raising any exceptions,\n            # verify its output.\n            if exception is None:\n                if check(example.want, got, self.optionflags):\n                    outcome = SUCCESS\n\n            # The example raised an exception:  check if it was expected.\n            else:\n                exc_info = sys.exc_info()\n                exc_msg = traceback.format_exception_only(*exc_info[:2])[-1]\n                if not quiet:\n                    got += _exception_traceback(exc_info)\n\n                # If `example.exc_msg` is None, then we weren't expecting\n                # an exception.\n                if example.exc_msg is None:\n                    outcome = BOOM\n\n                # We expected an exception:  see whether it matches.\n                elif check(example.exc_msg, exc_msg, self.optionflags):\n                    outcome = SUCCESS\n\n                # Another chance if they didn't care about the detail.\n                elif self.optionflags & IGNORE_EXCEPTION_DETAIL:\n                    if check(_strip_exception_details(example.exc_msg),\n                             _strip_exception_details(exc_msg),\n                             self.optionflags):\n                        outcome = SUCCESS\n\n            # Report the outcome.\n            if outcome is SUCCESS:\n                if not quiet:\n                    self.report_success(out, test, example, got)\n            elif outcome is FAILURE:\n                if not quiet:\n                    self.report_failure(out, test, example, got)\n                failures += 1\n            elif outcome is BOOM:\n                if not quiet:\n                    self.report_unexpected_exception(out, test, example,\n                                                     exc_info)\n                failures += 1\n            else:\n                assert False, (\"unknown outcome\", outcome)\n\n        # Restore the option flags (in case they were modified)\n        self.optionflags = original_optionflags\n\n        # Record and return the number of failures and tries.\n        self.__record_outcome(test, failures, tries)\n        return TestResults(failures, tries)\n\n    def __record_outcome(self, test, f, t):\n        \"\"\"\n        Record the fact that the given DocTest (`test`) generated `f`\n        failures out of `t` tried examples.\n        \"\"\"\n        f2, t2 = self._name2ft.get(test.name, (0,0))\n        self._name2ft[test.name] = (f+f2, t+t2)\n        self.failures += f\n        self.tries += t\n\n    __LINECACHE_FILENAME_RE = re.compile(r'<doctest '\n                                         r'(?P<name>.+)'\n                                         r'\\[(?P<examplenum>\\d+)\\]>$')\n    def __patched_linecache_getlines(self, filename, module_globals=None):\n        m = self.__LINECACHE_FILENAME_RE.match(filename)\n        if m and m.group('name') == self.test.name:\n            example = self.test.examples[int(m.group('examplenum'))]\n            source = example.source\n            if isinstance(source, unicode):\n                source = source.encode('ascii', 'backslashreplace')\n            return source.splitlines(True)\n        else:\n            return self.save_linecache_getlines(filename, module_globals)\n\n    def run(self, test, compileflags=None, out=None, clear_globs=True):\n        \"\"\"\n        Run the examples in `test`, and display the results using the\n        writer function `out`.\n\n        The examples are run in the namespace `test.globs`.  If\n        `clear_globs` is true (the default), then this namespace will\n        be cleared after the test runs, to help with garbage\n        collection.  If you would like to examine the namespace after\n        the test completes, then use `clear_globs=False`.\n\n        `compileflags` gives the set of flags that should be used by\n        the Python compiler when running the examples.  If not\n        specified, then it will default to the set of future-import\n        flags that apply to `globs`.\n\n        The output of each example is checked using\n        `DocTestRunner.check_output`, and the results are formatted by\n        the `DocTestRunner.report_*` methods.\n        \"\"\"\n        self.test = test\n\n        if compileflags is None:\n            compileflags = _extract_future_flags(test.globs)\n\n        save_stdout = sys.stdout\n        if out is None:\n            out = save_stdout.write\n        sys.stdout = self._fakeout\n\n        # Patch pdb.set_trace to restore sys.stdout during interactive\n        # debugging (so it's not still redirected to self._fakeout).\n        # Note that the interactive output will go to *our*\n        # save_stdout, even if that's not the real sys.stdout; this\n        # allows us to write test cases for the set_trace behavior.\n        save_set_trace = pdb.set_trace\n        self.debugger = _OutputRedirectingPdb(save_stdout)\n        self.debugger.reset()\n        pdb.set_trace = self.debugger.set_trace\n\n        # Patch linecache.getlines, so we can see the example's source\n        # when we're inside the debugger.\n        self.save_linecache_getlines = linecache.getlines\n        linecache.getlines = self.__patched_linecache_getlines\n\n        # Make sure sys.displayhook just prints the value to stdout\n        save_displayhook = sys.displayhook\n        sys.displayhook = sys.__displayhook__\n\n        try:\n            return self.__run(test, compileflags, out)\n        finally:\n            sys.stdout = save_stdout\n            pdb.set_trace = save_set_trace\n            linecache.getlines = self.save_linecache_getlines\n            sys.displayhook = save_displayhook\n            if clear_globs:\n                test.globs.clear()\n\n    #/////////////////////////////////////////////////////////////////\n    # Summarization\n    #/////////////////////////////////////////////////////////////////\n    def summarize(self, verbose=None):\n        \"\"\"\n        Print a summary of all the test cases that have been run by\n        this DocTestRunner, and return a tuple `(f, t)`, where `f` is\n        the total number of failed examples, and `t` is the total\n        number of tried examples.\n\n        The optional `verbose` argument controls how detailed the\n        summary is.  If the verbosity is not specified, then the\n        DocTestRunner's verbosity is used.\n        \"\"\"\n        if verbose is None:\n            verbose = self._verbose\n        notests = []\n        passed = []\n        failed = []\n        totalt = totalf = 0\n        for x in self._name2ft.items():\n            name, (f, t) = x\n            assert f <= t\n            totalt += t\n            totalf += f\n            if t == 0:\n                notests.append(name)\n            elif f == 0:\n                passed.append( (name, t) )\n            else:\n                failed.append(x)\n        if verbose:\n            if notests:\n                print len(notests), \"items had no tests:\"\n                notests.sort()\n                for thing in notests:\n                    print \"   \", thing\n            if passed:\n                print len(passed), \"items passed all tests:\"\n                passed.sort()\n                for thing, count in passed:\n                    print \" %3d tests in %s\" % (count, thing)\n        if failed:\n            print self.DIVIDER\n            print len(failed), \"items had failures:\"\n            failed.sort()\n            for thing, (f, t) in failed:\n                print \" %3d of %3d in %s\" % (f, t, thing)\n        if verbose:\n            print totalt, \"tests in\", len(self._name2ft), \"items.\"\n            print totalt - totalf, \"passed and\", totalf, \"failed.\"\n        if totalf:\n            print \"***Test Failed***\", totalf, \"failures.\"\n        elif verbose:\n            print \"Test passed.\"\n        return TestResults(totalf, totalt)\n\n    #/////////////////////////////////////////////////////////////////\n    # Backward compatibility cruft to maintain doctest.master.\n    #/////////////////////////////////////////////////////////////////\n    def merge(self, other):\n        d = self._name2ft\n        for name, (f, t) in other._name2ft.items():\n            if name in d:\n                # Don't print here by default, since doing\n                #     so breaks some of the buildbots\n                #print \"*** DocTestRunner.merge: '\" + name + \"' in both\" \\\n                #    \" testers; summing outcomes.\"\n                f2, t2 = d[name]\n                f = f + f2\n                t = t + t2\n            d[name] = f, t\n\nclass OutputChecker:\n    \"\"\"\n    A class used to check the whether the actual output from a doctest\n    example matches the expected output.  `OutputChecker` defines two\n    methods: `check_output`, which compares a given pair of outputs,\n    and returns true if they match; and `output_difference`, which\n    returns a string describing the differences between two outputs.\n    \"\"\"\n    def check_output(self, want, got, optionflags):\n        \"\"\"\n        Return True iff the actual output from an example (`got`)\n        matches the expected output (`want`).  These strings are\n        always considered to match if they are identical; but\n        depending on what option flags the test runner is using,\n        several non-exact match types are also possible.  See the\n        documentation for `TestRunner` for more information about\n        option flags.\n        \"\"\"\n        # Handle the common case first, for efficiency:\n        # if they're string-identical, always return true.\n        if got == want:\n            return True\n\n        # The values True and False replaced 1 and 0 as the return\n        # value for boolean comparisons in Python 2.3.\n        if not (optionflags & DONT_ACCEPT_TRUE_FOR_1):\n            if (got,want) == (\"True\\n\", \"1\\n\"):\n                return True\n            if (got,want) == (\"False\\n\", \"0\\n\"):\n                return True\n\n        # <BLANKLINE> can be used as a special sequence to signify a\n        # blank line, unless the DONT_ACCEPT_BLANKLINE flag is used.\n        if not (optionflags & DONT_ACCEPT_BLANKLINE):\n            # Replace <BLANKLINE> in want with a blank line.\n            want = re.sub('(?m)^%s\\s*?$' % re.escape(BLANKLINE_MARKER),\n                          '', want)\n            # If a line in got contains only spaces, then remove the\n            # spaces.\n            got = re.sub('(?m)^\\s*?$', '', got)\n            if got == want:\n                return True\n\n        # This flag causes doctest to ignore any differences in the\n        # contents of whitespace strings.  Note that this can be used\n        # in conjunction with the ELLIPSIS flag.\n        if optionflags & NORMALIZE_WHITESPACE:\n            got = ' '.join(got.split())\n            want = ' '.join(want.split())\n            if got == want:\n                return True\n\n        # The ELLIPSIS flag says to let the sequence \"...\" in `want`\n        # match any substring in `got`.\n        if optionflags & ELLIPSIS:\n            if _ellipsis_match(want, got):\n                return True\n\n        # We didn't find any match; return false.\n        return False\n\n    # Should we do a fancy diff?\n    def _do_a_fancy_diff(self, want, got, optionflags):\n        # Not unless they asked for a fancy diff.\n        if not optionflags & (REPORT_UDIFF |\n                              REPORT_CDIFF |\n                              REPORT_NDIFF):\n            return False\n\n        # If expected output uses ellipsis, a meaningful fancy diff is\n        # too hard ... or maybe not.  In two real-life failures Tim saw,\n        # a diff was a major help anyway, so this is commented out.\n        # [todo] _ellipsis_match() knows which pieces do and don't match,\n        # and could be the basis for a kick-ass diff in this case.\n        ##if optionflags & ELLIPSIS and ELLIPSIS_MARKER in want:\n        ##    return False\n\n        # ndiff does intraline difference marking, so can be useful even\n        # for 1-line differences.\n        if optionflags & REPORT_NDIFF:\n            return True\n\n        # The other diff types need at least a few lines to be helpful.\n        return want.count('\\n') > 2 and got.count('\\n') > 2\n\n    def output_difference(self, example, got, optionflags):\n        \"\"\"\n        Return a string describing the differences between the\n        expected output for a given example (`example`) and the actual\n        output (`got`).  `optionflags` is the set of option flags used\n        to compare `want` and `got`.\n        \"\"\"\n        want = example.want\n        # If <BLANKLINE>s are being used, then replace blank lines\n        # with <BLANKLINE> in the actual output string.\n        if not (optionflags & DONT_ACCEPT_BLANKLINE):\n            got = re.sub('(?m)^[ ]*(?=\\n)', BLANKLINE_MARKER, got)\n\n        # Check if we should use diff.\n        if self._do_a_fancy_diff(want, got, optionflags):\n            # Split want & got into lines.\n            want_lines = want.splitlines(True)  # True == keep line ends\n            got_lines = got.splitlines(True)\n            # Use difflib to find their differences.\n            if optionflags & REPORT_UDIFF:\n                diff = difflib.unified_diff(want_lines, got_lines, n=2)\n                diff = list(diff)[2:] # strip the diff header\n                kind = 'unified diff with -expected +actual'\n            elif optionflags & REPORT_CDIFF:\n                diff = difflib.context_diff(want_lines, got_lines, n=2)\n                diff = list(diff)[2:] # strip the diff header\n                kind = 'context diff with expected followed by actual'\n            elif optionflags & REPORT_NDIFF:\n                engine = difflib.Differ(charjunk=difflib.IS_CHARACTER_JUNK)\n                diff = list(engine.compare(want_lines, got_lines))\n                kind = 'ndiff with -expected +actual'\n            else:\n                assert 0, 'Bad diff option'\n            # Remove trailing whitespace on diff output.\n            diff = [line.rstrip() + '\\n' for line in diff]\n            return 'Differences (%s):\\n' % kind + _indent(''.join(diff))\n\n        # If we're not using diff, then simply list the expected\n        # output followed by the actual output.\n        if want and got:\n            return 'Expected:\\n%sGot:\\n%s' % (_indent(want), _indent(got))\n        elif want:\n            return 'Expected:\\n%sGot nothing\\n' % _indent(want)\n        elif got:\n            return 'Expected nothing\\nGot:\\n%s' % _indent(got)\n        else:\n            return 'Expected nothing\\nGot nothing\\n'\n\nclass DocTestFailure(Exception):\n    \"\"\"A DocTest example has failed in debugging mode.\n\n    The exception instance has variables:\n\n    - test: the DocTest object being run\n\n    - example: the Example object that failed\n\n    - got: the actual output\n    \"\"\"\n    def __init__(self, test, example, got):\n        self.test = test\n        self.example = example\n        self.got = got\n\n    def __str__(self):\n        return str(self.test)\n\nclass UnexpectedException(Exception):\n    \"\"\"A DocTest example has encountered an unexpected exception\n\n    The exception instance has variables:\n\n    - test: the DocTest object being run\n\n    - example: the Example object that failed\n\n    - exc_info: the exception info\n    \"\"\"\n    def __init__(self, test, example, exc_info):\n        self.test = test\n        self.example = example\n        self.exc_info = exc_info\n\n    def __str__(self):\n        return str(self.test)\n\nclass DebugRunner(DocTestRunner):\n    r\"\"\"Run doc tests but raise an exception as soon as there is a failure.\n\n       If an unexpected exception occurs, an UnexpectedException is raised.\n       It contains the test, the example, and the original exception:\n\n         >>> runner = DebugRunner(verbose=False)\n         >>> test = DocTestParser().get_doctest('>>> raise KeyError\\n42',\n         ...                                    {}, 'foo', 'foo.py', 0)\n         >>> try:\n         ...     runner.run(test)\n         ... except UnexpectedException, failure:\n         ...     pass\n\n         >>> failure.test is test\n         True\n\n         >>> failure.example.want\n         '42\\n'\n\n         >>> exc_info = failure.exc_info\n         >>> raise exc_info[0], exc_info[1], exc_info[2]\n         Traceback (most recent call last):\n         ...\n         KeyError\n\n       We wrap the original exception to give the calling application\n       access to the test and example information.\n\n       If the output doesn't match, then a DocTestFailure is raised:\n\n         >>> test = DocTestParser().get_doctest('''\n         ...      >>> x = 1\n         ...      >>> x\n         ...      2\n         ...      ''', {}, 'foo', 'foo.py', 0)\n\n         >>> try:\n         ...    runner.run(test)\n         ... except DocTestFailure, failure:\n         ...    pass\n\n       DocTestFailure objects provide access to the test:\n\n         >>> failure.test is test\n         True\n\n       As well as to the example:\n\n         >>> failure.example.want\n         '2\\n'\n\n       and the actual output:\n\n         >>> failure.got\n         '1\\n'\n\n       If a failure or error occurs, the globals are left intact:\n\n         >>> del test.globs['__builtins__']\n         >>> test.globs\n         {'x': 1}\n\n         >>> test = DocTestParser().get_doctest('''\n         ...      >>> x = 2\n         ...      >>> raise KeyError\n         ...      ''', {}, 'foo', 'foo.py', 0)\n\n         >>> runner.run(test)\n         Traceback (most recent call last):\n         ...\n         UnexpectedException: <DocTest foo from foo.py:0 (2 examples)>\n\n         >>> del test.globs['__builtins__']\n         >>> test.globs\n         {'x': 2}\n\n       But the globals are cleared if there is no error:\n\n         >>> test = DocTestParser().get_doctest('''\n         ...      >>> x = 2\n         ...      ''', {}, 'foo', 'foo.py', 0)\n\n         >>> runner.run(test)\n         TestResults(failed=0, attempted=1)\n\n         >>> test.globs\n         {}\n\n       \"\"\"\n\n    def run(self, test, compileflags=None, out=None, clear_globs=True):\n        r = DocTestRunner.run(self, test, compileflags, out, False)\n        if clear_globs:\n            test.globs.clear()\n        return r\n\n    def report_unexpected_exception(self, out, test, example, exc_info):\n        raise UnexpectedException(test, example, exc_info)\n\n    def report_failure(self, out, test, example, got):\n        raise DocTestFailure(test, example, got)\n\n######################################################################\n## 6. Test Functions\n######################################################################\n# These should be backwards compatible.\n\n# For backward compatibility, a global instance of a DocTestRunner\n# class, updated by testmod.\nmaster = None\n\ndef testmod(m=None, name=None, globs=None, verbose=None,\n            report=True, optionflags=0, extraglobs=None,\n            raise_on_error=False, exclude_empty=False):\n    \"\"\"m=None, name=None, globs=None, verbose=None, report=True,\n       optionflags=0, extraglobs=None, raise_on_error=False,\n       exclude_empty=False\n\n    Test examples in docstrings in functions and classes reachable\n    from module m (or the current module if m is not supplied), starting\n    with m.__doc__.\n\n    Also test examples reachable from dict m.__test__ if it exists and is\n    not None.  m.__test__ maps names to functions, classes and strings;\n    function and class docstrings are tested even if the name is private;\n    strings are tested directly, as if they were docstrings.\n\n    Return (#failures, #tests).\n\n    See help(doctest) for an overview.\n\n    Optional keyword arg \"name\" gives the name of the module; by default\n    use m.__name__.\n\n    Optional keyword arg \"globs\" gives a dict to be used as the globals\n    when executing examples; by default, use m.__dict__.  A copy of this\n    dict is actually used for each docstring, so that each docstring's\n    examples start with a clean slate.\n\n    Optional keyword arg \"extraglobs\" gives a dictionary that should be\n    merged into the globals that are used to execute examples.  By\n    default, no extra globals are used.  This is new in 2.4.\n\n    Optional keyword arg \"verbose\" prints lots of stuff if true, prints\n    only failures if false; by default, it's true iff \"-v\" is in sys.argv.\n\n    Optional keyword arg \"report\" prints a summary at the end when true,\n    else prints nothing at the end.  In verbose mode, the summary is\n    detailed, else very brief (in fact, empty if all tests passed).\n\n    Optional keyword arg \"optionflags\" or's together module constants,\n    and defaults to 0.  This is new in 2.3.  Possible values (see the\n    docs for details):\n\n        DONT_ACCEPT_TRUE_FOR_1\n        DONT_ACCEPT_BLANKLINE\n        NORMALIZE_WHITESPACE\n        ELLIPSIS\n        SKIP\n        IGNORE_EXCEPTION_DETAIL\n        REPORT_UDIFF\n        REPORT_CDIFF\n        REPORT_NDIFF\n        REPORT_ONLY_FIRST_FAILURE\n\n    Optional keyword arg \"raise_on_error\" raises an exception on the\n    first unexpected exception or failure. This allows failures to be\n    post-mortem debugged.\n\n    Advanced tomfoolery:  testmod runs methods of a local instance of\n    class doctest.Tester, then merges the results into (or creates)\n    global Tester instance doctest.master.  Methods of doctest.master\n    can be called directly too, if you want to do something unusual.\n    Passing report=0 to testmod is especially useful then, to delay\n    displaying a summary.  Invoke doctest.master.summarize(verbose)\n    when you're done fiddling.\n    \"\"\"\n    global master\n\n    # If no module was given, then use __main__.\n    if m is None:\n        # DWA - m will still be None if this wasn't invoked from the command\n        # line, in which case the following TypeError is about as good an error\n        # as we should expect\n        m = sys.modules.get('__main__')\n\n    # Check that we were actually given a module.\n    if not inspect.ismodule(m):\n        raise TypeError(\"testmod: module required; %r\" % (m,))\n\n    # If no name was given, then use the module's name.\n    if name is None:\n        name = m.__name__\n\n    # Find, parse, and run all tests in the given module.\n    finder = DocTestFinder(exclude_empty=exclude_empty)\n\n    if raise_on_error:\n        runner = DebugRunner(verbose=verbose, optionflags=optionflags)\n    else:\n        runner = DocTestRunner(verbose=verbose, optionflags=optionflags)\n\n    for test in finder.find(m, name, globs=globs, extraglobs=extraglobs):\n        runner.run(test)\n\n    if report:\n        runner.summarize()\n\n    if master is None:\n        master = runner\n    else:\n        master.merge(runner)\n\n    return TestResults(runner.failures, runner.tries)\n\ndef testfile(filename, module_relative=True, name=None, package=None,\n             globs=None, verbose=None, report=True, optionflags=0,\n             extraglobs=None, raise_on_error=False, parser=DocTestParser(),\n             encoding=None):\n    \"\"\"\n    Test examples in the given file.  Return (#failures, #tests).\n\n    Optional keyword arg \"module_relative\" specifies how filenames\n    should be interpreted:\n\n      - If \"module_relative\" is True (the default), then \"filename\"\n         specifies a module-relative path.  By default, this path is\n         relative to the calling module's directory; but if the\n         \"package\" argument is specified, then it is relative to that\n         package.  To ensure os-independence, \"filename\" should use\n         \"/\" characters to separate path segments, and should not\n         be an absolute path (i.e., it may not begin with \"/\").\n\n      - If \"module_relative\" is False, then \"filename\" specifies an\n        os-specific path.  The path may be absolute or relative (to\n        the current working directory).\n\n    Optional keyword arg \"name\" gives the name of the test; by default\n    use the file's basename.\n\n    Optional keyword argument \"package\" is a Python package or the\n    name of a Python package whose directory should be used as the\n    base directory for a module relative filename.  If no package is\n    specified, then the calling module's directory is used as the base\n    directory for module relative filenames.  It is an error to\n    specify \"package\" if \"module_relative\" is False.\n\n    Optional keyword arg \"globs\" gives a dict to be used as the globals\n    when executing examples; by default, use {}.  A copy of this dict\n    is actually used for each docstring, so that each docstring's\n    examples start with a clean slate.\n\n    Optional keyword arg \"extraglobs\" gives a dictionary that should be\n    merged into the globals that are used to execute examples.  By\n    default, no extra globals are used.\n\n    Optional keyword arg \"verbose\" prints lots of stuff if true, prints\n    only failures if false; by default, it's true iff \"-v\" is in sys.argv.\n\n    Optional keyword arg \"report\" prints a summary at the end when true,\n    else prints nothing at the end.  In verbose mode, the summary is\n    detailed, else very brief (in fact, empty if all tests passed).\n\n    Optional keyword arg \"optionflags\" or's together module constants,\n    and defaults to 0.  Possible values (see the docs for details):\n\n        DONT_ACCEPT_TRUE_FOR_1\n        DONT_ACCEPT_BLANKLINE\n        NORMALIZE_WHITESPACE\n        ELLIPSIS\n        SKIP\n        IGNORE_EXCEPTION_DETAIL\n        REPORT_UDIFF\n        REPORT_CDIFF\n        REPORT_NDIFF\n        REPORT_ONLY_FIRST_FAILURE\n\n    Optional keyword arg \"raise_on_error\" raises an exception on the\n    first unexpected exception or failure. This allows failures to be\n    post-mortem debugged.\n\n    Optional keyword arg \"parser\" specifies a DocTestParser (or\n    subclass) that should be used to extract tests from the files.\n\n    Optional keyword arg \"encoding\" specifies an encoding that should\n    be used to convert the file to unicode.\n\n    Advanced tomfoolery:  testmod runs methods of a local instance of\n    class doctest.Tester, then merges the results into (or creates)\n    global Tester instance doctest.master.  Methods of doctest.master\n    can be called directly too, if you want to do something unusual.\n    Passing report=0 to testmod is especially useful then, to delay\n    displaying a summary.  Invoke doctest.master.summarize(verbose)\n    when you're done fiddling.\n    \"\"\"\n    global master\n\n    if package and not module_relative:\n        raise ValueError(\"Package may only be specified for module-\"\n                         \"relative paths.\")\n\n    # Relativize the path\n    text, filename = _load_testfile(filename, package, module_relative)\n\n    # If no name was given, then use the file's name.\n    if name is None:\n        name = os.path.basename(filename)\n\n    # Assemble the globals.\n    if globs is None:\n        globs = {}\n    else:\n        globs = globs.copy()\n    if extraglobs is not None:\n        globs.update(extraglobs)\n    if '__name__' not in globs:\n        globs['__name__'] = '__main__'\n\n    if raise_on_error:\n        runner = DebugRunner(verbose=verbose, optionflags=optionflags)\n    else:\n        runner = DocTestRunner(verbose=verbose, optionflags=optionflags)\n\n    if encoding is not None:\n        text = text.decode(encoding)\n\n    # Read the file, convert it to a test, and run it.\n    test = parser.get_doctest(text, globs, name, filename, 0)\n    runner.run(test)\n\n    if report:\n        runner.summarize()\n\n    if master is None:\n        master = runner\n    else:\n        master.merge(runner)\n\n    return TestResults(runner.failures, runner.tries)\n\ndef run_docstring_examples(f, globs, verbose=False, name=\"NoName\",\n                           compileflags=None, optionflags=0):\n    \"\"\"\n    Test examples in the given object's docstring (`f`), using `globs`\n    as globals.  Optional argument `name` is used in failure messages.\n    If the optional argument `verbose` is true, then generate output\n    even if there are no failures.\n\n    `compileflags` gives the set of flags that should be used by the\n    Python compiler when running the examples.  If not specified, then\n    it will default to the set of future-import flags that apply to\n    `globs`.\n\n    Optional keyword arg `optionflags` specifies options for the\n    testing and output.  See the documentation for `testmod` for more\n    information.\n    \"\"\"\n    # Find, parse, and run all tests in the given module.\n    finder = DocTestFinder(verbose=verbose, recurse=False)\n    runner = DocTestRunner(verbose=verbose, optionflags=optionflags)\n    for test in finder.find(f, name, globs=globs):\n        runner.run(test, compileflags=compileflags)\n\n######################################################################\n## 7. Tester\n######################################################################\n# This is provided only for backwards compatibility.  It's not\n# actually used in any way.\n\nclass Tester:\n    def __init__(self, mod=None, globs=None, verbose=None, optionflags=0):\n\n        warnings.warn(\"class Tester is deprecated; \"\n                      \"use class doctest.DocTestRunner instead\",\n                      DeprecationWarning, stacklevel=2)\n        if mod is None and globs is None:\n            raise TypeError(\"Tester.__init__: must specify mod or globs\")\n        if mod is not None and not inspect.ismodule(mod):\n            raise TypeError(\"Tester.__init__: mod must be a module; %r\" %\n                            (mod,))\n        if globs is None:\n            globs = mod.__dict__\n        self.globs = globs\n\n        self.verbose = verbose\n        self.optionflags = optionflags\n        self.testfinder = DocTestFinder()\n        self.testrunner = DocTestRunner(verbose=verbose,\n                                        optionflags=optionflags)\n\n    def runstring(self, s, name):\n        test = DocTestParser().get_doctest(s, self.globs, name, None, None)\n        if self.verbose:\n            print \"Running string\", name\n        (f,t) = self.testrunner.run(test)\n        if self.verbose:\n            print f, \"of\", t, \"examples failed in string\", name\n        return TestResults(f,t)\n\n    def rundoc(self, object, name=None, module=None):\n        f = t = 0\n        tests = self.testfinder.find(object, name, module=module,\n                                     globs=self.globs)\n        for test in tests:\n            (f2, t2) = self.testrunner.run(test)\n            (f,t) = (f+f2, t+t2)\n        return TestResults(f,t)\n\n    def rundict(self, d, name, module=None):\n        import types\n        m = types.ModuleType(name)\n        m.__dict__.update(d)\n        if module is None:\n            module = False\n        return self.rundoc(m, name, module)\n\n    def run__test__(self, d, name):\n        import types\n        m = types.ModuleType(name)\n        m.__test__ = d\n        return self.rundoc(m, name)\n\n    def summarize(self, verbose=None):\n        return self.testrunner.summarize(verbose)\n\n    def merge(self, other):\n        self.testrunner.merge(other.testrunner)\n\n######################################################################\n## 8. Unittest Support\n######################################################################\n\n_unittest_reportflags = 0\n\ndef set_unittest_reportflags(flags):\n    \"\"\"Sets the unittest option flags.\n\n    The old flag is returned so that a runner could restore the old\n    value if it wished to:\n\n      >>> import doctest\n      >>> old = doctest._unittest_reportflags\n      >>> doctest.set_unittest_reportflags(REPORT_NDIFF |\n      ...                          REPORT_ONLY_FIRST_FAILURE) == old\n      True\n\n      >>> doctest._unittest_reportflags == (REPORT_NDIFF |\n      ...                                   REPORT_ONLY_FIRST_FAILURE)\n      True\n\n    Only reporting flags can be set:\n\n      >>> doctest.set_unittest_reportflags(ELLIPSIS)\n      Traceback (most recent call last):\n      ...\n      ValueError: ('Only reporting flags allowed', 8)\n\n      >>> doctest.set_unittest_reportflags(old) == (REPORT_NDIFF |\n      ...                                   REPORT_ONLY_FIRST_FAILURE)\n      True\n    \"\"\"\n    global _unittest_reportflags\n\n    if (flags & REPORTING_FLAGS) != flags:\n        raise ValueError(\"Only reporting flags allowed\", flags)\n    old = _unittest_reportflags\n    _unittest_reportflags = flags\n    return old\n\n\nclass DocTestCase(unittest.TestCase):\n\n    def __init__(self, test, optionflags=0, setUp=None, tearDown=None,\n                 checker=None):\n\n        unittest.TestCase.__init__(self)\n        self._dt_optionflags = optionflags\n        self._dt_checker = checker\n        self._dt_test = test\n        self._dt_setUp = setUp\n        self._dt_tearDown = tearDown\n\n    def setUp(self):\n        test = self._dt_test\n\n        if self._dt_setUp is not None:\n            self._dt_setUp(test)\n\n    def tearDown(self):\n        test = self._dt_test\n\n        if self._dt_tearDown is not None:\n            self._dt_tearDown(test)\n\n        test.globs.clear()\n\n    def runTest(self):\n        test = self._dt_test\n        old = sys.stdout\n        new = StringIO()\n        optionflags = self._dt_optionflags\n\n        if not (optionflags & REPORTING_FLAGS):\n            # The option flags don't include any reporting flags,\n            # so add the default reporting flags\n            optionflags |= _unittest_reportflags\n\n        runner = DocTestRunner(optionflags=optionflags,\n                               checker=self._dt_checker, verbose=False)\n\n        try:\n            runner.DIVIDER = \"-\"*70\n            failures, tries = runner.run(\n                test, out=new.write, clear_globs=False)\n        finally:\n            sys.stdout = old\n\n        if failures:\n            raise self.failureException(self.format_failure(new.getvalue()))\n\n    def format_failure(self, err):\n        test = self._dt_test\n        if test.lineno is None:\n            lineno = 'unknown line number'\n        else:\n            lineno = '%s' % test.lineno\n        lname = '.'.join(test.name.split('.')[-1:])\n        return ('Failed doctest test for %s\\n'\n                '  File \"%s\", line %s, in %s\\n\\n%s'\n                % (test.name, test.filename, lineno, lname, err)\n                )\n\n    def debug(self):\n        r\"\"\"Run the test case without results and without catching exceptions\n\n           The unit test framework includes a debug method on test cases\n           and test suites to support post-mortem debugging.  The test code\n           is run in such a way that errors are not caught.  This way a\n           caller can catch the errors and initiate post-mortem debugging.\n\n           The DocTestCase provides a debug method that raises\n           UnexpectedException errors if there is an unexpected\n           exception:\n\n             >>> test = DocTestParser().get_doctest('>>> raise KeyError\\n42',\n             ...                {}, 'foo', 'foo.py', 0)\n             >>> case = DocTestCase(test)\n             >>> try:\n             ...     case.debug()\n             ... except UnexpectedException, failure:\n             ...     pass\n\n           The UnexpectedException contains the test, the example, and\n           the original exception:\n\n             >>> failure.test is test\n             True\n\n             >>> failure.example.want\n             '42\\n'\n\n             >>> exc_info = failure.exc_info\n             >>> raise exc_info[0], exc_info[1], exc_info[2]\n             Traceback (most recent call last):\n             ...\n             KeyError\n\n           If the output doesn't match, then a DocTestFailure is raised:\n\n             >>> test = DocTestParser().get_doctest('''\n             ...      >>> x = 1\n             ...      >>> x\n             ...      2\n             ...      ''', {}, 'foo', 'foo.py', 0)\n             >>> case = DocTestCase(test)\n\n             >>> try:\n             ...    case.debug()\n             ... except DocTestFailure, failure:\n             ...    pass\n\n           DocTestFailure objects provide access to the test:\n\n             >>> failure.test is test\n             True\n\n           As well as to the example:\n\n             >>> failure.example.want\n             '2\\n'\n\n           and the actual output:\n\n             >>> failure.got\n             '1\\n'\n\n           \"\"\"\n\n        self.setUp()\n        runner = DebugRunner(optionflags=self._dt_optionflags,\n                             checker=self._dt_checker, verbose=False)\n        runner.run(self._dt_test, clear_globs=False)\n        self.tearDown()\n\n    def id(self):\n        return self._dt_test.name\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self._dt_test == other._dt_test and \\\n               self._dt_optionflags == other._dt_optionflags and \\\n               self._dt_setUp == other._dt_setUp and \\\n               self._dt_tearDown == other._dt_tearDown and \\\n               self._dt_checker == other._dt_checker\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self._dt_optionflags, self._dt_setUp, self._dt_tearDown,\n                     self._dt_checker))\n\n    def __repr__(self):\n        name = self._dt_test.name.split('.')\n        return \"%s (%s)\" % (name[-1], '.'.join(name[:-1]))\n\n    __str__ = __repr__\n\n    def shortDescription(self):\n        return \"Doctest: \" + self._dt_test.name\n\nclass SkipDocTestCase(DocTestCase):\n    def __init__(self, module):\n        self.module = module\n        DocTestCase.__init__(self, None)\n\n    def setUp(self):\n        self.skipTest(\"DocTestSuite will not work with -O2 and above\")\n\n    def test_skip(self):\n        pass\n\n    def shortDescription(self):\n        return \"Skipping tests from %s\" % self.module.__name__\n\n    __str__ = shortDescription\n\n\ndef DocTestSuite(module=None, globs=None, extraglobs=None, test_finder=None,\n                 **options):\n    \"\"\"\n    Convert doctest tests for a module to a unittest test suite.\n\n    This converts each documentation string in a module that\n    contains doctest tests to a unittest test case.  If any of the\n    tests in a doc string fail, then the test case fails.  An exception\n    is raised showing the name of the file containing the test and a\n    (sometimes approximate) line number.\n\n    The `module` argument provides the module to be tested.  The argument\n    can be either a module or a module name.\n\n    If no argument is given, the calling module is used.\n\n    A number of options may be provided as keyword arguments:\n\n    setUp\n      A set-up function.  This is called before running the\n      tests in each file. The setUp function will be passed a DocTest\n      object.  The setUp function can access the test globals as the\n      globs attribute of the test passed.\n\n    tearDown\n      A tear-down function.  This is called after running the\n      tests in each file.  The tearDown function will be passed a DocTest\n      object.  The tearDown function can access the test globals as the\n      globs attribute of the test passed.\n\n    globs\n      A dictionary containing initial global variables for the tests.\n\n    optionflags\n       A set of doctest option flags expressed as an integer.\n    \"\"\"\n\n    if test_finder is None:\n        test_finder = DocTestFinder()\n\n    module = _normalize_module(module)\n    tests = test_finder.find(module, globs=globs, extraglobs=extraglobs)\n\n    if not tests and sys.flags.optimize >=2:\n        # Skip doctests when running with -O2\n        suite = unittest.TestSuite()\n        suite.addTest(SkipDocTestCase(module))\n        return suite\n    elif not tests:\n        # Why do we want to do this? Because it reveals a bug that might\n        # otherwise be hidden.\n        # It is probably a bug that this exception is not also raised if the\n        # number of doctest examples in tests is zero (i.e. if no doctest\n        # examples were found).  However, we should probably not be raising\n        # an exception at all here, though it is too late to make this change\n        # for a maintenance release.  See also issue #14649.\n        raise ValueError(module, \"has no docstrings\")\n\n    tests.sort()\n    suite = unittest.TestSuite()\n\n    for test in tests:\n        if len(test.examples) == 0:\n            continue\n        if not test.filename:\n            filename = module.__file__\n            if filename[-4:] in (\".pyc\", \".pyo\"):\n                filename = filename[:-1]\n            test.filename = filename\n        suite.addTest(DocTestCase(test, **options))\n\n    return suite\n\nclass DocFileCase(DocTestCase):\n\n    def id(self):\n        return '_'.join(self._dt_test.name.split('.'))\n\n    def __repr__(self):\n        return self._dt_test.filename\n    __str__ = __repr__\n\n    def format_failure(self, err):\n        return ('Failed doctest test for %s\\n  File \"%s\", line 0\\n\\n%s'\n                % (self._dt_test.name, self._dt_test.filename, err)\n                )\n\ndef DocFileTest(path, module_relative=True, package=None,\n                globs=None, parser=DocTestParser(),\n                encoding=None, **options):\n    if globs is None:\n        globs = {}\n    else:\n        globs = globs.copy()\n\n    if package and not module_relative:\n        raise ValueError(\"Package may only be specified for module-\"\n                         \"relative paths.\")\n\n    # Relativize the path.\n    doc, path = _load_testfile(path, package, module_relative)\n\n    if \"__file__\" not in globs:\n        globs[\"__file__\"] = path\n\n    # Find the file and read it.\n    name = os.path.basename(path)\n\n    # If an encoding is specified, use it to convert the file to unicode\n    if encoding is not None:\n        doc = doc.decode(encoding)\n\n    # Convert it to a test, and wrap it in a DocFileCase.\n    test = parser.get_doctest(doc, globs, name, path, 0)\n    return DocFileCase(test, **options)\n\ndef DocFileSuite(*paths, **kw):\n    \"\"\"A unittest suite for one or more doctest files.\n\n    The path to each doctest file is given as a string; the\n    interpretation of that string depends on the keyword argument\n    \"module_relative\".\n\n    A number of options may be provided as keyword arguments:\n\n    module_relative\n      If \"module_relative\" is True, then the given file paths are\n      interpreted as os-independent module-relative paths.  By\n      default, these paths are relative to the calling module's\n      directory; but if the \"package\" argument is specified, then\n      they are relative to that package.  To ensure os-independence,\n      \"filename\" should use \"/\" characters to separate path\n      segments, and may not be an absolute path (i.e., it may not\n      begin with \"/\").\n\n      If \"module_relative\" is False, then the given file paths are\n      interpreted as os-specific paths.  These paths may be absolute\n      or relative (to the current working directory).\n\n    package\n      A Python package or the name of a Python package whose directory\n      should be used as the base directory for module relative paths.\n      If \"package\" is not specified, then the calling module's\n      directory is used as the base directory for module relative\n      filenames.  It is an error to specify \"package\" if\n      \"module_relative\" is False.\n\n    setUp\n      A set-up function.  This is called before running the\n      tests in each file. The setUp function will be passed a DocTest\n      object.  The setUp function can access the test globals as the\n      globs attribute of the test passed.\n\n    tearDown\n      A tear-down function.  This is called after running the\n      tests in each file.  The tearDown function will be passed a DocTest\n      object.  The tearDown function can access the test globals as the\n      globs attribute of the test passed.\n\n    globs\n      A dictionary containing initial global variables for the tests.\n\n    optionflags\n      A set of doctest option flags expressed as an integer.\n\n    parser\n      A DocTestParser (or subclass) that should be used to extract\n      tests from the files.\n\n    encoding\n      An encoding that will be used to convert the files to unicode.\n    \"\"\"\n    suite = unittest.TestSuite()\n\n    # We do this here so that _normalize_module is called at the right\n    # level.  If it were called in DocFileTest, then this function\n    # would be the caller and we might guess the package incorrectly.\n    if kw.get('module_relative', True):\n        kw['package'] = _normalize_module(kw.get('package'))\n\n    for path in paths:\n        suite.addTest(DocFileTest(path, **kw))\n\n    return suite\n\n######################################################################\n## 9. Debugging Support\n######################################################################\n\ndef script_from_examples(s):\n    r\"\"\"Extract script from text with examples.\n\n       Converts text with examples to a Python script.  Example input is\n       converted to regular code.  Example output and all other words\n       are converted to comments:\n\n       >>> text = '''\n       ...       Here are examples of simple math.\n       ...\n       ...           Python has super accurate integer addition\n       ...\n       ...           >>> 2 + 2\n       ...           5\n       ...\n       ...           And very friendly error messages:\n       ...\n       ...           >>> 1/0\n       ...           To Infinity\n       ...           And\n       ...           Beyond\n       ...\n       ...           You can use logic if you want:\n       ...\n       ...           >>> if 0:\n       ...           ...    blah\n       ...           ...    blah\n       ...           ...\n       ...\n       ...           Ho hum\n       ...           '''\n\n       >>> print script_from_examples(text)\n       # Here are examples of simple math.\n       #\n       #     Python has super accurate integer addition\n       #\n       2 + 2\n       # Expected:\n       ## 5\n       #\n       #     And very friendly error messages:\n       #\n       1/0\n       # Expected:\n       ## To Infinity\n       ## And\n       ## Beyond\n       #\n       #     You can use logic if you want:\n       #\n       if 0:\n          blah\n          blah\n       #\n       #     Ho hum\n       <BLANKLINE>\n       \"\"\"\n    output = []\n    for piece in DocTestParser().parse(s):\n        if isinstance(piece, Example):\n            # Add the example's source code (strip trailing NL)\n            output.append(piece.source[:-1])\n            # Add the expected output:\n            want = piece.want\n            if want:\n                output.append('# Expected:')\n                output += ['## '+l for l in want.split('\\n')[:-1]]\n        else:\n            # Add non-example text.\n            output += [_comment_line(l)\n                       for l in piece.split('\\n')[:-1]]\n\n    # Trim junk on both ends.\n    while output and output[-1] == '#':\n        output.pop()\n    while output and output[0] == '#':\n        output.pop(0)\n    # Combine the output, and return it.\n    # Add a courtesy newline to prevent exec from choking (see bug #1172785)\n    return '\\n'.join(output) + '\\n'\n\ndef testsource(module, name):\n    \"\"\"Extract the test sources from a doctest docstring as a script.\n\n    Provide the module (or dotted name of the module) containing the\n    test to be debugged and the name (within the module) of the object\n    with the doc string with tests to be debugged.\n    \"\"\"\n    module = _normalize_module(module)\n    tests = DocTestFinder().find(module)\n    test = [t for t in tests if t.name == name]\n    if not test:\n        raise ValueError(name, \"not found in tests\")\n    test = test[0]\n    testsrc = script_from_examples(test.docstring)\n    return testsrc\n\ndef debug_src(src, pm=False, globs=None):\n    \"\"\"Debug a single doctest docstring, in argument `src`'\"\"\"\n    testsrc = script_from_examples(src)\n    debug_script(testsrc, pm, globs)\n\ndef debug_script(src, pm=False, globs=None):\n    \"Debug a test script.  `src` is the script, as a string.\"\n    import pdb\n\n    # Note that tempfile.NameTemporaryFile() cannot be used.  As the\n    # docs say, a file so created cannot be opened by name a second time\n    # on modern Windows boxes, and execfile() needs to open it.\n    srcfilename = tempfile.mktemp(\".py\", \"doctestdebug\")\n    f = open(srcfilename, 'w')\n    f.write(src)\n    f.close()\n\n    try:\n        if globs:\n            globs = globs.copy()\n        else:\n            globs = {}\n\n        if pm:\n            try:\n                execfile(srcfilename, globs, globs)\n            except:\n                print sys.exc_info()[1]\n                pdb.post_mortem(sys.exc_info()[2])\n        else:\n            # Note that %r is vital here.  '%s' instead can, e.g., cause\n            # backslashes to get treated as metacharacters on Windows.\n            pdb.run(\"execfile(%r)\" % srcfilename, globs, globs)\n\n    finally:\n        os.remove(srcfilename)\n\ndef debug(module, name, pm=False):\n    \"\"\"Debug a single doctest docstring.\n\n    Provide the module (or dotted name of the module) containing the\n    test to be debugged and the name (within the module) of the object\n    with the docstring with tests to be debugged.\n    \"\"\"\n    module = _normalize_module(module)\n    testsrc = testsource(module, name)\n    debug_script(testsrc, pm, module.__dict__)\n\n######################################################################\n## 10. Example Usage\n######################################################################\nclass _TestClass:\n    \"\"\"\n    A pointless class, for sanity-checking of docstring testing.\n\n    Methods:\n        square()\n        get()\n\n    >>> _TestClass(13).get() + _TestClass(-12).get()\n    1\n    >>> hex(_TestClass(13).square().get())\n    '0xa9'\n    \"\"\"\n\n    def __init__(self, val):\n        \"\"\"val -> _TestClass object with associated value val.\n\n        >>> t = _TestClass(123)\n        >>> print t.get()\n        123\n        \"\"\"\n\n        self.val = val\n\n    def square(self):\n        \"\"\"square() -> square TestClass's associated value\n\n        >>> _TestClass(13).square().get()\n        169\n        \"\"\"\n\n        self.val = self.val ** 2\n        return self\n\n    def get(self):\n        \"\"\"get() -> return TestClass's associated value.\n\n        >>> x = _TestClass(-42)\n        >>> print x.get()\n        -42\n        \"\"\"\n\n        return self.val\n\n__test__ = {\"_TestClass\": _TestClass,\n            \"string\": r\"\"\"\n                      Example of a string object, searched as-is.\n                      >>> x = 1; y = 2\n                      >>> x + y, x * y\n                      (3, 2)\n                      \"\"\",\n\n            \"bool-int equivalence\": r\"\"\"\n                                    In 2.2, boolean expressions displayed\n                                    0 or 1.  By default, we still accept\n                                    them.  This can be disabled by passing\n                                    DONT_ACCEPT_TRUE_FOR_1 to the new\n                                    optionflags argument.\n                                    >>> 4 == 4\n                                    1\n                                    >>> 4 == 4\n                                    True\n                                    >>> 4 > 4\n                                    0\n                                    >>> 4 > 4\n                                    False\n                                    \"\"\",\n\n            \"blank lines\": r\"\"\"\n                Blank lines can be marked with <BLANKLINE>:\n                    >>> print 'foo\\n\\nbar\\n'\n                    foo\n                    <BLANKLINE>\n                    bar\n                    <BLANKLINE>\n            \"\"\",\n\n            \"ellipsis\": r\"\"\"\n                If the ellipsis flag is used, then '...' can be used to\n                elide substrings in the desired output:\n                    >>> print range(1000) #doctest: +ELLIPSIS\n                    [0, 1, 2, ..., 999]\n            \"\"\",\n\n            \"whitespace normalization\": r\"\"\"\n                If the whitespace normalization flag is used, then\n                differences in whitespace are ignored.\n                    >>> print range(30) #doctest: +NORMALIZE_WHITESPACE\n                    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n                     15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n                     27, 28, 29]\n            \"\"\",\n           }\n\n\ndef _test():\n    testfiles = [arg for arg in sys.argv[1:] if arg and arg[0] != '-']\n    if not testfiles:\n        name = os.path.basename(sys.argv[0])\n        if '__loader__' in globals():          # python -m\n            name, _ = os.path.splitext(name)\n        print(\"usage: {0} [-v] file ...\".format(name))\n        return 2\n    for filename in testfiles:\n        if filename.endswith(\".py\"):\n            # It is a module -- insert its dir into sys.path and try to\n            # import it. If it is part of a package, that possibly\n            # won't work because of package imports.\n            dirname, filename = os.path.split(filename)\n            sys.path.insert(0, dirname)\n            m = __import__(filename[:-3])\n            del sys.path[0]\n            failures, _ = testmod(m)\n        else:\n            failures, _ = testfile(filename, module_relative=False)\n        if failures:\n            return 1\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(_test())\n",
		"file_name": "doctest.py"
	},
	{
		"content": "\"\"\"\nModule difflib -- helpers for computing deltas between objects.\n\nFunction get_close_matches(word, possibilities, n=3, cutoff=0.6):\n    Use SequenceMatcher to return list of the best \"good enough\" matches.\n\nFunction context_diff(a, b):\n    For two lists of strings, return a delta in context diff format.\n\nFunction ndiff(a, b):\n    Return a delta: the difference between `a` and `b` (lists of strings).\n\nFunction restore(delta, which):\n    Return one of the two sequences that generated an ndiff delta.\n\nFunction unified_diff(a, b):\n    For two lists of strings, return a delta in unified diff format.\n\nClass SequenceMatcher:\n    A flexible class for comparing pairs of sequences of any type.\n\nClass Differ:\n    For producing human-readable deltas from sequences of lines of text.\n\nClass HtmlDiff:\n    For producing HTML side by side comparison with change highlights.\n\"\"\"\n\n__all__ = ['get_close_matches', 'ndiff', 'restore', 'SequenceMatcher',\n           'Differ','IS_CHARACTER_JUNK', 'IS_LINE_JUNK', 'context_diff',\n           'unified_diff', 'HtmlDiff', 'Match']\n\nimport heapq\nfrom collections import namedtuple as _namedtuple\nfrom functools import reduce\n\nMatch = _namedtuple('Match', 'a b size')\n\ndef _calculate_ratio(matches, length):\n    if length:\n        return 2.0 * matches / length\n    return 1.0\n\nclass SequenceMatcher:\n\n    \"\"\"\n    SequenceMatcher is a flexible class for comparing pairs of sequences of\n    any type, so long as the sequence elements are hashable.  The basic\n    algorithm predates, and is a little fancier than, an algorithm\n    published in the late 1980's by Ratcliff and Obershelp under the\n    hyperbolic name \"gestalt pattern matching\".  The basic idea is to find\n    the longest contiguous matching subsequence that contains no \"junk\"\n    elements (R-O doesn't address junk).  The same idea is then applied\n    recursively to the pieces of the sequences to the left and to the right\n    of the matching subsequence.  This does not yield minimal edit\n    sequences, but does tend to yield matches that \"look right\" to people.\n\n    SequenceMatcher tries to compute a \"human-friendly diff\" between two\n    sequences.  Unlike e.g. UNIX(tm) diff, the fundamental notion is the\n    longest *contiguous* & junk-free matching subsequence.  That's what\n    catches peoples' eyes.  The Windows(tm) windiff has another interesting\n    notion, pairing up elements that appear uniquely in each sequence.\n    That, and the method here, appear to yield more intuitive difference\n    reports than does diff.  This method appears to be the least vulnerable\n    to synching up on blocks of \"junk lines\", though (like blank lines in\n    ordinary text files, or maybe \"<P>\" lines in HTML files).  That may be\n    because this is the only method of the 3 that has a *concept* of\n    \"junk\" <wink>.\n\n    Example, comparing two strings, and considering blanks to be \"junk\":\n\n    >>> s = SequenceMatcher(lambda x: x == \" \",\n    ...                     \"private Thread currentThread;\",\n    ...                     \"private volatile Thread currentThread;\")\n    >>>\n\n    .ratio() returns a float in [0, 1], measuring the \"similarity\" of the\n    sequences.  As a rule of thumb, a .ratio() value over 0.6 means the\n    sequences are close matches:\n\n    >>> print round(s.ratio(), 3)\n    0.866\n    >>>\n\n    If you're only interested in where the sequences match,\n    .get_matching_blocks() is handy:\n\n    >>> for block in s.get_matching_blocks():\n    ...     print \"a[%d] and b[%d] match for %d elements\" % block\n    a[0] and b[0] match for 8 elements\n    a[8] and b[17] match for 21 elements\n    a[29] and b[38] match for 0 elements\n\n    Note that the last tuple returned by .get_matching_blocks() is always a\n    dummy, (len(a), len(b), 0), and this is the only case in which the last\n    tuple element (number of elements matched) is 0.\n\n    If you want to know how to change the first sequence into the second,\n    use .get_opcodes():\n\n    >>> for opcode in s.get_opcodes():\n    ...     print \"%6s a[%d:%d] b[%d:%d]\" % opcode\n     equal a[0:8] b[0:8]\n    insert a[8:8] b[8:17]\n     equal a[8:29] b[17:38]\n\n    See the Differ class for a fancy human-friendly file differencer, which\n    uses SequenceMatcher both to compare sequences of lines, and to compare\n    sequences of characters within similar (near-matching) lines.\n\n    See also function get_close_matches() in this module, which shows how\n    simple code building on SequenceMatcher can be used to do useful work.\n\n    Timing:  Basic R-O is cubic time worst case and quadratic time expected\n    case.  SequenceMatcher is quadratic time for the worst case and has\n    expected-case behavior dependent in a complicated way on how many\n    elements the sequences have in common; best case time is linear.\n\n    Methods:\n\n    __init__(isjunk=None, a='', b='')\n        Construct a SequenceMatcher.\n\n    set_seqs(a, b)\n        Set the two sequences to be compared.\n\n    set_seq1(a)\n        Set the first sequence to be compared.\n\n    set_seq2(b)\n        Set the second sequence to be compared.\n\n    find_longest_match(alo, ahi, blo, bhi)\n        Find longest matching block in a[alo:ahi] and b[blo:bhi].\n\n    get_matching_blocks()\n        Return list of triples describing matching subsequences.\n\n    get_opcodes()\n        Return list of 5-tuples describing how to turn a into b.\n\n    ratio()\n        Return a measure of the sequences' similarity (float in [0,1]).\n\n    quick_ratio()\n        Return an upper bound on .ratio() relatively quickly.\n\n    real_quick_ratio()\n        Return an upper bound on ratio() very quickly.\n    \"\"\"\n\n    def __init__(self, isjunk=None, a='', b='', autojunk=True):\n        \"\"\"Construct a SequenceMatcher.\n\n        Optional arg isjunk is None (the default), or a one-argument\n        function that takes a sequence element and returns true iff the\n        element is junk.  None is equivalent to passing \"lambda x: 0\", i.e.\n        no elements are considered to be junk.  For example, pass\n            lambda x: x in \" \\\\t\"\n        if you're comparing lines as sequences of characters, and don't\n        want to synch up on blanks or hard tabs.\n\n        Optional arg a is the first of two sequences to be compared.  By\n        default, an empty string.  The elements of a must be hashable.  See\n        also .set_seqs() and .set_seq1().\n\n        Optional arg b is the second of two sequences to be compared.  By\n        default, an empty string.  The elements of b must be hashable. See\n        also .set_seqs() and .set_seq2().\n\n        Optional arg autojunk should be set to False to disable the\n        \"automatic junk heuristic\" that treats popular elements as junk\n        (see module documentation for more information).\n        \"\"\"\n\n        # Members:\n        # a\n        #      first sequence\n        # b\n        #      second sequence; differences are computed as \"what do\n        #      we need to do to 'a' to change it into 'b'?\"\n        # b2j\n        #      for x in b, b2j[x] is a list of the indices (into b)\n        #      at which x appears; junk elements do not appear\n        # fullbcount\n        #      for x in b, fullbcount[x] == the number of times x\n        #      appears in b; only materialized if really needed (used\n        #      only for computing quick_ratio())\n        # matching_blocks\n        #      a list of (i, j, k) triples, where a[i:i+k] == b[j:j+k];\n        #      ascending & non-overlapping in i and in j; terminated by\n        #      a dummy (len(a), len(b), 0) sentinel\n        # opcodes\n        #      a list of (tag, i1, i2, j1, j2) tuples, where tag is\n        #      one of\n        #          'replace'   a[i1:i2] should be replaced by b[j1:j2]\n        #          'delete'    a[i1:i2] should be deleted\n        #          'insert'    b[j1:j2] should be inserted\n        #          'equal'     a[i1:i2] == b[j1:j2]\n        # isjunk\n        #      a user-supplied function taking a sequence element and\n        #      returning true iff the element is \"junk\" -- this has\n        #      subtle but helpful effects on the algorithm, which I'll\n        #      get around to writing up someday <0.9 wink>.\n        #      DON'T USE!  Only __chain_b uses this.  Use isbjunk.\n        # isbjunk\n        #      for x in b, isbjunk(x) == isjunk(x) but much faster;\n        #      it's really the __contains__ method of a hidden dict.\n        #      DOES NOT WORK for x in a!\n        # isbpopular\n        #      for x in b, isbpopular(x) is true iff b is reasonably long\n        #      (at least 200 elements) and x accounts for more than 1 + 1% of\n        #      its elements (when autojunk is enabled).\n        #      DOES NOT WORK for x in a!\n\n        self.isjunk = isjunk\n        self.a = self.b = None\n        self.autojunk = autojunk\n        self.set_seqs(a, b)\n\n    def set_seqs(self, a, b):\n        \"\"\"Set the two sequences to be compared.\n\n        >>> s = SequenceMatcher()\n        >>> s.set_seqs(\"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        \"\"\"\n\n        self.set_seq1(a)\n        self.set_seq2(b)\n\n    def set_seq1(self, a):\n        \"\"\"Set the first sequence to be compared.\n\n        The second sequence to be compared is not changed.\n\n        >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        >>> s.set_seq1(\"bcde\")\n        >>> s.ratio()\n        1.0\n        >>>\n\n        SequenceMatcher computes and caches detailed information about the\n        second sequence, so if you want to compare one sequence S against\n        many sequences, use .set_seq2(S) once and call .set_seq1(x)\n        repeatedly for each of the other sequences.\n\n        See also set_seqs() and set_seq2().\n        \"\"\"\n\n        if a is self.a:\n            return\n        self.a = a\n        self.matching_blocks = self.opcodes = None\n\n    def set_seq2(self, b):\n        \"\"\"Set the second sequence to be compared.\n\n        The first sequence to be compared is not changed.\n\n        >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        >>> s.set_seq2(\"abcd\")\n        >>> s.ratio()\n        1.0\n        >>>\n\n        SequenceMatcher computes and caches detailed information about the\n        second sequence, so if you want to compare one sequence S against\n        many sequences, use .set_seq2(S) once and call .set_seq1(x)\n        repeatedly for each of the other sequences.\n\n        See also set_seqs() and set_seq1().\n        \"\"\"\n\n        if b is self.b:\n            return\n        self.b = b\n        self.matching_blocks = self.opcodes = None\n        self.fullbcount = None\n        self.__chain_b()\n\n    # For each element x in b, set b2j[x] to a list of the indices in\n    # b where x appears; the indices are in increasing order; note that\n    # the number of times x appears in b is len(b2j[x]) ...\n    # when self.isjunk is defined, junk elements don't show up in this\n    # map at all, which stops the central find_longest_match method\n    # from starting any matching block at a junk element ...\n    # also creates the fast isbjunk function ...\n    # b2j also does not contain entries for \"popular\" elements, meaning\n    # elements that account for more than 1 + 1% of the total elements, and\n    # when the sequence is reasonably large (>= 200 elements); this can\n    # be viewed as an adaptive notion of semi-junk, and yields an enormous\n    # speedup when, e.g., comparing program files with hundreds of\n    # instances of \"return NULL;\" ...\n    # note that this is only called when b changes; so for cross-product\n    # kinds of matches, it's best to call set_seq2 once, then set_seq1\n    # repeatedly\n\n    def __chain_b(self):\n        # Because isjunk is a user-defined (not C) function, and we test\n        # for junk a LOT, it's important to minimize the number of calls.\n        # Before the tricks described here, __chain_b was by far the most\n        # time-consuming routine in the whole module!  If anyone sees\n        # Jim Roskind, thank him again for profile.py -- I never would\n        # have guessed that.\n        # The first trick is to build b2j ignoring the possibility\n        # of junk.  I.e., we don't call isjunk at all yet.  Throwing\n        # out the junk later is much cheaper than building b2j \"right\"\n        # from the start.\n        b = self.b\n        self.b2j = b2j = {}\n\n        for i, elt in enumerate(b):\n            indices = b2j.setdefault(elt, [])\n            indices.append(i)\n\n        # Purge junk elements\n        junk = set()\n        isjunk = self.isjunk\n        if isjunk:\n            for elt in list(b2j.keys()):  # using list() since b2j is modified\n                if isjunk(elt):\n                    junk.add(elt)\n                    del b2j[elt]\n\n        # Purge popular elements that are not junk\n        popular = set()\n        n = len(b)\n        if self.autojunk and n >= 200:\n            ntest = n // 100 + 1\n            for elt, idxs in list(b2j.items()):\n                if len(idxs) > ntest:\n                    popular.add(elt)\n                    del b2j[elt]\n\n        # Now for x in b, isjunk(x) == x in junk, but the latter is much faster.\n        # Sicne the number of *unique* junk elements is probably small, the\n        # memory burden of keeping this set alive is likely trivial compared to\n        # the size of b2j.\n        self.isbjunk = junk.__contains__\n        self.isbpopular = popular.__contains__\n\n    def find_longest_match(self, alo, ahi, blo, bhi):\n        \"\"\"Find longest matching block in a[alo:ahi] and b[blo:bhi].\n\n        If isjunk is not defined:\n\n        Return (i,j,k) such that a[i:i+k] is equal to b[j:j+k], where\n            alo <= i <= i+k <= ahi\n            blo <= j <= j+k <= bhi\n        and for all (i',j',k') meeting those conditions,\n            k >= k'\n            i <= i'\n            and if i == i', j <= j'\n\n        In other words, of all maximal matching blocks, return one that\n        starts earliest in a, and of all those maximal matching blocks that\n        start earliest in a, return the one that starts earliest in b.\n\n        >>> s = SequenceMatcher(None, \" abcd\", \"abcd abcd\")\n        >>> s.find_longest_match(0, 5, 0, 9)\n        Match(a=0, b=4, size=5)\n\n        If isjunk is defined, first the longest matching block is\n        determined as above, but with the additional restriction that no\n        junk element appears in the block.  Then that block is extended as\n        far as possible by matching (only) junk elements on both sides.  So\n        the resulting block never matches on junk except as identical junk\n        happens to be adjacent to an \"interesting\" match.\n\n        Here's the same example as before, but considering blanks to be\n        junk.  That prevents \" abcd\" from matching the \" abcd\" at the tail\n        end of the second sequence directly.  Instead only the \"abcd\" can\n        match, and matches the leftmost \"abcd\" in the second sequence:\n\n        >>> s = SequenceMatcher(lambda x: x==\" \", \" abcd\", \"abcd abcd\")\n        >>> s.find_longest_match(0, 5, 0, 9)\n        Match(a=1, b=0, size=4)\n\n        If no blocks match, return (alo, blo, 0).\n\n        >>> s = SequenceMatcher(None, \"ab\", \"c\")\n        >>> s.find_longest_match(0, 2, 0, 1)\n        Match(a=0, b=0, size=0)\n        \"\"\"\n\n        # CAUTION:  stripping common prefix or suffix would be incorrect.\n        # E.g.,\n        #    ab\n        #    acab\n        # Longest matching block is \"ab\", but if common prefix is\n        # stripped, it's \"a\" (tied with \"b\").  UNIX(tm) diff does so\n        # strip, so ends up claiming that ab is changed to acab by\n        # inserting \"ca\" in the middle.  That's minimal but unintuitive:\n        # \"it's obvious\" that someone inserted \"ac\" at the front.\n        # Windiff ends up at the same place as diff, but by pairing up\n        # the unique 'b's and then matching the first two 'a's.\n\n        a, b, b2j, isbjunk = self.a, self.b, self.b2j, self.isbjunk\n        besti, bestj, bestsize = alo, blo, 0\n        # find longest junk-free match\n        # during an iteration of the loop, j2len[j] = length of longest\n        # junk-free match ending with a[i-1] and b[j]\n        j2len = {}\n        nothing = []\n        for i in xrange(alo, ahi):\n            # look at all instances of a[i] in b; note that because\n            # b2j has no junk keys, the loop is skipped if a[i] is junk\n            j2lenget = j2len.get\n            newj2len = {}\n            for j in b2j.get(a[i], nothing):\n                # a[i] matches b[j]\n                if j < blo:\n                    continue\n                if j >= bhi:\n                    break\n                k = newj2len[j] = j2lenget(j-1, 0) + 1\n                if k > bestsize:\n                    besti, bestj, bestsize = i-k+1, j-k+1, k\n            j2len = newj2len\n\n        # Extend the best by non-junk elements on each end.  In particular,\n        # \"popular\" non-junk elements aren't in b2j, which greatly speeds\n        # the inner loop above, but also means \"the best\" match so far\n        # doesn't contain any junk *or* popular non-junk elements.\n        while besti > alo and bestj > blo and \\\n              not isbjunk(b[bestj-1]) and \\\n              a[besti-1] == b[bestj-1]:\n            besti, bestj, bestsize = besti-1, bestj-1, bestsize+1\n        while besti+bestsize < ahi and bestj+bestsize < bhi and \\\n              not isbjunk(b[bestj+bestsize]) and \\\n              a[besti+bestsize] == b[bestj+bestsize]:\n            bestsize += 1\n\n        # Now that we have a wholly interesting match (albeit possibly\n        # empty!), we may as well suck up the matching junk on each\n        # side of it too.  Can't think of a good reason not to, and it\n        # saves post-processing the (possibly considerable) expense of\n        # figuring out what to do with it.  In the case of an empty\n        # interesting match, this is clearly the right thing to do,\n        # because no other kind of match is possible in the regions.\n        while besti > alo and bestj > blo and \\\n              isbjunk(b[bestj-1]) and \\\n              a[besti-1] == b[bestj-1]:\n            besti, bestj, bestsize = besti-1, bestj-1, bestsize+1\n        while besti+bestsize < ahi and bestj+bestsize < bhi and \\\n              isbjunk(b[bestj+bestsize]) and \\\n              a[besti+bestsize] == b[bestj+bestsize]:\n            bestsize = bestsize + 1\n\n        return Match(besti, bestj, bestsize)\n\n    def get_matching_blocks(self):\n        \"\"\"Return list of triples describing matching subsequences.\n\n        Each triple is of the form (i, j, n), and means that\n        a[i:i+n] == b[j:j+n].  The triples are monotonically increasing in\n        i and in j.  New in Python 2.5, it's also guaranteed that if\n        (i, j, n) and (i', j', n') are adjacent triples in the list, and\n        the second is not the last triple in the list, then i+n != i' or\n        j+n != j'.  IOW, adjacent triples never describe adjacent equal\n        blocks.\n\n        The last triple is a dummy, (len(a), len(b), 0), and is the only\n        triple with n==0.\n\n        >>> s = SequenceMatcher(None, \"abxcd\", \"abcd\")\n        >>> s.get_matching_blocks()\n        [Match(a=0, b=0, size=2), Match(a=3, b=2, size=2), Match(a=5, b=4, size=0)]\n        \"\"\"\n\n        if self.matching_blocks is not None:\n            return self.matching_blocks\n        la, lb = len(self.a), len(self.b)\n\n        # This is most naturally expressed as a recursive algorithm, but\n        # at least one user bumped into extreme use cases that exceeded\n        # the recursion limit on their box.  So, now we maintain a list\n        # ('queue`) of blocks we still need to look at, and append partial\n        # results to `matching_blocks` in a loop; the matches are sorted\n        # at the end.\n        queue = [(0, la, 0, lb)]\n        matching_blocks = []\n        while queue:\n            alo, ahi, blo, bhi = queue.pop()\n            i, j, k = x = self.find_longest_match(alo, ahi, blo, bhi)\n            # a[alo:i] vs b[blo:j] unknown\n            # a[i:i+k] same as b[j:j+k]\n            # a[i+k:ahi] vs b[j+k:bhi] unknown\n            if k:   # if k is 0, there was no matching block\n                matching_blocks.append(x)\n                if alo < i and blo < j:\n                    queue.append((alo, i, blo, j))\n                if i+k < ahi and j+k < bhi:\n                    queue.append((i+k, ahi, j+k, bhi))\n        matching_blocks.sort()\n\n        # It's possible that we have adjacent equal blocks in the\n        # matching_blocks list now.  Starting with 2.5, this code was added\n        # to collapse them.\n        i1 = j1 = k1 = 0\n        non_adjacent = []\n        for i2, j2, k2 in matching_blocks:\n            # Is this block adjacent to i1, j1, k1?\n            if i1 + k1 == i2 and j1 + k1 == j2:\n                # Yes, so collapse them -- this just increases the length of\n                # the first block by the length of the second, and the first\n                # block so lengthened remains the block to compare against.\n                k1 += k2\n            else:\n                # Not adjacent.  Remember the first block (k1==0 means it's\n                # the dummy we started with), and make the second block the\n                # new block to compare against.\n                if k1:\n                    non_adjacent.append((i1, j1, k1))\n                i1, j1, k1 = i2, j2, k2\n        if k1:\n            non_adjacent.append((i1, j1, k1))\n\n        non_adjacent.append( (la, lb, 0) )\n        self.matching_blocks = map(Match._make, non_adjacent)\n        return self.matching_blocks\n\n    def get_opcodes(self):\n        \"\"\"Return list of 5-tuples describing how to turn a into b.\n\n        Each tuple is of the form (tag, i1, i2, j1, j2).  The first tuple\n        has i1 == j1 == 0, and remaining tuples have i1 == the i2 from the\n        tuple preceding it, and likewise for j1 == the previous j2.\n\n        The tags are strings, with these meanings:\n\n        'replace':  a[i1:i2] should be replaced by b[j1:j2]\n        'delete':   a[i1:i2] should be deleted.\n                    Note that j1==j2 in this case.\n        'insert':   b[j1:j2] should be inserted at a[i1:i1].\n                    Note that i1==i2 in this case.\n        'equal':    a[i1:i2] == b[j1:j2]\n\n        >>> a = \"qabxcd\"\n        >>> b = \"abycdf\"\n        >>> s = SequenceMatcher(None, a, b)\n        >>> for tag, i1, i2, j1, j2 in s.get_opcodes():\n        ...    print (\"%7s a[%d:%d] (%s) b[%d:%d] (%s)\" %\n        ...           (tag, i1, i2, a[i1:i2], j1, j2, b[j1:j2]))\n         delete a[0:1] (q) b[0:0] ()\n          equal a[1:3] (ab) b[0:2] (ab)\n        replace a[3:4] (x) b[2:3] (y)\n          equal a[4:6] (cd) b[3:5] (cd)\n         insert a[6:6] () b[5:6] (f)\n        \"\"\"\n\n        if self.opcodes is not None:\n            return self.opcodes\n        i = j = 0\n        self.opcodes = answer = []\n        for ai, bj, size in self.get_matching_blocks():\n            # invariant:  we've pumped out correct diffs to change\n            # a[:i] into b[:j], and the next matching block is\n            # a[ai:ai+size] == b[bj:bj+size].  So we need to pump\n            # out a diff to change a[i:ai] into b[j:bj], pump out\n            # the matching block, and move (i,j) beyond the match\n            tag = ''\n            if i < ai and j < bj:\n                tag = 'replace'\n            elif i < ai:\n                tag = 'delete'\n            elif j < bj:\n                tag = 'insert'\n            if tag:\n                answer.append( (tag, i, ai, j, bj) )\n            i, j = ai+size, bj+size\n            # the list of matching blocks is terminated by a\n            # sentinel with size 0\n            if size:\n                answer.append( ('equal', ai, i, bj, j) )\n        return answer\n\n    def get_grouped_opcodes(self, n=3):\n        \"\"\" Isolate change clusters by eliminating ranges with no changes.\n\n        Return a generator of groups with up to n lines of context.\n        Each group is in the same format as returned by get_opcodes().\n\n        >>> from pprint import pprint\n        >>> a = map(str, range(1,40))\n        >>> b = a[:]\n        >>> b[8:8] = ['i']     # Make an insertion\n        >>> b[20] += 'x'       # Make a replacement\n        >>> b[23:28] = []      # Make a deletion\n        >>> b[30] += 'y'       # Make another replacement\n        >>> pprint(list(SequenceMatcher(None,a,b).get_grouped_opcodes()))\n        [[('equal', 5, 8, 5, 8), ('insert', 8, 8, 8, 9), ('equal', 8, 11, 9, 12)],\n         [('equal', 16, 19, 17, 20),\n          ('replace', 19, 20, 20, 21),\n          ('equal', 20, 22, 21, 23),\n          ('delete', 22, 27, 23, 23),\n          ('equal', 27, 30, 23, 26)],\n         [('equal', 31, 34, 27, 30),\n          ('replace', 34, 35, 30, 31),\n          ('equal', 35, 38, 31, 34)]]\n        \"\"\"\n\n        codes = self.get_opcodes()\n        if not codes:\n            codes = [(\"equal\", 0, 1, 0, 1)]\n        # Fixup leading and trailing groups if they show no changes.\n        if codes[0][0] == 'equal':\n            tag, i1, i2, j1, j2 = codes[0]\n            codes[0] = tag, max(i1, i2-n), i2, max(j1, j2-n), j2\n        if codes[-1][0] == 'equal':\n            tag, i1, i2, j1, j2 = codes[-1]\n            codes[-1] = tag, i1, min(i2, i1+n), j1, min(j2, j1+n)\n\n        nn = n + n\n        group = []\n        for tag, i1, i2, j1, j2 in codes:\n            # End the current group and start a new one whenever\n            # there is a large range with no changes.\n            if tag == 'equal' and i2-i1 > nn:\n                group.append((tag, i1, min(i2, i1+n), j1, min(j2, j1+n)))\n                yield group\n                group = []\n                i1, j1 = max(i1, i2-n), max(j1, j2-n)\n            group.append((tag, i1, i2, j1 ,j2))\n        if group and not (len(group)==1 and group[0][0] == 'equal'):\n            yield group\n\n    def ratio(self):\n        \"\"\"Return a measure of the sequences' similarity (float in [0,1]).\n\n        Where T is the total number of elements in both sequences, and\n        M is the number of matches, this is 2.0*M / T.\n        Note that this is 1 if the sequences are identical, and 0 if\n        they have nothing in common.\n\n        .ratio() is expensive to compute if you haven't already computed\n        .get_matching_blocks() or .get_opcodes(), in which case you may\n        want to try .quick_ratio() or .real_quick_ratio() first to get an\n        upper bound.\n\n        >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        >>> s.quick_ratio()\n        0.75\n        >>> s.real_quick_ratio()\n        1.0\n        \"\"\"\n\n        matches = reduce(lambda sum, triple: sum + triple[-1],\n                         self.get_matching_blocks(), 0)\n        return _calculate_ratio(matches, len(self.a) + len(self.b))\n\n    def quick_ratio(self):\n        \"\"\"Return an upper bound on ratio() relatively quickly.\n\n        This isn't defined beyond that it is an upper bound on .ratio(), and\n        is faster to compute.\n        \"\"\"\n\n        # viewing a and b as multisets, set matches to the cardinality\n        # of their intersection; this counts the number of matches\n        # without regard to order, so is clearly an upper bound\n        if self.fullbcount is None:\n            self.fullbcount = fullbcount = {}\n            for elt in self.b:\n                fullbcount[elt] = fullbcount.get(elt, 0) + 1\n        fullbcount = self.fullbcount\n        # avail[x] is the number of times x appears in 'b' less the\n        # number of times we've seen it in 'a' so far ... kinda\n        avail = {}\n        availhas, matches = avail.__contains__, 0\n        for elt in self.a:\n            if availhas(elt):\n                numb = avail[elt]\n            else:\n                numb = fullbcount.get(elt, 0)\n            avail[elt] = numb - 1\n            if numb > 0:\n                matches = matches + 1\n        return _calculate_ratio(matches, len(self.a) + len(self.b))\n\n    def real_quick_ratio(self):\n        \"\"\"Return an upper bound on ratio() very quickly.\n\n        This isn't defined beyond that it is an upper bound on .ratio(), and\n        is faster to compute than either .ratio() or .quick_ratio().\n        \"\"\"\n\n        la, lb = len(self.a), len(self.b)\n        # can't have more matches than the number of elements in the\n        # shorter sequence\n        return _calculate_ratio(min(la, lb), la + lb)\n\ndef get_close_matches(word, possibilities, n=3, cutoff=0.6):\n    \"\"\"Use SequenceMatcher to return list of the best \"good enough\" matches.\n\n    word is a sequence for which close matches are desired (typically a\n    string).\n\n    possibilities is a list of sequences against which to match word\n    (typically a list of strings).\n\n    Optional arg n (default 3) is the maximum number of close matches to\n    return.  n must be > 0.\n\n    Optional arg cutoff (default 0.6) is a float in [0, 1].  Possibilities\n    that don't score at least that similar to word are ignored.\n\n    The best (no more than n) matches among the possibilities are returned\n    in a list, sorted by similarity score, most similar first.\n\n    >>> get_close_matches(\"appel\", [\"ape\", \"apple\", \"peach\", \"puppy\"])\n    ['apple', 'ape']\n    >>> import keyword as _keyword\n    >>> get_close_matches(\"wheel\", _keyword.kwlist)\n    ['while']\n    >>> get_close_matches(\"apple\", _keyword.kwlist)\n    []\n    >>> get_close_matches(\"accept\", _keyword.kwlist)\n    ['except']\n    \"\"\"\n\n    if not n >  0:\n        raise ValueError(\"n must be > 0: %r\" % (n,))\n    if not 0.0 <= cutoff <= 1.0:\n        raise ValueError(\"cutoff must be in [0.0, 1.0]: %r\" % (cutoff,))\n    result = []\n    s = SequenceMatcher()\n    s.set_seq2(word)\n    for x in possibilities:\n        s.set_seq1(x)\n        if s.real_quick_ratio() >= cutoff and \\\n           s.quick_ratio() >= cutoff and \\\n           s.ratio() >= cutoff:\n            result.append((s.ratio(), x))\n\n    # Move the best scorers to head of list\n    result = heapq.nlargest(n, result)\n    # Strip scores for the best n matches\n    return [x for score, x in result]\n\ndef _count_leading(line, ch):\n    \"\"\"\n    Return number of `ch` characters at the start of `line`.\n\n    Example:\n\n    >>> _count_leading('   abc', ' ')\n    3\n    \"\"\"\n\n    i, n = 0, len(line)\n    while i < n and line[i] == ch:\n        i += 1\n    return i\n\nclass Differ:\n    r\"\"\"\n    Differ is a class for comparing sequences of lines of text, and\n    producing human-readable differences or deltas.  Differ uses\n    SequenceMatcher both to compare sequences of lines, and to compare\n    sequences of characters within similar (near-matching) lines.\n\n    Each line of a Differ delta begins with a two-letter code:\n\n        '- '    line unique to sequence 1\n        '+ '    line unique to sequence 2\n        '  '    line common to both sequences\n        '? '    line not present in either input sequence\n\n    Lines beginning with '? ' attempt to guide the eye to intraline\n    differences, and were not present in either input sequence.  These lines\n    can be confusing if the sequences contain tab characters.\n\n    Note that Differ makes no claim to produce a *minimal* diff.  To the\n    contrary, minimal diffs are often counter-intuitive, because they synch\n    up anywhere possible, sometimes accidental matches 100 pages apart.\n    Restricting synch points to contiguous matches preserves some notion of\n    locality, at the occasional cost of producing a longer diff.\n\n    Example: Comparing two texts.\n\n    First we set up the texts, sequences of individual single-line strings\n    ending with newlines (such sequences can also be obtained from the\n    `readlines()` method of file-like objects):\n\n    >>> text1 = '''  1. Beautiful is better than ugly.\n    ...   2. Explicit is better than implicit.\n    ...   3. Simple is better than complex.\n    ...   4. Complex is better than complicated.\n    ... '''.splitlines(1)\n    >>> len(text1)\n    4\n    >>> text1[0][-1]\n    '\\n'\n    >>> text2 = '''  1. Beautiful is better than ugly.\n    ...   3.   Simple is better than complex.\n    ...   4. Complicated is better than complex.\n    ...   5. Flat is better than nested.\n    ... '''.splitlines(1)\n\n    Next we instantiate a Differ object:\n\n    >>> d = Differ()\n\n    Note that when instantiating a Differ object we may pass functions to\n    filter out line and character 'junk'.  See Differ.__init__ for details.\n\n    Finally, we compare the two:\n\n    >>> result = list(d.compare(text1, text2))\n\n    'result' is a list of strings, so let's pretty-print it:\n\n    >>> from pprint import pprint as _pprint\n    >>> _pprint(result)\n    ['    1. Beautiful is better than ugly.\\n',\n     '-   2. Explicit is better than implicit.\\n',\n     '-   3. Simple is better than complex.\\n',\n     '+   3.   Simple is better than complex.\\n',\n     '?     ++\\n',\n     '-   4. Complex is better than complicated.\\n',\n     '?            ^                     ---- ^\\n',\n     '+   4. Complicated is better than complex.\\n',\n     '?           ++++ ^                      ^\\n',\n     '+   5. Flat is better than nested.\\n']\n\n    As a single multi-line string it looks like this:\n\n    >>> print ''.join(result),\n        1. Beautiful is better than ugly.\n    -   2. Explicit is better than implicit.\n    -   3. Simple is better than complex.\n    +   3.   Simple is better than complex.\n    ?     ++\n    -   4. Complex is better than complicated.\n    ?            ^                     ---- ^\n    +   4. Complicated is better than complex.\n    ?           ++++ ^                      ^\n    +   5. Flat is better than nested.\n\n    Methods:\n\n    __init__(linejunk=None, charjunk=None)\n        Construct a text differencer, with optional filters.\n\n    compare(a, b)\n        Compare two sequences of lines; generate the resulting delta.\n    \"\"\"\n\n    def __init__(self, linejunk=None, charjunk=None):\n        \"\"\"\n        Construct a text differencer, with optional filters.\n\n        The two optional keyword parameters are for filter functions:\n\n        - `linejunk`: A function that should accept a single string argument,\n          and return true iff the string is junk. The module-level function\n          `IS_LINE_JUNK` may be used to filter out lines without visible\n          characters, except for at most one splat ('#').  It is recommended\n          to leave linejunk None; as of Python 2.3, the underlying\n          SequenceMatcher class has grown an adaptive notion of \"noise\" lines\n          that's better than any static definition the author has ever been\n          able to craft.\n\n        - `charjunk`: A function that should accept a string of length 1. The\n          module-level function `IS_CHARACTER_JUNK` may be used to filter out\n          whitespace characters (a blank or tab; **note**: bad idea to include\n          newline in this!).  Use of IS_CHARACTER_JUNK is recommended.\n        \"\"\"\n\n        self.linejunk = linejunk\n        self.charjunk = charjunk\n\n    def compare(self, a, b):\n        r\"\"\"\n        Compare two sequences of lines; generate the resulting delta.\n\n        Each sequence must contain individual single-line strings ending with\n        newlines. Such sequences can be obtained from the `readlines()` method\n        of file-like objects.  The delta generated also consists of newline-\n        terminated strings, ready to be printed as-is via the writeline()\n        method of a file-like object.\n\n        Example:\n\n        >>> print ''.join(Differ().compare('one\\ntwo\\nthree\\n'.splitlines(1),\n        ...                                'ore\\ntree\\nemu\\n'.splitlines(1))),\n        - one\n        ?  ^\n        + ore\n        ?  ^\n        - two\n        - three\n        ?  -\n        + tree\n        + emu\n        \"\"\"\n\n        cruncher = SequenceMatcher(self.linejunk, a, b)\n        for tag, alo, ahi, blo, bhi in cruncher.get_opcodes():\n            if tag == 'replace':\n                g = self._fancy_replace(a, alo, ahi, b, blo, bhi)\n            elif tag == 'delete':\n                g = self._dump('-', a, alo, ahi)\n            elif tag == 'insert':\n                g = self._dump('+', b, blo, bhi)\n            elif tag == 'equal':\n                g = self._dump(' ', a, alo, ahi)\n            else:\n                raise ValueError, 'unknown tag %r' % (tag,)\n\n            for line in g:\n                yield line\n\n    def _dump(self, tag, x, lo, hi):\n        \"\"\"Generate comparison results for a same-tagged range.\"\"\"\n        for i in xrange(lo, hi):\n            yield '%s %s' % (tag, x[i])\n\n    def _plain_replace(self, a, alo, ahi, b, blo, bhi):\n        assert alo < ahi and blo < bhi\n        # dump the shorter block first -- reduces the burden on short-term\n        # memory if the blocks are of very different sizes\n        if bhi - blo < ahi - alo:\n            first  = self._dump('+', b, blo, bhi)\n            second = self._dump('-', a, alo, ahi)\n        else:\n            first  = self._dump('-', a, alo, ahi)\n            second = self._dump('+', b, blo, bhi)\n\n        for g in first, second:\n            for line in g:\n                yield line\n\n    def _fancy_replace(self, a, alo, ahi, b, blo, bhi):\n        r\"\"\"\n        When replacing one block of lines with another, search the blocks\n        for *similar* lines; the best-matching pair (if any) is used as a\n        synch point, and intraline difference marking is done on the\n        similar pair. Lots of work, but often worth it.\n\n        Example:\n\n        >>> d = Differ()\n        >>> results = d._fancy_replace(['abcDefghiJkl\\n'], 0, 1,\n        ...                            ['abcdefGhijkl\\n'], 0, 1)\n        >>> print ''.join(results),\n        - abcDefghiJkl\n        ?    ^  ^  ^\n        + abcdefGhijkl\n        ?    ^  ^  ^\n        \"\"\"\n\n        # don't synch up unless the lines have a similarity score of at\n        # least cutoff; best_ratio tracks the best score seen so far\n        best_ratio, cutoff = 0.74, 0.75\n        cruncher = SequenceMatcher(self.charjunk)\n        eqi, eqj = None, None   # 1st indices of equal lines (if any)\n\n        # search for the pair that matches best without being identical\n        # (identical lines must be junk lines, & we don't want to synch up\n        # on junk -- unless we have to)\n        for j in xrange(blo, bhi):\n            bj = b[j]\n            cruncher.set_seq2(bj)\n            for i in xrange(alo, ahi):\n                ai = a[i]\n                if ai == bj:\n                    if eqi is None:\n                        eqi, eqj = i, j\n                    continue\n                cruncher.set_seq1(ai)\n                # computing similarity is expensive, so use the quick\n                # upper bounds first -- have seen this speed up messy\n                # compares by a factor of 3.\n                # note that ratio() is only expensive to compute the first\n                # time it's called on a sequence pair; the expensive part\n                # of the computation is cached by cruncher\n                if cruncher.real_quick_ratio() > best_ratio and \\\n                      cruncher.quick_ratio() > best_ratio and \\\n                      cruncher.ratio() > best_ratio:\n                    best_ratio, best_i, best_j = cruncher.ratio(), i, j\n        if best_ratio < cutoff:\n            # no non-identical \"pretty close\" pair\n            if eqi is None:\n                # no identical pair either -- treat it as a straight replace\n                for line in self._plain_replace(a, alo, ahi, b, blo, bhi):\n                    yield line\n                return\n            # no close pair, but an identical pair -- synch up on that\n            best_i, best_j, best_ratio = eqi, eqj, 1.0\n        else:\n            # there's a close pair, so forget the identical pair (if any)\n            eqi = None\n\n        # a[best_i] very similar to b[best_j]; eqi is None iff they're not\n        # identical\n\n        # pump out diffs from before the synch point\n        for line in self._fancy_helper(a, alo, best_i, b, blo, best_j):\n            yield line\n\n        # do intraline marking on the synch pair\n        aelt, belt = a[best_i], b[best_j]\n        if eqi is None:\n            # pump out a '-', '?', '+', '?' quad for the synched lines\n            atags = btags = \"\"\n            cruncher.set_seqs(aelt, belt)\n            for tag, ai1, ai2, bj1, bj2 in cruncher.get_opcodes():\n                la, lb = ai2 - ai1, bj2 - bj1\n                if tag == 'replace':\n                    atags += '^' * la\n                    btags += '^' * lb\n                elif tag == 'delete':\n                    atags += '-' * la\n                elif tag == 'insert':\n                    btags += '+' * lb\n                elif tag == 'equal':\n                    atags += ' ' * la\n                    btags += ' ' * lb\n                else:\n                    raise ValueError, 'unknown tag %r' % (tag,)\n            for line in self._qformat(aelt, belt, atags, btags):\n                yield line\n        else:\n            # the synch pair is identical\n            yield '  ' + aelt\n\n        # pump out diffs from after the synch point\n        for line in self._fancy_helper(a, best_i+1, ahi, b, best_j+1, bhi):\n            yield line\n\n    def _fancy_helper(self, a, alo, ahi, b, blo, bhi):\n        g = []\n        if alo < ahi:\n            if blo < bhi:\n                g = self._fancy_replace(a, alo, ahi, b, blo, bhi)\n            else:\n                g = self._dump('-', a, alo, ahi)\n        elif blo < bhi:\n            g = self._dump('+', b, blo, bhi)\n\n        for line in g:\n            yield line\n\n    def _qformat(self, aline, bline, atags, btags):\n        r\"\"\"\n        Format \"?\" output and deal with leading tabs.\n\n        Example:\n\n        >>> d = Differ()\n        >>> results = d._qformat('\\tabcDefghiJkl\\n', '\\tabcdefGhijkl\\n',\n        ...                      '  ^ ^  ^      ', '  ^ ^  ^      ')\n        >>> for line in results: print repr(line)\n        ...\n        '- \\tabcDefghiJkl\\n'\n        '? \\t ^ ^  ^\\n'\n        '+ \\tabcdefGhijkl\\n'\n        '? \\t ^ ^  ^\\n'\n        \"\"\"\n\n        # Can hurt, but will probably help most of the time.\n        common = min(_count_leading(aline, \"\\t\"),\n                     _count_leading(bline, \"\\t\"))\n        common = min(common, _count_leading(atags[:common], \" \"))\n        common = min(common, _count_leading(btags[:common], \" \"))\n        atags = atags[common:].rstrip()\n        btags = btags[common:].rstrip()\n\n        yield \"- \" + aline\n        if atags:\n            yield \"? %s%s\\n\" % (\"\\t\" * common, atags)\n\n        yield \"+ \" + bline\n        if btags:\n            yield \"? %s%s\\n\" % (\"\\t\" * common, btags)\n\n# With respect to junk, an earlier version of ndiff simply refused to\n# *start* a match with a junk element.  The result was cases like this:\n#     before: private Thread currentThread;\n#     after:  private volatile Thread currentThread;\n# If you consider whitespace to be junk, the longest contiguous match\n# not starting with junk is \"e Thread currentThread\".  So ndiff reported\n# that \"e volatil\" was inserted between the 't' and the 'e' in \"private\".\n# While an accurate view, to people that's absurd.  The current version\n# looks for matching blocks that are entirely junk-free, then extends the\n# longest one of those as far as possible but only with matching junk.\n# So now \"currentThread\" is matched, then extended to suck up the\n# preceding blank; then \"private\" is matched, and extended to suck up the\n# following blank; then \"Thread\" is matched; and finally ndiff reports\n# that \"volatile \" was inserted before \"Thread\".  The only quibble\n# remaining is that perhaps it was really the case that \" volatile\"\n# was inserted after \"private\".  I can live with that <wink>.\n\nimport re\n\ndef IS_LINE_JUNK(line, pat=re.compile(r\"\\s*#?\\s*$\").match):\n    r\"\"\"\n    Return 1 for ignorable line: iff `line` is blank or contains a single '#'.\n\n    Examples:\n\n    >>> IS_LINE_JUNK('\\n')\n    True\n    >>> IS_LINE_JUNK('  #   \\n')\n    True\n    >>> IS_LINE_JUNK('hello\\n')\n    False\n    \"\"\"\n\n    return pat(line) is not None\n\ndef IS_CHARACTER_JUNK(ch, ws=\" \\t\"):\n    r\"\"\"\n    Return 1 for ignorable character: iff `ch` is a space or tab.\n\n    Examples:\n\n    >>> IS_CHARACTER_JUNK(' ')\n    True\n    >>> IS_CHARACTER_JUNK('\\t')\n    True\n    >>> IS_CHARACTER_JUNK('\\n')\n    False\n    >>> IS_CHARACTER_JUNK('x')\n    False\n    \"\"\"\n\n    return ch in ws\n\n\n########################################################################\n###  Unified Diff\n########################################################################\n\ndef _format_range_unified(start, stop):\n    'Convert range to the \"ed\" format'\n    # Per the diff spec at http://www.unix.org/single_unix_specification/\n    beginning = start + 1     # lines start numbering with one\n    length = stop - start\n    if length == 1:\n        return '{}'.format(beginning)\n    if not length:\n        beginning -= 1        # empty ranges begin at line just before the range\n    return '{},{}'.format(beginning, length)\n\ndef unified_diff(a, b, fromfile='', tofile='', fromfiledate='',\n                 tofiledate='', n=3, lineterm='\\n'):\n    r\"\"\"\n    Compare two sequences of lines; generate the delta as a unified diff.\n\n    Unified diffs are a compact way of showing line changes and a few\n    lines of context.  The number of context lines is set by 'n' which\n    defaults to three.\n\n    By default, the diff control lines (those with ---, +++, or @@) are\n    created with a trailing newline.  This is helpful so that inputs\n    created from file.readlines() result in diffs that are suitable for\n    file.writelines() since both the inputs and outputs have trailing\n    newlines.\n\n    For inputs that do not have trailing newlines, set the lineterm\n    argument to \"\" so that the output will be uniformly newline free.\n\n    The unidiff format normally has a header for filenames and modification\n    times.  Any or all of these may be specified using strings for\n    'fromfile', 'tofile', 'fromfiledate', and 'tofiledate'.\n    The modification times are normally expressed in the ISO 8601 format.\n\n    Example:\n\n    >>> for line in unified_diff('one two three four'.split(),\n    ...             'zero one tree four'.split(), 'Original', 'Current',\n    ...             '2005-01-26 23:30:50', '2010-04-02 10:20:52',\n    ...             lineterm=''):\n    ...     print line                  # doctest: +NORMALIZE_WHITESPACE\n    --- Original        2005-01-26 23:30:50\n    +++ Current         2010-04-02 10:20:52\n    @@ -1,4 +1,4 @@\n    +zero\n     one\n    -two\n    -three\n    +tree\n     four\n    \"\"\"\n\n    started = False\n    for group in SequenceMatcher(None,a,b).get_grouped_opcodes(n):\n        if not started:\n            started = True\n            fromdate = '\\t{}'.format(fromfiledate) if fromfiledate else ''\n            todate = '\\t{}'.format(tofiledate) if tofiledate else ''\n            yield '--- {}{}{}'.format(fromfile, fromdate, lineterm)\n            yield '+++ {}{}{}'.format(tofile, todate, lineterm)\n\n        first, last = group[0], group[-1]\n        file1_range = _format_range_unified(first[1], last[2])\n        file2_range = _format_range_unified(first[3], last[4])\n        yield '@@ -{} +{} @@{}'.format(file1_range, file2_range, lineterm)\n\n        for tag, i1, i2, j1, j2 in group:\n            if tag == 'equal':\n                for line in a[i1:i2]:\n                    yield ' ' + line\n                continue\n            if tag in ('replace', 'delete'):\n                for line in a[i1:i2]:\n                    yield '-' + line\n            if tag in ('replace', 'insert'):\n                for line in b[j1:j2]:\n                    yield '+' + line\n\n\n########################################################################\n###  Context Diff\n########################################################################\n\ndef _format_range_context(start, stop):\n    'Convert range to the \"ed\" format'\n    # Per the diff spec at http://www.unix.org/single_unix_specification/\n    beginning = start + 1     # lines start numbering with one\n    length = stop - start\n    if not length:\n        beginning -= 1        # empty ranges begin at line just before the range\n    if length <= 1:\n        return '{}'.format(beginning)\n    return '{},{}'.format(beginning, beginning + length - 1)\n\n# See http://www.unix.org/single_unix_specification/\ndef context_diff(a, b, fromfile='', tofile='',\n                 fromfiledate='', tofiledate='', n=3, lineterm='\\n'):\n    r\"\"\"\n    Compare two sequences of lines; generate the delta as a context diff.\n\n    Context diffs are a compact way of showing line changes and a few\n    lines of context.  The number of context lines is set by 'n' which\n    defaults to three.\n\n    By default, the diff control lines (those with *** or ---) are\n    created with a trailing newline.  This is helpful so that inputs\n    created from file.readlines() result in diffs that are suitable for\n    file.writelines() since both the inputs and outputs have trailing\n    newlines.\n\n    For inputs that do not have trailing newlines, set the lineterm\n    argument to \"\" so that the output will be uniformly newline free.\n\n    The context diff format normally has a header for filenames and\n    modification times.  Any or all of these may be specified using\n    strings for 'fromfile', 'tofile', 'fromfiledate', and 'tofiledate'.\n    The modification times are normally expressed in the ISO 8601 format.\n    If not specified, the strings default to blanks.\n\n    Example:\n\n    >>> print ''.join(context_diff('one\\ntwo\\nthree\\nfour\\n'.splitlines(1),\n    ...       'zero\\none\\ntree\\nfour\\n'.splitlines(1), 'Original', 'Current')),\n    *** Original\n    --- Current\n    ***************\n    *** 1,4 ****\n      one\n    ! two\n    ! three\n      four\n    --- 1,4 ----\n    + zero\n      one\n    ! tree\n      four\n    \"\"\"\n\n    prefix = dict(insert='+ ', delete='- ', replace='! ', equal='  ')\n    started = False\n    for group in SequenceMatcher(None,a,b).get_grouped_opcodes(n):\n        if not started:\n            started = True\n            fromdate = '\\t{}'.format(fromfiledate) if fromfiledate else ''\n            todate = '\\t{}'.format(tofiledate) if tofiledate else ''\n            yield '*** {}{}{}'.format(fromfile, fromdate, lineterm)\n            yield '--- {}{}{}'.format(tofile, todate, lineterm)\n\n        first, last = group[0], group[-1]\n        yield '***************' + lineterm\n\n        file1_range = _format_range_context(first[1], last[2])\n        yield '*** {} ****{}'.format(file1_range, lineterm)\n\n        if any(tag in ('replace', 'delete') for tag, _, _, _, _ in group):\n            for tag, i1, i2, _, _ in group:\n                if tag != 'insert':\n                    for line in a[i1:i2]:\n                        yield prefix[tag] + line\n\n        file2_range = _format_range_context(first[3], last[4])\n        yield '--- {} ----{}'.format(file2_range, lineterm)\n\n        if any(tag in ('replace', 'insert') for tag, _, _, _, _ in group):\n            for tag, _, _, j1, j2 in group:\n                if tag != 'delete':\n                    for line in b[j1:j2]:\n                        yield prefix[tag] + line\n\ndef ndiff(a, b, linejunk=None, charjunk=IS_CHARACTER_JUNK):\n    r\"\"\"\n    Compare `a` and `b` (lists of strings); return a `Differ`-style delta.\n\n    Optional keyword parameters `linejunk` and `charjunk` are for filter\n    functions (or None):\n\n    - linejunk: A function that should accept a single string argument, and\n      return true iff the string is junk.  The default is None, and is\n      recommended; as of Python 2.3, an adaptive notion of \"noise\" lines is\n      used that does a good job on its own.\n\n    - charjunk: A function that should accept a string of length 1. The\n      default is module-level function IS_CHARACTER_JUNK, which filters out\n      whitespace characters (a blank or tab; note: bad idea to include newline\n      in this!).\n\n    Tools/scripts/ndiff.py is a command-line front-end to this function.\n\n    Example:\n\n    >>> diff = ndiff('one\\ntwo\\nthree\\n'.splitlines(1),\n    ...              'ore\\ntree\\nemu\\n'.splitlines(1))\n    >>> print ''.join(diff),\n    - one\n    ?  ^\n    + ore\n    ?  ^\n    - two\n    - three\n    ?  -\n    + tree\n    + emu\n    \"\"\"\n    return Differ(linejunk, charjunk).compare(a, b)\n\ndef _mdiff(fromlines, tolines, context=None, linejunk=None,\n           charjunk=IS_CHARACTER_JUNK):\n    r\"\"\"Returns generator yielding marked up from/to side by side differences.\n\n    Arguments:\n    fromlines -- list of text lines to compared to tolines\n    tolines -- list of text lines to be compared to fromlines\n    context -- number of context lines to display on each side of difference,\n               if None, all from/to text lines will be generated.\n    linejunk -- passed on to ndiff (see ndiff documentation)\n    charjunk -- passed on to ndiff (see ndiff documentation)\n\n    This function returns an iterator which returns a tuple:\n    (from line tuple, to line tuple, boolean flag)\n\n    from/to line tuple -- (line num, line text)\n        line num -- integer or None (to indicate a context separation)\n        line text -- original line text with following markers inserted:\n            '\\0+' -- marks start of added text\n            '\\0-' -- marks start of deleted text\n            '\\0^' -- marks start of changed text\n            '\\1' -- marks end of added/deleted/changed text\n\n    boolean flag -- None indicates context separation, True indicates\n        either \"from\" or \"to\" line contains a change, otherwise False.\n\n    This function/iterator was originally developed to generate side by side\n    file difference for making HTML pages (see HtmlDiff class for example\n    usage).\n\n    Note, this function utilizes the ndiff function to generate the side by\n    side difference markup.  Optional ndiff arguments may be passed to this\n    function and they in turn will be passed to ndiff.\n    \"\"\"\n    import re\n\n    # regular expression for finding intraline change indices\n    change_re = re.compile('(\\++|\\-+|\\^+)')\n\n    # create the difference iterator to generate the differences\n    diff_lines_iterator = ndiff(fromlines,tolines,linejunk,charjunk)\n\n    def _make_line(lines, format_key, side, num_lines=[0,0]):\n        \"\"\"Returns line of text with user's change markup and line formatting.\n\n        lines -- list of lines from the ndiff generator to produce a line of\n                 text from.  When producing the line of text to return, the\n                 lines used are removed from this list.\n        format_key -- '+' return first line in list with \"add\" markup around\n                          the entire line.\n                      '-' return first line in list with \"delete\" markup around\n                          the entire line.\n                      '?' return first line in list with add/delete/change\n                          intraline markup (indices obtained from second line)\n                      None return first line in list with no markup\n        side -- indice into the num_lines list (0=from,1=to)\n        num_lines -- from/to current line number.  This is NOT intended to be a\n                     passed parameter.  It is present as a keyword argument to\n                     maintain memory of the current line numbers between calls\n                     of this function.\n\n        Note, this function is purposefully not defined at the module scope so\n        that data it needs from its parent function (within whose context it\n        is defined) does not need to be of module scope.\n        \"\"\"\n        num_lines[side] += 1\n        # Handle case where no user markup is to be added, just return line of\n        # text with user's line format to allow for usage of the line number.\n        if format_key is None:\n            return (num_lines[side],lines.pop(0)[2:])\n        # Handle case of intraline changes\n        if format_key == '?':\n            text, markers = lines.pop(0), lines.pop(0)\n            # find intraline changes (store change type and indices in tuples)\n            sub_info = []\n            def record_sub_info(match_object,sub_info=sub_info):\n                sub_info.append([match_object.group(1)[0],match_object.span()])\n                return match_object.group(1)\n            change_re.sub(record_sub_info,markers)\n            # process each tuple inserting our special marks that won't be\n            # noticed by an xml/html escaper.\n            for key,(begin,end) in sub_info[::-1]:\n                text = text[0:begin]+'\\0'+key+text[begin:end]+'\\1'+text[end:]\n            text = text[2:]\n        # Handle case of add/delete entire line\n        else:\n            text = lines.pop(0)[2:]\n            # if line of text is just a newline, insert a space so there is\n            # something for the user to highlight and see.\n            if not text:\n                text = ' '\n            # insert marks that won't be noticed by an xml/html escaper.\n            text = '\\0' + format_key + text + '\\1'\n        # Return line of text, first allow user's line formatter to do its\n        # thing (such as adding the line number) then replace the special\n        # marks with what the user's change markup.\n        return (num_lines[side],text)\n\n    def _line_iterator():\n        \"\"\"Yields from/to lines of text with a change indication.\n\n        This function is an iterator.  It itself pulls lines from a\n        differencing iterator, processes them and yields them.  When it can\n        it yields both a \"from\" and a \"to\" line, otherwise it will yield one\n        or the other.  In addition to yielding the lines of from/to text, a\n        boolean flag is yielded to indicate if the text line(s) have\n        differences in them.\n\n        Note, this function is purposefully not defined at the module scope so\n        that data it needs from its parent function (within whose context it\n        is defined) does not need to be of module scope.\n        \"\"\"\n        lines = []\n        num_blanks_pending, num_blanks_to_yield = 0, 0\n        while True:\n            # Load up next 4 lines so we can look ahead, create strings which\n            # are a concatenation of the first character of each of the 4 lines\n            # so we can do some very readable comparisons.\n            while len(lines) < 4:\n                try:\n                    lines.append(diff_lines_iterator.next())\n                except StopIteration:\n                    lines.append('X')\n            s = ''.join([line[0] for line in lines])\n            if s.startswith('X'):\n                # When no more lines, pump out any remaining blank lines so the\n                # corresponding add/delete lines get a matching blank line so\n                # all line pairs get yielded at the next level.\n                num_blanks_to_yield = num_blanks_pending\n            elif s.startswith('-?+?'):\n                # simple intraline change\n                yield _make_line(lines,'?',0), _make_line(lines,'?',1), True\n                continue\n            elif s.startswith('--++'):\n                # in delete block, add block coming: we do NOT want to get\n                # caught up on blank lines yet, just process the delete line\n                num_blanks_pending -= 1\n                yield _make_line(lines,'-',0), None, True\n                continue\n            elif s.startswith(('--?+', '--+', '- ')):\n                # in delete block and see a intraline change or unchanged line\n                # coming: yield the delete line and then blanks\n                from_line,to_line = _make_line(lines,'-',0), None\n                num_blanks_to_yield,num_blanks_pending = num_blanks_pending-1,0\n            elif s.startswith('-+?'):\n                # intraline change\n                yield _make_line(lines,None,0), _make_line(lines,'?',1), True\n                continue\n            elif s.startswith('-?+'):\n                # intraline change\n                yield _make_line(lines,'?',0), _make_line(lines,None,1), True\n                continue\n            elif s.startswith('-'):\n                # delete FROM line\n                num_blanks_pending -= 1\n                yield _make_line(lines,'-',0), None, True\n                continue\n            elif s.startswith('+--'):\n                # in add block, delete block coming: we do NOT want to get\n                # caught up on blank lines yet, just process the add line\n                num_blanks_pending += 1\n                yield None, _make_line(lines,'+',1), True\n                continue\n            elif s.startswith(('+ ', '+-')):\n                # will be leaving an add block: yield blanks then add line\n                from_line, to_line = None, _make_line(lines,'+',1)\n                num_blanks_to_yield,num_blanks_pending = num_blanks_pending+1,0\n            elif s.startswith('+'):\n                # inside an add block, yield the add line\n                num_blanks_pending += 1\n                yield None, _make_line(lines,'+',1), True\n                continue\n            elif s.startswith(' '):\n                # unchanged text, yield it to both sides\n                yield _make_line(lines[:],None,0),_make_line(lines,None,1),False\n                continue\n            # Catch up on the blank lines so when we yield the next from/to\n            # pair, they are lined up.\n            while(num_blanks_to_yield < 0):\n                num_blanks_to_yield += 1\n                yield None,('','\\n'),True\n            while(num_blanks_to_yield > 0):\n                num_blanks_to_yield -= 1\n                yield ('','\\n'),None,True\n            if s.startswith('X'):\n                raise StopIteration\n            else:\n                yield from_line,to_line,True\n\n    def _line_pair_iterator():\n        \"\"\"Yields from/to lines of text with a change indication.\n\n        This function is an iterator.  It itself pulls lines from the line\n        iterator.  Its difference from that iterator is that this function\n        always yields a pair of from/to text lines (with the change\n        indication).  If necessary it will collect single from/to lines\n        until it has a matching pair from/to pair to yield.\n\n        Note, this function is purposefully not defined at the module scope so\n        that data it needs from its parent function (within whose context it\n        is defined) does not need to be of module scope.\n        \"\"\"\n        line_iterator = _line_iterator()\n        fromlines,tolines=[],[]\n        while True:\n            # Collecting lines of text until we have a from/to pair\n            while (len(fromlines)==0 or len(tolines)==0):\n                from_line, to_line, found_diff =line_iterator.next()\n                if from_line is not None:\n                    fromlines.append((from_line,found_diff))\n                if to_line is not None:\n                    tolines.append((to_line,found_diff))\n            # Once we have a pair, remove them from the collection and yield it\n            from_line, fromDiff = fromlines.pop(0)\n            to_line, to_diff = tolines.pop(0)\n            yield (from_line,to_line,fromDiff or to_diff)\n\n    # Handle case where user does not want context differencing, just yield\n    # them up without doing anything else with them.\n    line_pair_iterator = _line_pair_iterator()\n    if context is None:\n        while True:\n            yield line_pair_iterator.next()\n    # Handle case where user wants context differencing.  We must do some\n    # storage of lines until we know for sure that they are to be yielded.\n    else:\n        context += 1\n        lines_to_write = 0\n        while True:\n            # Store lines up until we find a difference, note use of a\n            # circular queue because we only need to keep around what\n            # we need for context.\n            index, contextLines = 0, [None]*(context)\n            found_diff = False\n            while(found_diff is False):\n                from_line, to_line, found_diff = line_pair_iterator.next()\n                i = index % context\n                contextLines[i] = (from_line, to_line, found_diff)\n                index += 1\n            # Yield lines that we have collected so far, but first yield\n            # the user's separator.\n            if index > context:\n                yield None, None, None\n                lines_to_write = context\n            else:\n                lines_to_write = index\n                index = 0\n            while(lines_to_write):\n                i = index % context\n                index += 1\n                yield contextLines[i]\n                lines_to_write -= 1\n            # Now yield the context lines after the change\n            lines_to_write = context-1\n            while(lines_to_write):\n                from_line, to_line, found_diff = line_pair_iterator.next()\n                # If another change within the context, extend the context\n                if found_diff:\n                    lines_to_write = context-1\n                else:\n                    lines_to_write -= 1\n                yield from_line, to_line, found_diff\n\n\n_file_template = \"\"\"\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n          \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n\n<html>\n\n<head>\n    <meta http-equiv=\"Content-Type\"\n          content=\"text/html; charset=ISO-8859-1\" />\n    <title></title>\n    <style type=\"text/css\">%(styles)s\n    </style>\n</head>\n\n<body>\n    %(table)s%(legend)s\n</body>\n\n</html>\"\"\"\n\n_styles = \"\"\"\n        table.diff {font-family:Courier; border:medium;}\n        .diff_header {background-color:#e0e0e0}\n        td.diff_header {text-align:right}\n        .diff_next {background-color:#c0c0c0}\n        .diff_add {background-color:#aaffaa}\n        .diff_chg {background-color:#ffff77}\n        .diff_sub {background-color:#ffaaaa}\"\"\"\n\n_table_template = \"\"\"\n    <table class=\"diff\" id=\"difflib_chg_%(prefix)s_top\"\n           cellspacing=\"0\" cellpadding=\"0\" rules=\"groups\" >\n        <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>\n        <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>\n        %(header_row)s\n        <tbody>\n%(data_rows)s        </tbody>\n    </table>\"\"\"\n\n_legend = \"\"\"\n    <table class=\"diff\" summary=\"Legends\">\n        <tr> <th colspan=\"2\"> Legends </th> </tr>\n        <tr> <td> <table border=\"\" summary=\"Colors\">\n                      <tr><th> Colors </th> </tr>\n                      <tr><td class=\"diff_add\">&nbsp;Added&nbsp;</td></tr>\n                      <tr><td class=\"diff_chg\">Changed</td> </tr>\n                      <tr><td class=\"diff_sub\">Deleted</td> </tr>\n                  </table></td>\n             <td> <table border=\"\" summary=\"Links\">\n                      <tr><th colspan=\"2\"> Links </th> </tr>\n                      <tr><td>(f)irst change</td> </tr>\n                      <tr><td>(n)ext change</td> </tr>\n                      <tr><td>(t)op</td> </tr>\n                  </table></td> </tr>\n    </table>\"\"\"\n\nclass HtmlDiff(object):\n    \"\"\"For producing HTML side by side comparison with change highlights.\n\n    This class can be used to create an HTML table (or a complete HTML file\n    containing the table) showing a side by side, line by line comparison\n    of text with inter-line and intra-line change highlights.  The table can\n    be generated in either full or contextual difference mode.\n\n    The following methods are provided for HTML generation:\n\n    make_table -- generates HTML for a single side by side table\n    make_file -- generates complete HTML file with a single side by side table\n\n    See tools/scripts/diff.py for an example usage of this class.\n    \"\"\"\n\n    _file_template = _file_template\n    _styles = _styles\n    _table_template = _table_template\n    _legend = _legend\n    _default_prefix = 0\n\n    def __init__(self,tabsize=8,wrapcolumn=None,linejunk=None,\n                 charjunk=IS_CHARACTER_JUNK):\n        \"\"\"HtmlDiff instance initializer\n\n        Arguments:\n        tabsize -- tab stop spacing, defaults to 8.\n        wrapcolumn -- column number where lines are broken and wrapped,\n            defaults to None where lines are not wrapped.\n        linejunk,charjunk -- keyword arguments passed into ndiff() (used to by\n            HtmlDiff() to generate the side by side HTML differences).  See\n            ndiff() documentation for argument default values and descriptions.\n        \"\"\"\n        self._tabsize = tabsize\n        self._wrapcolumn = wrapcolumn\n        self._linejunk = linejunk\n        self._charjunk = charjunk\n\n    def make_file(self,fromlines,tolines,fromdesc='',todesc='',context=False,\n                  numlines=5):\n        \"\"\"Returns HTML file of side by side comparison with change highlights\n\n        Arguments:\n        fromlines -- list of \"from\" lines\n        tolines -- list of \"to\" lines\n        fromdesc -- \"from\" file column header string\n        todesc -- \"to\" file column header string\n        context -- set to True for contextual differences (defaults to False\n            which shows full differences).\n        numlines -- number of context lines.  When context is set True,\n            controls number of lines displayed before and after the change.\n            When context is False, controls the number of lines to place\n            the \"next\" link anchors before the next change (so click of\n            \"next\" link jumps to just before the change).\n        \"\"\"\n\n        return self._file_template % dict(\n            styles = self._styles,\n            legend = self._legend,\n            table = self.make_table(fromlines,tolines,fromdesc,todesc,\n                                    context=context,numlines=numlines))\n\n    def _tab_newline_replace(self,fromlines,tolines):\n        \"\"\"Returns from/to line lists with tabs expanded and newlines removed.\n\n        Instead of tab characters being replaced by the number of spaces\n        needed to fill in to the next tab stop, this function will fill\n        the space with tab characters.  This is done so that the difference\n        algorithms can identify changes in a file when tabs are replaced by\n        spaces and vice versa.  At the end of the HTML generation, the tab\n        characters will be replaced with a nonbreakable space.\n        \"\"\"\n        def expand_tabs(line):\n            # hide real spaces\n            line = line.replace(' ','\\0')\n            # expand tabs into spaces\n            line = line.expandtabs(self._tabsize)\n            # replace spaces from expanded tabs back into tab characters\n            # (we'll replace them with markup after we do differencing)\n            line = line.replace(' ','\\t')\n            return line.replace('\\0',' ').rstrip('\\n')\n        fromlines = [expand_tabs(line) for line in fromlines]\n        tolines = [expand_tabs(line) for line in tolines]\n        return fromlines,tolines\n\n    def _split_line(self,data_list,line_num,text):\n        \"\"\"Builds list of text lines by splitting text lines at wrap point\n\n        This function will determine if the input text line needs to be\n        wrapped (split) into separate lines.  If so, the first wrap point\n        will be determined and the first line appended to the output\n        text line list.  This function is used recursively to handle\n        the second part of the split line to further split it.\n        \"\"\"\n        # if blank line or context separator, just add it to the output list\n        if not line_num:\n            data_list.append((line_num,text))\n            return\n\n        # if line text doesn't need wrapping, just add it to the output list\n        size = len(text)\n        max = self._wrapcolumn\n        if (size <= max) or ((size -(text.count('\\0')*3)) <= max):\n            data_list.append((line_num,text))\n            return\n\n        # scan text looking for the wrap point, keeping track if the wrap\n        # point is inside markers\n        i = 0\n        n = 0\n        mark = ''\n        while n < max and i < size:\n            if text[i] == '\\0':\n                i += 1\n                mark = text[i]\n                i += 1\n            elif text[i] == '\\1':\n                i += 1\n                mark = ''\n            else:\n                i += 1\n                n += 1\n\n        # wrap point is inside text, break it up into separate lines\n        line1 = text[:i]\n        line2 = text[i:]\n\n        # if wrap point is inside markers, place end marker at end of first\n        # line and start marker at beginning of second line because each\n        # line will have its own table tag markup around it.\n        if mark:\n            line1 = line1 + '\\1'\n            line2 = '\\0' + mark + line2\n\n        # tack on first line onto the output list\n        data_list.append((line_num,line1))\n\n        # use this routine again to wrap the remaining text\n        self._split_line(data_list,'>',line2)\n\n    def _line_wrapper(self,diffs):\n        \"\"\"Returns iterator that splits (wraps) mdiff text lines\"\"\"\n\n        # pull from/to data and flags from mdiff iterator\n        for fromdata,todata,flag in diffs:\n            # check for context separators and pass them through\n            if flag is None:\n                yield fromdata,todata,flag\n                continue\n            (fromline,fromtext),(toline,totext) = fromdata,todata\n            # for each from/to line split it at the wrap column to form\n            # list of text lines.\n            fromlist,tolist = [],[]\n            self._split_line(fromlist,fromline,fromtext)\n            self._split_line(tolist,toline,totext)\n            # yield from/to line in pairs inserting blank lines as\n            # necessary when one side has more wrapped lines\n            while fromlist or tolist:\n                if fromlist:\n                    fromdata = fromlist.pop(0)\n                else:\n                    fromdata = ('',' ')\n                if tolist:\n                    todata = tolist.pop(0)\n                else:\n                    todata = ('',' ')\n                yield fromdata,todata,flag\n\n    def _collect_lines(self,diffs):\n        \"\"\"Collects mdiff output into separate lists\n\n        Before storing the mdiff from/to data into a list, it is converted\n        into a single line of text with HTML markup.\n        \"\"\"\n\n        fromlist,tolist,flaglist = [],[],[]\n        # pull from/to data and flags from mdiff style iterator\n        for fromdata,todata,flag in diffs:\n            try:\n                # store HTML markup of the lines into the lists\n                fromlist.append(self._format_line(0,flag,*fromdata))\n                tolist.append(self._format_line(1,flag,*todata))\n            except TypeError:\n                # exceptions occur for lines where context separators go\n                fromlist.append(None)\n                tolist.append(None)\n            flaglist.append(flag)\n        return fromlist,tolist,flaglist\n\n    def _format_line(self,side,flag,linenum,text):\n        \"\"\"Returns HTML markup of \"from\" / \"to\" text lines\n\n        side -- 0 or 1 indicating \"from\" or \"to\" text\n        flag -- indicates if difference on line\n        linenum -- line number (used for line number column)\n        text -- line text to be marked up\n        \"\"\"\n        try:\n            linenum = '%d' % linenum\n            id = ' id=\"%s%s\"' % (self._prefix[side],linenum)\n        except TypeError:\n            # handle blank lines where linenum is '>' or ''\n            id = ''\n        # replace those things that would get confused with HTML symbols\n        text=text.replace(\"&\",\"&amp;\").replace(\">\",\"&gt;\").replace(\"<\",\"&lt;\")\n\n        # make space non-breakable so they don't get compressed or line wrapped\n        text = text.replace(' ','&nbsp;').rstrip()\n\n        return '<td class=\"diff_header\"%s>%s</td><td nowrap=\"nowrap\">%s</td>' \\\n               % (id,linenum,text)\n\n    def _make_prefix(self):\n        \"\"\"Create unique anchor prefixes\"\"\"\n\n        # Generate a unique anchor prefix so multiple tables\n        # can exist on the same HTML page without conflicts.\n        fromprefix = \"from%d_\" % HtmlDiff._default_prefix\n        toprefix = \"to%d_\" % HtmlDiff._default_prefix\n        HtmlDiff._default_prefix += 1\n        # store prefixes so line format method has access\n        self._prefix = [fromprefix,toprefix]\n\n    def _convert_flags(self,fromlist,tolist,flaglist,context,numlines):\n        \"\"\"Makes list of \"next\" links\"\"\"\n\n        # all anchor names will be generated using the unique \"to\" prefix\n        toprefix = self._prefix[1]\n\n        # process change flags, generating middle column of next anchors/links\n        next_id = ['']*len(flaglist)\n        next_href = ['']*len(flaglist)\n        num_chg, in_change = 0, False\n        last = 0\n        for i,flag in enumerate(flaglist):\n            if flag:\n                if not in_change:\n                    in_change = True\n                    last = i\n                    # at the beginning of a change, drop an anchor a few lines\n                    # (the context lines) before the change for the previous\n                    # link\n                    i = max([0,i-numlines])\n                    next_id[i] = ' id=\"difflib_chg_%s_%d\"' % (toprefix,num_chg)\n                    # at the beginning of a change, drop a link to the next\n                    # change\n                    num_chg += 1\n                    next_href[last] = '<a href=\"#difflib_chg_%s_%d\">n</a>' % (\n                         toprefix,num_chg)\n            else:\n                in_change = False\n        # check for cases where there is no content to avoid exceptions\n        if not flaglist:\n            flaglist = [False]\n            next_id = ['']\n            next_href = ['']\n            last = 0\n            if context:\n                fromlist = ['<td></td><td>&nbsp;No Differences Found&nbsp;</td>']\n                tolist = fromlist\n            else:\n                fromlist = tolist = ['<td></td><td>&nbsp;Empty File&nbsp;</td>']\n        # if not a change on first line, drop a link\n        if not flaglist[0]:\n            next_href[0] = '<a href=\"#difflib_chg_%s_0\">f</a>' % toprefix\n        # redo the last link to link to the top\n        next_href[last] = '<a href=\"#difflib_chg_%s_top\">t</a>' % (toprefix)\n\n        return fromlist,tolist,flaglist,next_href,next_id\n\n    def make_table(self,fromlines,tolines,fromdesc='',todesc='',context=False,\n                   numlines=5):\n        \"\"\"Returns HTML table of side by side comparison with change highlights\n\n        Arguments:\n        fromlines -- list of \"from\" lines\n        tolines -- list of \"to\" lines\n        fromdesc -- \"from\" file column header string\n        todesc -- \"to\" file column header string\n        context -- set to True for contextual differences (defaults to False\n            which shows full differences).\n        numlines -- number of context lines.  When context is set True,\n            controls number of lines displayed before and after the change.\n            When context is False, controls the number of lines to place\n            the \"next\" link anchors before the next change (so click of\n            \"next\" link jumps to just before the change).\n        \"\"\"\n\n        # make unique anchor prefixes so that multiple tables may exist\n        # on the same page without conflict.\n        self._make_prefix()\n\n        # change tabs to spaces before it gets more difficult after we insert\n        # markup\n        fromlines,tolines = self._tab_newline_replace(fromlines,tolines)\n\n        # create diffs iterator which generates side by side from/to data\n        if context:\n            context_lines = numlines\n        else:\n            context_lines = None\n        diffs = _mdiff(fromlines,tolines,context_lines,linejunk=self._linejunk,\n                      charjunk=self._charjunk)\n\n        # set up iterator to wrap lines that exceed desired width\n        if self._wrapcolumn:\n            diffs = self._line_wrapper(diffs)\n\n        # collect up from/to lines and flags into lists (also format the lines)\n        fromlist,tolist,flaglist = self._collect_lines(diffs)\n\n        # process change flags, generating middle column of next anchors/links\n        fromlist,tolist,flaglist,next_href,next_id = self._convert_flags(\n            fromlist,tolist,flaglist,context,numlines)\n\n        s = []\n        fmt = '            <tr><td class=\"diff_next\"%s>%s</td>%s' + \\\n              '<td class=\"diff_next\">%s</td>%s</tr>\\n'\n        for i in range(len(flaglist)):\n            if flaglist[i] is None:\n                # mdiff yields None on separator lines skip the bogus ones\n                # generated for the first line\n                if i > 0:\n                    s.append('        </tbody>        \\n        <tbody>\\n')\n            else:\n                s.append( fmt % (next_id[i],next_href[i],fromlist[i],\n                                           next_href[i],tolist[i]))\n        if fromdesc or todesc:\n            header_row = '<thead><tr>%s%s%s%s</tr></thead>' % (\n                '<th class=\"diff_next\"><br /></th>',\n                '<th colspan=\"2\" class=\"diff_header\">%s</th>' % fromdesc,\n                '<th class=\"diff_next\"><br /></th>',\n                '<th colspan=\"2\" class=\"diff_header\">%s</th>' % todesc)\n        else:\n            header_row = ''\n\n        table = self._table_template % dict(\n            data_rows=''.join(s),\n            header_row=header_row,\n            prefix=self._prefix[1])\n\n        return table.replace('\\0+','<span class=\"diff_add\">'). \\\n                     replace('\\0-','<span class=\"diff_sub\">'). \\\n                     replace('\\0^','<span class=\"diff_chg\">'). \\\n                     replace('\\1','</span>'). \\\n                     replace('\\t','&nbsp;')\n\ndel re\n\ndef restore(delta, which):\n    r\"\"\"\n    Generate one of the two sequences that generated a delta.\n\n    Given a `delta` produced by `Differ.compare()` or `ndiff()`, extract\n    lines originating from file 1 or 2 (parameter `which`), stripping off line\n    prefixes.\n\n    Examples:\n\n    >>> diff = ndiff('one\\ntwo\\nthree\\n'.splitlines(1),\n    ...              'ore\\ntree\\nemu\\n'.splitlines(1))\n    >>> diff = list(diff)\n    >>> print ''.join(restore(diff, 1)),\n    one\n    two\n    three\n    >>> print ''.join(restore(diff, 2)),\n    ore\n    tree\n    emu\n    \"\"\"\n    try:\n        tag = {1: \"- \", 2: \"+ \"}[int(which)]\n    except KeyError:\n        raise ValueError, ('unknown delta choice (must be 1 or 2): %r'\n                           % which)\n    prefixes = (\"  \", tag)\n    for line in delta:\n        if line[:2] in prefixes:\n            yield line[2:]\n\ndef _test():\n    import doctest, difflib\n    return doctest.testmod(difflib)\n\nif __name__ == \"__main__\":\n    _test()\n",
		"file_name": "difflib.py"
	},
	{
		"content": "# -*- coding: utf-8 -*-\n\n\"\"\"Heap queue algorithm (a.k.a. priority queue).\n\nHeaps are arrays for which a[k] <= a[2*k+1] and a[k] <= a[2*k+2] for\nall k, counting elements from 0.  For the sake of comparison,\nnon-existing elements are considered to be infinite.  The interesting\nproperty of a heap is that a[0] is always its smallest element.\n\nUsage:\n\nheap = []            # creates an empty heap\nheappush(heap, item) # pushes a new item on the heap\nitem = heappop(heap) # pops the smallest item from the heap\nitem = heap[0]       # smallest item on the heap without popping it\nheapify(x)           # transforms list into a heap, in-place, in linear time\nitem = heapreplace(heap, item) # pops and returns smallest item, and adds\n                               # new item; the heap size is unchanged\n\nOur API differs from textbook heap algorithms as follows:\n\n- We use 0-based indexing.  This makes the relationship between the\n  index for a node and the indexes for its children slightly less\n  obvious, but is more suitable since Python uses 0-based indexing.\n\n- Our heappop() method returns the smallest item, not the largest.\n\nThese two make it possible to view the heap as a regular Python list\nwithout surprises: heap[0] is the smallest item, and heap.sort()\nmaintains the heap invariant!\n\"\"\"\n\n# Original code by Kevin O'Connor, augmented by Tim Peters and Raymond Hettinger\n\n__about__ = \"\"\"Heap queues\n\n[explanation by Fran\u00e7ois Pinard]\n\nHeaps are arrays for which a[k] <= a[2*k+1] and a[k] <= a[2*k+2] for\nall k, counting elements from 0.  For the sake of comparison,\nnon-existing elements are considered to be infinite.  The interesting\nproperty of a heap is that a[0] is always its smallest element.\n\nThe strange invariant above is meant to be an efficient memory\nrepresentation for a tournament.  The numbers below are `k', not a[k]:\n\n                                   0\n\n                  1                                 2\n\n          3               4                5               6\n\n      7       8       9       10      11      12      13      14\n\n    15 16   17 18   19 20   21 22   23 24   25 26   27 28   29 30\n\n\nIn the tree above, each cell `k' is topping `2*k+1' and `2*k+2'.  In\nan usual binary tournament we see in sports, each cell is the winner\nover the two cells it tops, and we can trace the winner down the tree\nto see all opponents s/he had.  However, in many computer applications\nof such tournaments, we do not need to trace the history of a winner.\nTo be more memory efficient, when a winner is promoted, we try to\nreplace it by something else at a lower level, and the rule becomes\nthat a cell and the two cells it tops contain three different items,\nbut the top cell \"wins\" over the two topped cells.\n\nIf this heap invariant is protected at all time, index 0 is clearly\nthe overall winner.  The simplest algorithmic way to remove it and\nfind the \"next\" winner is to move some loser (let's say cell 30 in the\ndiagram above) into the 0 position, and then percolate this new 0 down\nthe tree, exchanging values, until the invariant is re-established.\nThis is clearly logarithmic on the total number of items in the tree.\nBy iterating over all items, you get an O(n ln n) sort.\n\nA nice feature of this sort is that you can efficiently insert new\nitems while the sort is going on, provided that the inserted items are\nnot \"better\" than the last 0'th element you extracted.  This is\nespecially useful in simulation contexts, where the tree holds all\nincoming events, and the \"win\" condition means the smallest scheduled\ntime.  When an event schedule other events for execution, they are\nscheduled into the future, so they can easily go into the heap.  So, a\nheap is a good structure for implementing schedulers (this is what I\nused for my MIDI sequencer :-).\n\nVarious structures for implementing schedulers have been extensively\nstudied, and heaps are good for this, as they are reasonably speedy,\nthe speed is almost constant, and the worst case is not much different\nthan the average case.  However, there are other representations which\nare more efficient overall, yet the worst cases might be terrible.\n\nHeaps are also very useful in big disk sorts.  You most probably all\nknow that a big sort implies producing \"runs\" (which are pre-sorted\nsequences, which size is usually related to the amount of CPU memory),\nfollowed by a merging passes for these runs, which merging is often\nvery cleverly organised[1].  It is very important that the initial\nsort produces the longest runs possible.  Tournaments are a good way\nto that.  If, using all the memory available to hold a tournament, you\nreplace and percolate items that happen to fit the current run, you'll\nproduce runs which are twice the size of the memory for random input,\nand much better for input fuzzily ordered.\n\nMoreover, if you output the 0'th item on disk and get an input which\nmay not fit in the current tournament (because the value \"wins\" over\nthe last output value), it cannot fit in the heap, so the size of the\nheap decreases.  The freed memory could be cleverly reused immediately\nfor progressively building a second heap, which grows at exactly the\nsame rate the first heap is melting.  When the first heap completely\nvanishes, you switch heaps and start a new run.  Clever and quite\neffective!\n\nIn a word, heaps are useful memory structures to know.  I use them in\na few applications, and I think it is good to keep a `heap' module\naround. :-)\n\n--------------------\n[1] The disk balancing algorithms which are current, nowadays, are\nmore annoying than clever, and this is a consequence of the seeking\ncapabilities of the disks.  On devices which cannot seek, like big\ntape drives, the story was quite different, and one had to be very\nclever to ensure (far in advance) that each tape movement will be the\nmost effective possible (that is, will best participate at\n\"progressing\" the merge).  Some tapes were even able to read\nbackwards, and this was also used to avoid the rewinding time.\nBelieve me, real good tape sorts were quite spectacular to watch!\nFrom all times, sorting has always been a Great Art! :-)\n\"\"\"\n\n__all__ = ['heappush', 'heappop', 'heapify', 'heapreplace', 'merge',\n           'nlargest', 'nsmallest', 'heappushpop']\n\nfrom itertools import islice, count, imap, izip, tee, chain\nfrom operator import itemgetter\n\ndef cmp_lt(x, y):\n    # Use __lt__ if available; otherwise, try __le__.\n    # In Py3.x, only __lt__ will be called.\n    return (x < y) if hasattr(x, '__lt__') else (not y <= x)\n\ndef heappush(heap, item):\n    \"\"\"Push item onto heap, maintaining the heap invariant.\"\"\"\n    heap.append(item)\n    _siftdown(heap, 0, len(heap)-1)\n\ndef heappop(heap):\n    \"\"\"Pop the smallest item off the heap, maintaining the heap invariant.\"\"\"\n    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty\n    if heap:\n        returnitem = heap[0]\n        heap[0] = lastelt\n        _siftup(heap, 0)\n    else:\n        returnitem = lastelt\n    return returnitem\n\ndef heapreplace(heap, item):\n    \"\"\"Pop and return the current smallest value, and add the new item.\n\n    This is more efficient than heappop() followed by heappush(), and can be\n    more appropriate when using a fixed-size heap.  Note that the value\n    returned may be larger than item!  That constrains reasonable uses of\n    this routine unless written as part of a conditional replacement:\n\n        if item > heap[0]:\n            item = heapreplace(heap, item)\n    \"\"\"\n    returnitem = heap[0]    # raises appropriate IndexError if heap is empty\n    heap[0] = item\n    _siftup(heap, 0)\n    return returnitem\n\ndef heappushpop(heap, item):\n    \"\"\"Fast version of a heappush followed by a heappop.\"\"\"\n    if heap and cmp_lt(heap[0], item):\n        item, heap[0] = heap[0], item\n        _siftup(heap, 0)\n    return item\n\ndef heapify(x):\n    \"\"\"Transform list into a heap, in-place, in O(len(x)) time.\"\"\"\n    n = len(x)\n    # Transform bottom-up.  The largest index there's any point to looking at\n    # is the largest with a child index in-range, so must have 2*i + 1 < n,\n    # or i < (n-1)/2.  If n is even = 2*j, this is (2*j-1)/2 = j-1/2 so\n    # j-1 is the largest, which is n//2 - 1.  If n is odd = 2*j+1, this is\n    # (2*j+1-1)/2 = j so j-1 is the largest, and that's again n//2-1.\n    for i in reversed(xrange(n//2)):\n        _siftup(x, i)\n\ndef _heappushpop_max(heap, item):\n    \"\"\"Maxheap version of a heappush followed by a heappop.\"\"\"\n    if heap and cmp_lt(item, heap[0]):\n        item, heap[0] = heap[0], item\n        _siftup_max(heap, 0)\n    return item\n\ndef _heapify_max(x):\n    \"\"\"Transform list into a maxheap, in-place, in O(len(x)) time.\"\"\"\n    n = len(x)\n    for i in reversed(range(n//2)):\n        _siftup_max(x, i)\n\ndef nlargest(n, iterable):\n    \"\"\"Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, reverse=True)[:n]\n    \"\"\"\n    if n < 0:\n        return []\n    it = iter(iterable)\n    result = list(islice(it, n))\n    if not result:\n        return result\n    heapify(result)\n    _heappushpop = heappushpop\n    for elem in it:\n        _heappushpop(result, elem)\n    result.sort(reverse=True)\n    return result\n\ndef nsmallest(n, iterable):\n    \"\"\"Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable)[:n]\n    \"\"\"\n    if n < 0:\n        return []\n    it = iter(iterable)\n    result = list(islice(it, n))\n    if not result:\n        return result\n    _heapify_max(result)\n    _heappushpop = _heappushpop_max\n    for elem in it:\n        _heappushpop(result, elem)\n    result.sort()\n    return result\n\n# 'heap' is a heap at all indices >= startpos, except possibly for pos.  pos\n# is the index of a leaf with a possibly out-of-order value.  Restore the\n# heap invariant.\ndef _siftdown(heap, startpos, pos):\n    newitem = heap[pos]\n    # Follow the path to the root, moving parents down until finding a place\n    # newitem fits.\n    while pos > startpos:\n        parentpos = (pos - 1) >> 1\n        parent = heap[parentpos]\n        if cmp_lt(newitem, parent):\n            heap[pos] = parent\n            pos = parentpos\n            continue\n        break\n    heap[pos] = newitem\n\n# The child indices of heap index pos are already heaps, and we want to make\n# a heap at index pos too.  We do this by bubbling the smaller child of\n# pos up (and so on with that child's children, etc) until hitting a leaf,\n# then using _siftdown to move the oddball originally at index pos into place.\n#\n# We *could* break out of the loop as soon as we find a pos where newitem <=\n# both its children, but turns out that's not a good idea, and despite that\n# many books write the algorithm that way.  During a heap pop, the last array\n# element is sifted in, and that tends to be large, so that comparing it\n# against values starting from the root usually doesn't pay (= usually doesn't\n# get us out of the loop early).  See Knuth, Volume 3, where this is\n# explained and quantified in an exercise.\n#\n# Cutting the # of comparisons is important, since these routines have no\n# way to extract \"the priority\" from an array element, so that intelligence\n# is likely to be hiding in custom __cmp__ methods, or in array elements\n# storing (priority, record) tuples.  Comparisons are thus potentially\n# expensive.\n#\n# On random arrays of length 1000, making this change cut the number of\n# comparisons made by heapify() a little, and those made by exhaustive\n# heappop() a lot, in accord with theory.  Here are typical results from 3\n# runs (3 just to demonstrate how small the variance is):\n#\n# Compares needed by heapify     Compares needed by 1000 heappops\n# --------------------------     --------------------------------\n# 1837 cut to 1663               14996 cut to 8680\n# 1855 cut to 1659               14966 cut to 8678\n# 1847 cut to 1660               15024 cut to 8703\n#\n# Building the heap by using heappush() 1000 times instead required\n# 2198, 2148, and 2219 compares:  heapify() is more efficient, when\n# you can use it.\n#\n# The total compares needed by list.sort() on the same lists were 8627,\n# 8627, and 8632 (this should be compared to the sum of heapify() and\n# heappop() compares):  list.sort() is (unsurprisingly!) more efficient\n# for sorting.\n\ndef _siftup(heap, pos):\n    endpos = len(heap)\n    startpos = pos\n    newitem = heap[pos]\n    # Bubble up the smaller child until hitting a leaf.\n    childpos = 2*pos + 1    # leftmost child position\n    while childpos < endpos:\n        # Set childpos to index of smaller child.\n        rightpos = childpos + 1\n        if rightpos < endpos and not cmp_lt(heap[childpos], heap[rightpos]):\n            childpos = rightpos\n        # Move the smaller child up.\n        heap[pos] = heap[childpos]\n        pos = childpos\n        childpos = 2*pos + 1\n    # The leaf at pos is empty now.  Put newitem there, and bubble it up\n    # to its final resting place (by sifting its parents down).\n    heap[pos] = newitem\n    _siftdown(heap, startpos, pos)\n\ndef _siftdown_max(heap, startpos, pos):\n    'Maxheap variant of _siftdown'\n    newitem = heap[pos]\n    # Follow the path to the root, moving parents down until finding a place\n    # newitem fits.\n    while pos > startpos:\n        parentpos = (pos - 1) >> 1\n        parent = heap[parentpos]\n        if cmp_lt(parent, newitem):\n            heap[pos] = parent\n            pos = parentpos\n            continue\n        break\n    heap[pos] = newitem\n\ndef _siftup_max(heap, pos):\n    'Maxheap variant of _siftup'\n    endpos = len(heap)\n    startpos = pos\n    newitem = heap[pos]\n    # Bubble up the larger child until hitting a leaf.\n    childpos = 2*pos + 1    # leftmost child position\n    while childpos < endpos:\n        # Set childpos to index of larger child.\n        rightpos = childpos + 1\n        if rightpos < endpos and not cmp_lt(heap[rightpos], heap[childpos]):\n            childpos = rightpos\n        # Move the larger child up.\n        heap[pos] = heap[childpos]\n        pos = childpos\n        childpos = 2*pos + 1\n    # The leaf at pos is empty now.  Put newitem there, and bubble it up\n    # to its final resting place (by sifting its parents down).\n    heap[pos] = newitem\n    _siftdown_max(heap, startpos, pos)\n\n# If available, use C implementation\ntry:\n    from _heapq import *\nexcept ImportError:\n    pass\n\ndef merge(*iterables):\n    '''Merge multiple sorted inputs into a single sorted output.\n\n    Similar to sorted(itertools.chain(*iterables)) but returns a generator,\n    does not pull the data into memory all at once, and assumes that each of\n    the input streams is already sorted (smallest to largest).\n\n    >>> list(merge([1,3,5,7], [0,2,4,8], [5,10,15,20], [], [25]))\n    [0, 1, 2, 3, 4, 5, 5, 7, 8, 10, 15, 20, 25]\n\n    '''\n    _heappop, _heapreplace, _StopIteration = heappop, heapreplace, StopIteration\n    _len = len\n\n    h = []\n    h_append = h.append\n    for itnum, it in enumerate(map(iter, iterables)):\n        try:\n            next = it.next\n            h_append([next(), itnum, next])\n        except _StopIteration:\n            pass\n    heapify(h)\n\n    while _len(h) > 1:\n        try:\n            while 1:\n                v, itnum, next = s = h[0]\n                yield v\n                s[0] = next()               # raises StopIteration when exhausted\n                _heapreplace(h, s)          # restore heap condition\n        except _StopIteration:\n            _heappop(h)                     # remove empty iterator\n    if h:\n        # fast case when only a single iterator remains\n        v, itnum, next = h[0]\n        yield v\n        for v in next.__self__:\n            yield v\n\n# Extend the implementations of nsmallest and nlargest to use a key= argument\n_nsmallest = nsmallest\ndef nsmallest(n, iterable, key=None):\n    \"\"\"Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key)[:n]\n    \"\"\"\n    # Short-cut for n==1 is to use min() when len(iterable)>0\n    if n == 1:\n        it = iter(iterable)\n        head = list(islice(it, 1))\n        if not head:\n            return []\n        if key is None:\n            return [min(chain(head, it))]\n        return [min(chain(head, it), key=key)]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = izip(iterable, count())                        # decorate\n        result = _nsmallest(n, it)\n        return map(itemgetter(0), result)                   # undecorate\n\n    # General case, slowest method\n    in1, in2 = tee(iterable)\n    it = izip(imap(key, in1), count(), in2)                 # decorate\n    result = _nsmallest(n, it)\n    return map(itemgetter(2), result)                       # undecorate\n\n_nlargest = nlargest\ndef nlargest(n, iterable, key=None):\n    \"\"\"Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key, reverse=True)[:n]\n    \"\"\"\n\n    # Short-cut for n==1 is to use max() when len(iterable)>0\n    if n == 1:\n        it = iter(iterable)\n        head = list(islice(it, 1))\n        if not head:\n            return []\n        if key is None:\n            return [max(chain(head, it))]\n        return [max(chain(head, it), key=key)]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key, reverse=True)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = izip(iterable, count(0,-1))                    # decorate\n        result = _nlargest(n, it)\n        return map(itemgetter(0), result)                   # undecorate\n\n    # General case, slowest method\n    in1, in2 = tee(iterable)\n    it = izip(imap(key, in1), count(0,-1), in2)             # decorate\n    result = _nlargest(n, it)\n    return map(itemgetter(2), result)                       # undecorate\n\nif __name__ == \"__main__\":\n    # Simple sanity test\n    heap = []\n    data = [1, 3, 5, 7, 9, 2, 4, 6, 8, 0]\n    for item in data:\n        heappush(heap, item)\n    sort = []\n    while heap:\n        sort.append(heappop(heap))\n    print sort\n\n    import doctest\n    doctest.testmod()\n",
		"file_name": "heapq.py"
	},
	{
		"content": "# -*- coding: utf-8 -*-\n\"\"\"Get useful information from live Python objects.\n\nThis module encapsulates the interface provided by the internal special\nattributes (func_*, co_*, im_*, tb_*, etc.) in a friendlier fashion.\nIt also provides some help for examining source code and class layout.\n\nHere are some of the useful functions provided by this module:\n\n    ismodule(), isclass(), ismethod(), isfunction(), isgeneratorfunction(),\n        isgenerator(), istraceback(), isframe(), iscode(), isbuiltin(),\n        isroutine() - check object types\n    getmembers() - get members of an object that satisfy a given condition\n\n    getfile(), getsourcefile(), getsource() - find an object's source code\n    getdoc(), getcomments() - get documentation on an object\n    getmodule() - determine the module that an object came from\n    getclasstree() - arrange classes so as to represent their hierarchy\n\n    getargspec(), getargvalues(), getcallargs() - get info about function arguments\n    formatargspec(), formatargvalues() - format an argument spec\n    getouterframes(), getinnerframes() - get info about frames\n    currentframe() - get the current stack frame\n    stack(), trace() - get info about frames on the stack or in a traceback\n\"\"\"\n\n# This module is in the public domain.  No warranties.\n\n__author__ = 'Ka-Ping Yee <ping@lfw.org>'\n__date__ = '1 Jan 2001'\n\nimport sys\nimport os\nimport types\nimport string\nimport re\nimport dis\nimport imp\nimport tokenize\nimport linecache\nfrom operator import attrgetter\nfrom collections import namedtuple\n\n# These constants are from Include/code.h.\nCO_OPTIMIZED, CO_NEWLOCALS, CO_VARARGS, CO_VARKEYWORDS = 0x1, 0x2, 0x4, 0x8\nCO_NESTED, CO_GENERATOR, CO_NOFREE = 0x10, 0x20, 0x40\n# See Include/object.h\nTPFLAGS_IS_ABSTRACT = 1 << 20\n\n# ----------------------------------------------------------- type-checking\ndef ismodule(object):\n    \"\"\"Return true if the object is a module.\n\n    Module objects provide these attributes:\n        __doc__         documentation string\n        __file__        filename (missing for built-in modules)\"\"\"\n    return isinstance(object, types.ModuleType)\n\ndef isclass(object):\n    \"\"\"Return true if the object is a class.\n\n    Class objects provide these attributes:\n        __doc__         documentation string\n        __module__      name of module in which this class was defined\"\"\"\n    return isinstance(object, (type, types.ClassType))\n\ndef ismethod(object):\n    \"\"\"Return true if the object is an instance method.\n\n    Instance method objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this method was defined\n        im_class        class object in which this method belongs\n        im_func         function object containing implementation of method\n        im_self         instance to which this method is bound, or None\"\"\"\n    return isinstance(object, types.MethodType)\n\ndef ismethoddescriptor(object):\n    \"\"\"Return true if the object is a method descriptor.\n\n    But not if ismethod() or isclass() or isfunction() are true.\n\n    This is new in Python 2.2, and, for example, is true of int.__add__.\n    An object passing this test has a __get__ attribute but not a __set__\n    attribute, but beyond that the set of attributes varies.  __name__ is\n    usually sensible, and __doc__ often is.\n\n    Methods implemented via descriptors that also pass one of the other\n    tests return false from the ismethoddescriptor() test, simply because\n    the other tests promise more -- you can, e.g., count on having the\n    im_func attribute (etc) when an object passes ismethod().\"\"\"\n    return (hasattr(object, \"__get__\")\n            and not hasattr(object, \"__set__\") # else it's a data descriptor\n            and not ismethod(object)           # mutual exclusion\n            and not isfunction(object)\n            and not isclass(object))\n\ndef isdatadescriptor(object):\n    \"\"\"Return true if the object is a data descriptor.\n\n    Data descriptors have both a __get__ and a __set__ attribute.  Examples are\n    properties (defined in Python) and getsets and members (defined in C).\n    Typically, data descriptors will also have __name__ and __doc__ attributes\n    (properties, getsets, and members have both of these attributes), but this\n    is not guaranteed.\"\"\"\n    return (hasattr(object, \"__set__\") and hasattr(object, \"__get__\"))\n\nif hasattr(types, 'MemberDescriptorType'):\n    # CPython and equivalent\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.MemberDescriptorType)\nelse:\n    # Other implementations\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\nif hasattr(types, 'GetSetDescriptorType'):\n    # CPython and equivalent\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.GetSetDescriptorType)\nelse:\n    # Other implementations\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\ndef isfunction(object):\n    \"\"\"Return true if the object is a user-defined function.\n\n    Function objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this function was defined\n        func_code       code object containing compiled function bytecode\n        func_defaults   tuple of any default values for arguments\n        func_doc        (same as __doc__)\n        func_globals    global namespace in which this function was defined\n        func_name       (same as __name__)\"\"\"\n    return isinstance(object, types.FunctionType)\n\ndef isgeneratorfunction(object):\n    \"\"\"Return true if the object is a user-defined generator function.\n\n    Generator function objects provides same attributes as functions.\n\n    See help(isfunction) for attributes listing.\"\"\"\n    return bool((isfunction(object) or ismethod(object)) and\n                object.func_code.co_flags & CO_GENERATOR)\n\ndef isgenerator(object):\n    \"\"\"Return true if the object is a generator.\n\n    Generator objects provide these attributes:\n        __iter__        defined to support iteration over container\n        close           raises a new GeneratorExit exception inside the\n                        generator to terminate the iteration\n        gi_code         code object\n        gi_frame        frame object or possibly None once the generator has\n                        been exhausted\n        gi_running      set to 1 when generator is executing, 0 otherwise\n        next            return the next item from the container\n        send            resumes the generator and \"sends\" a value that becomes\n                        the result of the current yield-expression\n        throw           used to raise an exception inside the generator\"\"\"\n    return isinstance(object, types.GeneratorType)\n\ndef istraceback(object):\n    \"\"\"Return true if the object is a traceback.\n\n    Traceback objects provide these attributes:\n        tb_frame        frame object at this level\n        tb_lasti        index of last attempted instruction in bytecode\n        tb_lineno       current line number in Python source code\n        tb_next         next inner traceback object (called by this level)\"\"\"\n    return isinstance(object, types.TracebackType)\n\ndef isframe(object):\n    \"\"\"Return true if the object is a frame object.\n\n    Frame objects provide these attributes:\n        f_back          next outer frame object (this frame's caller)\n        f_builtins      built-in namespace seen by this frame\n        f_code          code object being executed in this frame\n        f_exc_traceback traceback if raised in this frame, or None\n        f_exc_type      exception type if raised in this frame, or None\n        f_exc_value     exception value if raised in this frame, or None\n        f_globals       global namespace seen by this frame\n        f_lasti         index of last attempted instruction in bytecode\n        f_lineno        current line number in Python source code\n        f_locals        local namespace seen by this frame\n        f_restricted    0 or 1 if frame is in restricted execution mode\n        f_trace         tracing function for this frame, or None\"\"\"\n    return isinstance(object, types.FrameType)\n\ndef iscode(object):\n    \"\"\"Return true if the object is a code object.\n\n    Code objects provide these attributes:\n        co_argcount     number of arguments (not including * or ** args)\n        co_code         string of raw compiled bytecode\n        co_consts       tuple of constants used in the bytecode\n        co_filename     name of file in which this code object was created\n        co_firstlineno  number of first line in Python source code\n        co_flags        bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg\n        co_lnotab       encoded mapping of line numbers to bytecode indices\n        co_name         name with which this code object was defined\n        co_names        tuple of names of local variables\n        co_nlocals      number of local variables\n        co_stacksize    virtual machine stack space required\n        co_varnames     tuple of names of arguments and local variables\"\"\"\n    return isinstance(object, types.CodeType)\n\ndef isbuiltin(object):\n    \"\"\"Return true if the object is a built-in function or method.\n\n    Built-in functions and methods provide these attributes:\n        __doc__         documentation string\n        __name__        original name of this function or method\n        __self__        instance to which a method is bound, or None\"\"\"\n    return isinstance(object, types.BuiltinFunctionType)\n\ndef isroutine(object):\n    \"\"\"Return true if the object is any kind of function or method.\"\"\"\n    return (isbuiltin(object)\n            or isfunction(object)\n            or ismethod(object)\n            or ismethoddescriptor(object))\n\ndef isabstract(object):\n    \"\"\"Return true if the object is an abstract base class (ABC).\"\"\"\n    return bool(isinstance(object, type) and object.__flags__ & TPFLAGS_IS_ABSTRACT)\n\ndef getmembers(object, predicate=None):\n    \"\"\"Return all members of an object as (name, value) pairs sorted by name.\n    Optionally, only return members that satisfy a given predicate.\"\"\"\n    results = []\n    for key in dir(object):\n        try:\n            value = getattr(object, key)\n        except AttributeError:\n            continue\n        if not predicate or predicate(value):\n            results.append((key, value))\n    results.sort()\n    return results\n\nAttribute = namedtuple('Attribute', 'name kind defining_class object')\n\ndef classify_class_attrs(cls):\n    \"\"\"Return list of attribute-descriptor tuples.\n\n    For each name in dir(cls), the return list contains a 4-tuple\n    with these elements:\n\n        0. The name (a string).\n\n        1. The kind of attribute this is, one of these strings:\n               'class method'    created via classmethod()\n               'static method'   created via staticmethod()\n               'property'        created via property()\n               'method'          any other flavor of method\n               'data'            not a method\n\n        2. The class which defined this attribute (a class).\n\n        3. The object as obtained directly from the defining class's\n           __dict__, not via getattr.  This is especially important for\n           data attributes:  C.data is just a data object, but\n           C.__dict__['data'] may be a data descriptor with additional\n           info, like a __doc__ string.\n    \"\"\"\n\n    mro = getmro(cls)\n    names = dir(cls)\n    result = []\n    for name in names:\n        # Get the object associated with the name, and where it was defined.\n        # Getting an obj from the __dict__ sometimes reveals more than\n        # using getattr.  Static and class methods are dramatic examples.\n        # Furthermore, some objects may raise an Exception when fetched with\n        # getattr(). This is the case with some descriptors (bug #1785).\n        # Thus, we only use getattr() as a last resort.\n        homecls = None\n        for base in (cls,) + mro:\n            if name in base.__dict__:\n                obj = base.__dict__[name]\n                homecls = base\n                break\n        else:\n            obj = getattr(cls, name)\n            homecls = getattr(obj, \"__objclass__\", homecls)\n\n        # Classify the object.\n        if isinstance(obj, staticmethod):\n            kind = \"static method\"\n        elif isinstance(obj, classmethod):\n            kind = \"class method\"\n        elif isinstance(obj, property):\n            kind = \"property\"\n        elif ismethoddescriptor(obj):\n            kind = \"method\"\n        elif isdatadescriptor(obj):\n            kind = \"data\"\n        else:\n            obj_via_getattr = getattr(cls, name)\n            if (ismethod(obj_via_getattr) or\n                ismethoddescriptor(obj_via_getattr)):\n                kind = \"method\"\n            else:\n                kind = \"data\"\n            obj = obj_via_getattr\n\n        result.append(Attribute(name, kind, homecls, obj))\n\n    return result\n\n# ----------------------------------------------------------- class helpers\ndef _searchbases(cls, accum):\n    # Simulate the \"classic class\" search order.\n    if cls in accum:\n        return\n    accum.append(cls)\n    for base in cls.__bases__:\n        _searchbases(base, accum)\n\ndef getmro(cls):\n    \"Return tuple of base classes (including cls) in method resolution order.\"\n    if hasattr(cls, \"__mro__\"):\n        return cls.__mro__\n    else:\n        result = []\n        _searchbases(cls, result)\n        return tuple(result)\n\n# -------------------------------------------------- source code extraction\ndef indentsize(line):\n    \"\"\"Return the indent size, in spaces, at the start of a line of text.\"\"\"\n    expline = string.expandtabs(line)\n    return len(expline) - len(string.lstrip(expline))\n\ndef getdoc(object):\n    \"\"\"Get the documentation string for an object.\n\n    All tabs are expanded to spaces.  To clean up docstrings that are\n    indented to line up with blocks of code, any whitespace than can be\n    uniformly removed from the second line onwards is removed.\"\"\"\n    try:\n        doc = object.__doc__\n    except AttributeError:\n        return None\n    if not isinstance(doc, types.StringTypes):\n        return None\n    return cleandoc(doc)\n\ndef cleandoc(doc):\n    \"\"\"Clean up indentation from docstrings.\n\n    Any whitespace that can be uniformly removed from the second line\n    onwards is removed.\"\"\"\n    try:\n        lines = string.split(string.expandtabs(doc), '\\n')\n    except UnicodeError:\n        return None\n    else:\n        # Find minimum indentation of any non-blank lines after first line.\n        margin = sys.maxint\n        for line in lines[1:]:\n            content = len(string.lstrip(line))\n            if content:\n                indent = len(line) - content\n                margin = min(margin, indent)\n        # Remove indentation.\n        if lines:\n            lines[0] = lines[0].lstrip()\n        if margin < sys.maxint:\n            for i in range(1, len(lines)): lines[i] = lines[i][margin:]\n        # Remove any trailing or leading blank lines.\n        while lines and not lines[-1]:\n            lines.pop()\n        while lines and not lines[0]:\n            lines.pop(0)\n        return string.join(lines, '\\n')\n\ndef getfile(object):\n    \"\"\"Work out which source or compiled file an object was defined in.\"\"\"\n    if ismodule(object):\n        if hasattr(object, '__file__'):\n            return object.__file__\n        raise TypeError('{!r} is a built-in module'.format(object))\n    if isclass(object):\n        object = sys.modules.get(object.__module__)\n        if hasattr(object, '__file__'):\n            return object.__file__\n        raise TypeError('{!r} is a built-in class'.format(object))\n    if ismethod(object):\n        object = object.im_func\n    if isfunction(object):\n        object = object.func_code\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        return object.co_filename\n    raise TypeError('{!r} is not a module, class, method, '\n                    'function, traceback, frame, or code object'.format(object))\n\nModuleInfo = namedtuple('ModuleInfo', 'name suffix mode module_type')\n\ndef getmoduleinfo(path):\n    \"\"\"Get the module name, suffix, mode, and module type for a given file.\"\"\"\n    filename = os.path.basename(path)\n    suffixes = map(lambda info:\n                   (-len(info[0]), info[0], info[1], info[2]),\n                    imp.get_suffixes())\n    suffixes.sort() # try longest suffixes first, in case they overlap\n    for neglen, suffix, mode, mtype in suffixes:\n        if filename[neglen:] == suffix:\n            return ModuleInfo(filename[:neglen], suffix, mode, mtype)\n\ndef getmodulename(path):\n    \"\"\"Return the module name for a given file, or None.\"\"\"\n    info = getmoduleinfo(path)\n    if info: return info[0]\n\ndef getsourcefile(object):\n    \"\"\"Return the filename that can be used to locate an object's source.\n    Return None if no way can be identified to get the source.\n    \"\"\"\n    filename = getfile(object)\n    if string.lower(filename[-4:]) in ('.pyc', '.pyo'):\n        filename = filename[:-4] + '.py'\n    for suffix, mode, kind in imp.get_suffixes():\n        if 'b' in mode and string.lower(filename[-len(suffix):]) == suffix:\n            # Looks like a binary file.  We want to only return a text file.\n            return None\n    if os.path.exists(filename):\n        return filename\n    # only return a non-existent filename if the module has a PEP 302 loader\n    if hasattr(getmodule(object, filename), '__loader__'):\n        return filename\n    # or it is in the linecache\n    if filename in linecache.cache:\n        return filename\n\ndef getabsfile(object, _filename=None):\n    \"\"\"Return an absolute path to the source or compiled file for an object.\n\n    The idea is for each object to have a unique origin, so this routine\n    normalizes the result as much as possible.\"\"\"\n    if _filename is None:\n        _filename = getsourcefile(object) or getfile(object)\n    return os.path.normcase(os.path.abspath(_filename))\n\nmodulesbyfile = {}\n_filesbymodname = {}\n\ndef getmodule(object, _filename=None):\n    \"\"\"Return the module an object was defined in, or None if not found.\"\"\"\n    if ismodule(object):\n        return object\n    if hasattr(object, '__module__'):\n        return sys.modules.get(object.__module__)\n    # Try the filename to modulename cache\n    if _filename is not None and _filename in modulesbyfile:\n        return sys.modules.get(modulesbyfile[_filename])\n    # Try the cache again with the absolute file name\n    try:\n        file = getabsfile(object, _filename)\n    except TypeError:\n        return None\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Update the filename to module name cache and check yet again\n    # Copy sys.modules in order to cope with changes while iterating\n    for modname, module in sys.modules.items():\n        if ismodule(module) and hasattr(module, '__file__'):\n            f = module.__file__\n            if f == _filesbymodname.get(modname, None):\n                # Have already mapped this module, so skip it\n                continue\n            _filesbymodname[modname] = f\n            f = getabsfile(module)\n            # Always map to the name the module knows itself by\n            modulesbyfile[f] = modulesbyfile[\n                os.path.realpath(f)] = module.__name__\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Check the main module\n    main = sys.modules['__main__']\n    if not hasattr(object, '__name__'):\n        return None\n    if hasattr(main, object.__name__):\n        mainobject = getattr(main, object.__name__)\n        if mainobject is object:\n            return main\n    # Check builtins\n    builtin = sys.modules['__builtin__']\n    if hasattr(builtin, object.__name__):\n        builtinobject = getattr(builtin, object.__name__)\n        if builtinobject is object:\n            return builtin\n\ndef findsource(object):\n    \"\"\"Return the entire source file and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of all the lines\n    in the file and the line number indexes a line in that list.  An IOError\n    is raised if the source code cannot be retrieved.\"\"\"\n\n    file = getfile(object)\n    sourcefile = getsourcefile(object)\n    if not sourcefile and file[:1] + file[-1:] != '<>':\n        raise IOError('source code not available')\n    file = sourcefile if sourcefile else file\n\n    module = getmodule(object, file)\n    if module:\n        lines = linecache.getlines(file, module.__dict__)\n    else:\n        lines = linecache.getlines(file)\n    if not lines:\n        raise IOError('could not get source code')\n\n    if ismodule(object):\n        return lines, 0\n\n    if isclass(object):\n        name = object.__name__\n        pat = re.compile(r'^(\\s*)class\\s*' + name + r'\\b')\n        # make some effort to find the best matching class definition:\n        # use the one with the least indentation, which is the one\n        # that's most probably not inside a function definition.\n        candidates = []\n        for i in range(len(lines)):\n            match = pat.match(lines[i])\n            if match:\n                # if it's at toplevel, it's already the best one\n                if lines[i][0] == 'c':\n                    return lines, i\n                # else add whitespace to candidate list\n                candidates.append((match.group(1), i))\n        if candidates:\n            # this will sort by whitespace, and by line number,\n            # less whitespace first\n            candidates.sort()\n            return lines, candidates[0][1]\n        else:\n            raise IOError('could not find class definition')\n\n    if ismethod(object):\n        object = object.im_func\n    if isfunction(object):\n        object = object.func_code\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        if not hasattr(object, 'co_firstlineno'):\n            raise IOError('could not find function definition')\n        lnum = object.co_firstlineno - 1\n        pat = re.compile(r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n        while lnum > 0:\n            if pat.match(lines[lnum]): break\n            lnum = lnum - 1\n        return lines, lnum\n    raise IOError('could not find code object')\n\ndef getcomments(object):\n    \"\"\"Get lines of comments immediately preceding an object's source code.\n\n    Returns None when source can't be found.\n    \"\"\"\n    try:\n        lines, lnum = findsource(object)\n    except (IOError, TypeError):\n        return None\n\n    if ismodule(object):\n        # Look for a comment block at the top of the file.\n        start = 0\n        if lines and lines[0][:2] == '#!': start = 1\n        while start < len(lines) and string.strip(lines[start]) in ('', '#'):\n            start = start + 1\n        if start < len(lines) and lines[start][:1] == '#':\n            comments = []\n            end = start\n            while end < len(lines) and lines[end][:1] == '#':\n                comments.append(string.expandtabs(lines[end]))\n                end = end + 1\n            return string.join(comments, '')\n\n    # Look for a preceding block of comments at the same indentation.\n    elif lnum > 0:\n        indent = indentsize(lines[lnum])\n        end = lnum - 1\n        if end >= 0 and string.lstrip(lines[end])[:1] == '#' and \\\n            indentsize(lines[end]) == indent:\n            comments = [string.lstrip(string.expandtabs(lines[end]))]\n            if end > 0:\n                end = end - 1\n                comment = string.lstrip(string.expandtabs(lines[end]))\n                while comment[:1] == '#' and indentsize(lines[end]) == indent:\n                    comments[:0] = [comment]\n                    end = end - 1\n                    if end < 0: break\n                    comment = string.lstrip(string.expandtabs(lines[end]))\n            while comments and string.strip(comments[0]) == '#':\n                comments[:1] = []\n            while comments and string.strip(comments[-1]) == '#':\n                comments[-1:] = []\n            return string.join(comments, '')\n\nclass EndOfBlock(Exception): pass\n\nclass BlockFinder:\n    \"\"\"Provide a tokeneater() method to detect the end of a code block.\"\"\"\n    def __init__(self):\n        self.indent = 0\n        self.islambda = False\n        self.started = False\n        self.passline = False\n        self.last = 1\n\n    def tokeneater(self, type, token, srow_scol, erow_ecol, line):\n        srow, scol = srow_scol\n        erow, ecol = erow_ecol\n        if not self.started:\n            # look for the first \"def\", \"class\" or \"lambda\"\n            if token in (\"def\", \"class\", \"lambda\"):\n                if token == \"lambda\":\n                    self.islambda = True\n                self.started = True\n            self.passline = True    # skip to the end of the line\n        elif type == tokenize.NEWLINE:\n            self.passline = False   # stop skipping when a NEWLINE is seen\n            self.last = srow\n            if self.islambda:       # lambdas always end at the first NEWLINE\n                raise EndOfBlock\n        elif self.passline:\n            pass\n        elif type == tokenize.INDENT:\n            self.indent = self.indent + 1\n            self.passline = True\n        elif type == tokenize.DEDENT:\n            self.indent = self.indent - 1\n            # the end of matching indent/dedent pairs end a block\n            # (note that this only works for \"def\"/\"class\" blocks,\n            #  not e.g. for \"if: else:\" or \"try: finally:\" blocks)\n            if self.indent <= 0:\n                raise EndOfBlock\n        elif self.indent == 0 and type not in (tokenize.COMMENT, tokenize.NL):\n            # any other token on the same indentation level end the previous\n            # block as well, except the pseudo-tokens COMMENT and NL.\n            raise EndOfBlock\n\ndef getblock(lines):\n    \"\"\"Extract the block of code at the top of the given list of lines.\"\"\"\n    blockfinder = BlockFinder()\n    try:\n        tokenize.tokenize(iter(lines).next, blockfinder.tokeneater)\n    except (EndOfBlock, IndentationError):\n        pass\n    return lines[:blockfinder.last]\n\ndef getsourcelines(object):\n    \"\"\"Return a list of source lines and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of the lines\n    corresponding to the object and the line number indicates where in the\n    original source file the first line of code was found.  An IOError is\n    raised if the source code cannot be retrieved.\"\"\"\n    lines, lnum = findsource(object)\n\n    if ismodule(object): return lines, 0\n    else: return getblock(lines[lnum:]), lnum + 1\n\ndef getsource(object):\n    \"\"\"Return the text of the source code for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a single string.  An\n    IOError is raised if the source code cannot be retrieved.\"\"\"\n    lines, lnum = getsourcelines(object)\n    return string.join(lines, '')\n\n# --------------------------------------------------- class tree extraction\ndef walktree(classes, children, parent):\n    \"\"\"Recursive helper function for getclasstree().\"\"\"\n    results = []\n    classes.sort(key=attrgetter('__module__', '__name__'))\n    for c in classes:\n        results.append((c, c.__bases__))\n        if c in children:\n            results.append(walktree(children[c], children, c))\n    return results\n\ndef getclasstree(classes, unique=0):\n    \"\"\"Arrange the given list of classes into a hierarchy of nested lists.\n\n    Where a nested list appears, it contains classes derived from the class\n    whose entry immediately precedes the list.  Each entry is a 2-tuple\n    containing a class and a tuple of its base classes.  If the 'unique'\n    argument is true, exactly one entry appears in the returned structure\n    for each class in the given list.  Otherwise, classes using multiple\n    inheritance and their descendants will appear multiple times.\"\"\"\n    children = {}\n    roots = []\n    for c in classes:\n        if c.__bases__:\n            for parent in c.__bases__:\n                if not parent in children:\n                    children[parent] = []\n                if c not in children[parent]:\n                    children[parent].append(c)\n                if unique and parent in classes: break\n        elif c not in roots:\n            roots.append(c)\n    for parent in children:\n        if parent not in classes:\n            roots.append(parent)\n    return walktree(roots, children, None)\n\n# ------------------------------------------------ argument list extraction\nArguments = namedtuple('Arguments', 'args varargs keywords')\n\ndef getargs(co):\n    \"\"\"Get information about the arguments accepted by a code object.\n\n    Three things are returned: (args, varargs, varkw), where 'args' is\n    a list of argument names (possibly containing nested lists), and\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\"\"\"\n\n    if not iscode(co):\n        if hasattr(len, 'func_code') and type(co) is type(len.func_code):\n            # PyPy extension: built-in function objects have a func_code too.\n            # There is no co_code on it, but co_argcount and co_varnames and\n            # co_flags are present.\n            pass\n        else:\n            raise TypeError('{!r} is not a code object'.format(co))\n\n    code = getattr(co, 'co_code', '')\n    nargs = co.co_argcount\n    names = co.co_varnames\n    args = list(names[:nargs])\n    step = 0\n\n    # The following acrobatics are for anonymous (tuple) arguments.\n    for i in range(nargs):\n        if args[i][:1] in ('', '.'):\n            stack, remain, count = [], [], []\n            while step < len(code):\n                op = ord(code[step])\n                step = step + 1\n                if op >= dis.HAVE_ARGUMENT:\n                    opname = dis.opname[op]\n                    value = ord(code[step]) + ord(code[step+1])*256\n                    step = step + 2\n                    if opname in ('UNPACK_TUPLE', 'UNPACK_SEQUENCE'):\n                        remain.append(value)\n                        count.append(value)\n                    elif opname == 'STORE_FAST':\n                        stack.append(names[value])\n\n                        # Special case for sublists of length 1: def foo((bar))\n                        # doesn't generate the UNPACK_TUPLE bytecode, so if\n                        # `remain` is empty here, we have such a sublist.\n                        if not remain:\n                            stack[0] = [stack[0]]\n                            break\n                        else:\n                            remain[-1] = remain[-1] - 1\n                            while remain[-1] == 0:\n                                remain.pop()\n                                size = count.pop()\n                                stack[-size:] = [stack[-size:]]\n                                if not remain: break\n                                remain[-1] = remain[-1] - 1\n                            if not remain: break\n            args[i] = stack[0]\n\n    varargs = None\n    if co.co_flags & CO_VARARGS:\n        varargs = co.co_varnames[nargs]\n        nargs = nargs + 1\n    varkw = None\n    if co.co_flags & CO_VARKEYWORDS:\n        varkw = co.co_varnames[nargs]\n    return Arguments(args, varargs, varkw)\n\nArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')\n\ndef getargspec(func):\n    \"\"\"Get the names and default values of a function's arguments.\n\n    A tuple of four things is returned: (args, varargs, varkw, defaults).\n    'args' is a list of the argument names (it may contain nested lists).\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\n    'defaults' is an n-tuple of the default values of the last n arguments.\n    \"\"\"\n\n    if ismethod(func):\n        func = func.im_func\n    if not (isfunction(func) or\n            isbuiltin(func) and hasattr(func, 'func_code')):\n            # PyPy extension: this works for built-in functions too\n        raise TypeError('{!r} is not a Python function'.format(func))\n    args, varargs, varkw = getargs(func.func_code)\n    return ArgSpec(args, varargs, varkw, func.func_defaults)\n\nArgInfo = namedtuple('ArgInfo', 'args varargs keywords locals')\n\ndef getargvalues(frame):\n    \"\"\"Get information about arguments passed into a particular frame.\n\n    A tuple of four things is returned: (args, varargs, varkw, locals).\n    'args' is a list of the argument names (it may contain nested lists).\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\n    'locals' is the locals dictionary of the given frame.\"\"\"\n    args, varargs, varkw = getargs(frame.f_code)\n    return ArgInfo(args, varargs, varkw, frame.f_locals)\n\ndef joinseq(seq):\n    if len(seq) == 1:\n        return '(' + seq[0] + ',)'\n    else:\n        return '(' + string.join(seq, ', ') + ')'\n\ndef strseq(object, convert, join=joinseq):\n    \"\"\"Recursively walk a sequence, stringifying each element.\"\"\"\n    if type(object) in (list, tuple):\n        return join(map(lambda o, c=convert, j=join: strseq(o, c, j), object))\n    else:\n        return convert(object)\n\ndef formatargspec(args, varargs=None, varkw=None, defaults=None,\n                  formatarg=str,\n                  formatvarargs=lambda name: '*' + name,\n                  formatvarkw=lambda name: '**' + name,\n                  formatvalue=lambda value: '=' + repr(value),\n                  join=joinseq):\n    \"\"\"Format an argument spec from the 4 values returned by getargspec.\n\n    The first four arguments are (args, varargs, varkw, defaults).  The\n    other four arguments are the corresponding optional formatting functions\n    that are called to turn names and values into strings.  The ninth\n    argument is an optional function to format the sequence of arguments.\"\"\"\n    specs = []\n    if defaults:\n        firstdefault = len(args) - len(defaults)\n    for i, arg in enumerate(args):\n        spec = strseq(arg, formatarg, join)\n        if defaults and i >= firstdefault:\n            spec = spec + formatvalue(defaults[i - firstdefault])\n        specs.append(spec)\n    if varargs is not None:\n        specs.append(formatvarargs(varargs))\n    if varkw is not None:\n        specs.append(formatvarkw(varkw))\n    return '(' + string.join(specs, ', ') + ')'\n\ndef formatargvalues(args, varargs, varkw, locals,\n                    formatarg=str,\n                    formatvarargs=lambda name: '*' + name,\n                    formatvarkw=lambda name: '**' + name,\n                    formatvalue=lambda value: '=' + repr(value),\n                    join=joinseq):\n    \"\"\"Format an argument spec from the 4 values returned by getargvalues.\n\n    The first four arguments are (args, varargs, varkw, locals).  The\n    next four arguments are the corresponding optional formatting functions\n    that are called to turn names and values into strings.  The ninth\n    argument is an optional function to format the sequence of arguments.\"\"\"\n    def convert(name, locals=locals,\n                formatarg=formatarg, formatvalue=formatvalue):\n        return formatarg(name) + formatvalue(locals[name])\n    specs = []\n    for i in range(len(args)):\n        specs.append(strseq(args[i], convert, join))\n    if varargs:\n        specs.append(formatvarargs(varargs) + formatvalue(locals[varargs]))\n    if varkw:\n        specs.append(formatvarkw(varkw) + formatvalue(locals[varkw]))\n    return '(' + string.join(specs, ', ') + ')'\n\ndef getcallargs(func, *positional, **named):\n    \"\"\"Get the mapping of arguments to values.\n\n    A dict is returned, with keys the function argument names (including the\n    names of the * and ** arguments, if any), and values the respective bound\n    values from 'positional' and 'named'.\"\"\"\n    args, varargs, varkw, defaults = getargspec(func)\n    f_name = func.__name__\n    arg2value = {}\n\n    # The following closures are basically because of tuple parameter unpacking.\n    assigned_tuple_params = []\n    def assign(arg, value):\n        if isinstance(arg, str):\n            arg2value[arg] = value\n        else:\n            assigned_tuple_params.append(arg)\n            value = iter(value)\n            for i, subarg in enumerate(arg):\n                try:\n                    subvalue = next(value)\n                except StopIteration:\n                    raise ValueError('need more than %d %s to unpack' %\n                                     (i, 'values' if i > 1 else 'value'))\n                assign(subarg,subvalue)\n            try:\n                next(value)\n            except StopIteration:\n                pass\n            else:\n                raise ValueError('too many values to unpack')\n    def is_assigned(arg):\n        if isinstance(arg,str):\n            return arg in arg2value\n        return arg in assigned_tuple_params\n    if ismethod(func) and func.im_self is not None:\n        # implicit 'self' (or 'cls' for classmethods) argument\n        positional = (func.im_self,) + positional\n    num_pos = len(positional)\n    num_total = num_pos + len(named)\n    num_args = len(args)\n    num_defaults = len(defaults) if defaults else 0\n    for arg, value in zip(args, positional):\n        assign(arg, value)\n    if varargs:\n        if num_pos > num_args:\n            assign(varargs, positional[-(num_pos-num_args):])\n        else:\n            assign(varargs, ())\n    elif 0 < num_args < num_pos:\n        raise TypeError('%s() takes %s %d %s (%d given)' % (\n            f_name, 'at most' if defaults else 'exactly', num_args,\n            'arguments' if num_args > 1 else 'argument', num_total))\n    elif num_args == 0 and num_total:\n        if varkw:\n            if num_pos:\n                # XXX: We should use num_pos, but Python also uses num_total:\n                raise TypeError('%s() takes exactly 0 arguments '\n                                '(%d given)' % (f_name, num_total))\n        else:\n            raise TypeError('%s() takes no arguments (%d given)' %\n                            (f_name, num_total))\n    for arg in args:\n        if isinstance(arg, str) and arg in named:\n            if is_assigned(arg):\n                raise TypeError(\"%s() got multiple values for keyword \"\n                                \"argument '%s'\" % (f_name, arg))\n            else:\n                assign(arg, named.pop(arg))\n    if defaults:    # fill in any missing values with the defaults\n        for arg, value in zip(args[-num_defaults:], defaults):\n            if not is_assigned(arg):\n                assign(arg, value)\n    if varkw:\n        assign(varkw, named)\n    elif named:\n        unexpected = next(iter(named))\n        if isinstance(unexpected, unicode):\n            unexpected = unexpected.encode(sys.getdefaultencoding(), 'replace')\n        raise TypeError(\"%s() got an unexpected keyword argument '%s'\" %\n                        (f_name, unexpected))\n    unassigned = num_args - len([arg for arg in args if is_assigned(arg)])\n    if unassigned:\n        num_required = num_args - num_defaults\n        raise TypeError('%s() takes %s %d %s (%d given)' % (\n            f_name, 'at least' if defaults else 'exactly', num_required,\n            'arguments' if num_required > 1 else 'argument', num_total))\n    return arg2value\n\n# -------------------------------------------------- stack frame extraction\n\nTraceback = namedtuple('Traceback', 'filename lineno function code_context index')\n\ndef getframeinfo(frame, context=1):\n    \"\"\"Get information about a frame or traceback object.\n\n    A tuple of five things is returned: the filename, the line number of\n    the current line, the function name, a list of lines of context from\n    the source code, and the index of the current line within that list.\n    The optional second argument specifies the number of lines of context\n    to return, which are centered around the current line.\"\"\"\n    if istraceback(frame):\n        lineno = frame.tb_lineno\n        frame = frame.tb_frame\n    else:\n        lineno = frame.f_lineno\n    if not isframe(frame):\n        raise TypeError('{!r} is not a frame or traceback object'.format(frame))\n\n    filename = getsourcefile(frame) or getfile(frame)\n    if context > 0:\n        start = lineno - 1 - context//2\n        try:\n            lines, lnum = findsource(frame)\n        except IOError:\n            lines = index = None\n        else:\n            start = max(start, 1)\n            start = max(0, min(start, len(lines) - context))\n            lines = lines[start:start+context]\n            index = lineno - 1 - start\n    else:\n        lines = index = None\n\n    return Traceback(filename, lineno, frame.f_code.co_name, lines, index)\n\ndef getlineno(frame):\n    \"\"\"Get the line number from a frame object, allowing for optimization.\"\"\"\n    # FrameType.f_lineno is now a descriptor that grovels co_lnotab\n    return frame.f_lineno\n\ndef getouterframes(frame, context=1):\n    \"\"\"Get a list of records for a frame and all higher (calling) frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while frame:\n        framelist.append((frame,) + getframeinfo(frame, context))\n        frame = frame.f_back\n    return framelist\n\ndef getinnerframes(tb, context=1):\n    \"\"\"Get a list of records for a traceback's frame and all lower frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while tb:\n        framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n        tb = tb.tb_next\n    return framelist\n\nif hasattr(sys, '_getframe'):\n    currentframe = sys._getframe\nelse:\n    currentframe = lambda _=None: None\n\ndef stack(context=1):\n    \"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\n    return getouterframes(sys._getframe(1), context)\n\ndef trace(context=1):\n    \"\"\"Return a list of records for the stack below the current exception.\"\"\"\n    return getinnerframes(sys.exc_info()[2], context)\n",
		"file_name": "inspect.py"
	},
	{
		"content": "\"\"\"Disassembler of Python byte code into mnemonics.\"\"\"\n\nimport sys\nimport types\n\nfrom opcode import *\nfrom opcode import __all__ as _opcodes_all\n\n__all__ = [\"dis\", \"disassemble\", \"distb\", \"disco\",\n           \"findlinestarts\", \"findlabels\"] + _opcodes_all\ndel _opcodes_all\n\n_have_code = (types.MethodType, types.FunctionType, types.CodeType,\n              types.ClassType, type)\n\ndef dis(x=None):\n    \"\"\"Disassemble classes, methods, functions, or code.\n\n    With no argument, disassemble the last traceback.\n\n    \"\"\"\n    if x is None:\n        distb()\n        return\n    if isinstance(x, types.InstanceType):\n        x = x.__class__\n    if hasattr(x, 'im_func'):\n        x = x.im_func\n    if hasattr(x, 'func_code'):\n        x = x.func_code\n    if hasattr(x, '__dict__'):\n        items = x.__dict__.items()\n        items.sort()\n        for name, x1 in items:\n            if isinstance(x1, _have_code):\n                print \"Disassembly of %s:\" % name\n                try:\n                    dis(x1)\n                except TypeError, msg:\n                    print \"Sorry:\", msg\n                print\n    elif hasattr(x, 'co_code'):\n        disassemble(x)\n    elif isinstance(x, str):\n        disassemble_string(x)\n    else:\n        raise TypeError, \\\n              \"don't know how to disassemble %s objects\" % \\\n              type(x).__name__\n\ndef distb(tb=None):\n    \"\"\"Disassemble a traceback (default: last traceback).\"\"\"\n    if tb is None:\n        try:\n            tb = sys.last_traceback\n        except AttributeError:\n            raise RuntimeError, \"no last traceback to disassemble\"\n        while tb.tb_next: tb = tb.tb_next\n    disassemble(tb.tb_frame.f_code, tb.tb_lasti)\n\ndef disassemble(co, lasti=-1):\n    \"\"\"Disassemble a code object.\"\"\"\n    code = co.co_code\n    labels = findlabels(code)\n    linestarts = dict(findlinestarts(co))\n    n = len(code)\n    i = 0\n    extended_arg = 0\n    free = None\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        if i in linestarts:\n            if i > 0:\n                print\n            print \"%3d\" % linestarts[i],\n        else:\n            print '   ',\n\n        if i == lasti: print '-->',\n        else: print '   ',\n        if i in labels: print '>>',\n        else: print '  ',\n        print repr(i).rjust(4),\n        print opname[op].ljust(20),\n        i = i+1\n        if op >= HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i+1])*256 + extended_arg\n            extended_arg = 0\n            i = i+2\n            if op == EXTENDED_ARG:\n                extended_arg = oparg*65536L\n            print repr(oparg).rjust(5),\n            if op in hasconst:\n                print '(' + repr(co.co_consts[oparg]) + ')',\n            elif op in hasname:\n                print '(' + co.co_names[oparg] + ')',\n            elif op in hasjrel:\n                print '(to ' + repr(i + oparg) + ')',\n            elif op in haslocal:\n                print '(' + co.co_varnames[oparg] + ')',\n            elif op in hascompare:\n                print '(' + cmp_op[oparg] + ')',\n            elif op in hasfree:\n                if free is None:\n                    free = co.co_cellvars + co.co_freevars\n                print '(' + free[oparg] + ')',\n        print\n\ndef disassemble_string(code, lasti=-1, varnames=None, names=None,\n                       constants=None):\n    labels = findlabels(code)\n    n = len(code)\n    i = 0\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        if i == lasti: print '-->',\n        else: print '   ',\n        if i in labels: print '>>',\n        else: print '  ',\n        print repr(i).rjust(4),\n        print opname[op].ljust(15),\n        i = i+1\n        if op >= HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i+1])*256\n            i = i+2\n            print repr(oparg).rjust(5),\n            if op in hasconst:\n                if constants:\n                    print '(' + repr(constants[oparg]) + ')',\n                else:\n                    print '(%d)'%oparg,\n            elif op in hasname:\n                if names is not None:\n                    print '(' + names[oparg] + ')',\n                else:\n                    print '(%d)'%oparg,\n            elif op in hasjrel:\n                print '(to ' + repr(i + oparg) + ')',\n            elif op in haslocal:\n                if varnames:\n                    print '(' + varnames[oparg] + ')',\n                else:\n                    print '(%d)' % oparg,\n            elif op in hascompare:\n                print '(' + cmp_op[oparg] + ')',\n        print\n\ndisco = disassemble                     # XXX For backwards compatibility\n\ndef findlabels(code):\n    \"\"\"Detect all offsets in a byte code which are jump targets.\n\n    Return the list of offsets.\n\n    \"\"\"\n    labels = []\n    n = len(code)\n    i = 0\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        i = i+1\n        if op >= HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i+1])*256\n            i = i+2\n            label = -1\n            if op in hasjrel:\n                label = i+oparg\n            elif op in hasjabs:\n                label = oparg\n            if label >= 0:\n                if label not in labels:\n                    labels.append(label)\n    return labels\n\ndef findlinestarts(code):\n    \"\"\"Find the offsets in a byte code which are start of lines in the source.\n\n    Generate pairs (offset, lineno) as described in Python/compile.c.\n\n    \"\"\"\n    byte_increments = [ord(c) for c in code.co_lnotab[0::2]]\n    line_increments = [ord(c) for c in code.co_lnotab[1::2]]\n\n    lastlineno = None\n    lineno = code.co_firstlineno\n    addr = 0\n    for byte_incr, line_incr in zip(byte_increments, line_increments):\n        if byte_incr:\n            if lineno != lastlineno:\n                yield (addr, lineno)\n                lastlineno = lineno\n            addr += byte_incr\n        lineno += line_incr\n    if lineno != lastlineno:\n        yield (addr, lineno)\n\ndef _test():\n    \"\"\"Simple test program to disassemble a file.\"\"\"\n    if sys.argv[1:]:\n        if sys.argv[2:]:\n            sys.stderr.write(\"usage: python dis.py [-|file]\\n\")\n            sys.exit(2)\n        fn = sys.argv[1]\n        if not fn or fn == \"-\":\n            fn = None\n    else:\n        fn = None\n    if fn is None:\n        f = sys.stdin\n    else:\n        f = open(fn)\n    source = f.read()\n    if fn is not None:\n        f.close()\n    else:\n        fn = \"<stdin>\"\n    code = compile(source, fn, \"exec\")\n    dis(code)\n\nif __name__ == \"__main__\":\n    _test()\n",
		"file_name": "dis.py"
	},
	{
		"content": "\"\"\"\nopcode module - potentially shared between dis and other modules which\noperate on bytecodes (e.g. peephole optimizers).\n\"\"\"\n\n__all__ = [\"cmp_op\", \"hasconst\", \"hasname\", \"hasjrel\", \"hasjabs\",\n           \"haslocal\", \"hascompare\", \"hasfree\", \"opname\", \"opmap\",\n           \"HAVE_ARGUMENT\", \"EXTENDED_ARG\"]\n\ncmp_op = ('<', '<=', '==', '!=', '>', '>=', 'in', 'not in', 'is',\n        'is not', 'exception match', 'BAD')\n\nhasconst = []\nhasname = []\nhasjrel = []\nhasjabs = []\nhaslocal = []\nhascompare = []\nhasfree = []\n\nopmap = {}\nopname = [''] * 256\nfor op in range(256): opname[op] = '<%r>' % (op,)\ndel op\n\ndef def_op(name, op):\n    opname[op] = name\n    opmap[name] = op\n\ndef name_op(name, op):\n    def_op(name, op)\n    hasname.append(op)\n\ndef jrel_op(name, op):\n    def_op(name, op)\n    hasjrel.append(op)\n\ndef jabs_op(name, op):\n    def_op(name, op)\n    hasjabs.append(op)\n\n# Instruction opcodes for compiled code\n# Blank lines correspond to available opcodes\n\ndef_op('STOP_CODE', 0)\ndef_op('POP_TOP', 1)\ndef_op('ROT_TWO', 2)\ndef_op('ROT_THREE', 3)\ndef_op('DUP_TOP', 4)\ndef_op('ROT_FOUR', 5)\n\ndef_op('NOP', 9)\ndef_op('UNARY_POSITIVE', 10)\ndef_op('UNARY_NEGATIVE', 11)\ndef_op('UNARY_NOT', 12)\ndef_op('UNARY_CONVERT', 13)\n\ndef_op('UNARY_INVERT', 15)\n\ndef_op('BINARY_POWER', 19)\ndef_op('BINARY_MULTIPLY', 20)\ndef_op('BINARY_DIVIDE', 21)\ndef_op('BINARY_MODULO', 22)\ndef_op('BINARY_ADD', 23)\ndef_op('BINARY_SUBTRACT', 24)\ndef_op('BINARY_SUBSCR', 25)\ndef_op('BINARY_FLOOR_DIVIDE', 26)\ndef_op('BINARY_TRUE_DIVIDE', 27)\ndef_op('INPLACE_FLOOR_DIVIDE', 28)\ndef_op('INPLACE_TRUE_DIVIDE', 29)\ndef_op('SLICE+0', 30)\ndef_op('SLICE+1', 31)\ndef_op('SLICE+2', 32)\ndef_op('SLICE+3', 33)\n\ndef_op('STORE_SLICE+0', 40)\ndef_op('STORE_SLICE+1', 41)\ndef_op('STORE_SLICE+2', 42)\ndef_op('STORE_SLICE+3', 43)\n\ndef_op('DELETE_SLICE+0', 50)\ndef_op('DELETE_SLICE+1', 51)\ndef_op('DELETE_SLICE+2', 52)\ndef_op('DELETE_SLICE+3', 53)\n\ndef_op('STORE_MAP', 54)\ndef_op('INPLACE_ADD', 55)\ndef_op('INPLACE_SUBTRACT', 56)\ndef_op('INPLACE_MULTIPLY', 57)\ndef_op('INPLACE_DIVIDE', 58)\ndef_op('INPLACE_MODULO', 59)\ndef_op('STORE_SUBSCR', 60)\ndef_op('DELETE_SUBSCR', 61)\ndef_op('BINARY_LSHIFT', 62)\ndef_op('BINARY_RSHIFT', 63)\ndef_op('BINARY_AND', 64)\ndef_op('BINARY_XOR', 65)\ndef_op('BINARY_OR', 66)\ndef_op('INPLACE_POWER', 67)\ndef_op('GET_ITER', 68)\n\ndef_op('PRINT_EXPR', 70)\ndef_op('PRINT_ITEM', 71)\ndef_op('PRINT_NEWLINE', 72)\ndef_op('PRINT_ITEM_TO', 73)\ndef_op('PRINT_NEWLINE_TO', 74)\ndef_op('INPLACE_LSHIFT', 75)\ndef_op('INPLACE_RSHIFT', 76)\ndef_op('INPLACE_AND', 77)\ndef_op('INPLACE_XOR', 78)\ndef_op('INPLACE_OR', 79)\ndef_op('BREAK_LOOP', 80)\ndef_op('WITH_CLEANUP', 81)\ndef_op('LOAD_LOCALS', 82)\ndef_op('RETURN_VALUE', 83)\ndef_op('IMPORT_STAR', 84)\ndef_op('EXEC_STMT', 85)\ndef_op('YIELD_VALUE', 86)\ndef_op('POP_BLOCK', 87)\ndef_op('END_FINALLY', 88)\ndef_op('BUILD_CLASS', 89)\n\nHAVE_ARGUMENT = 90              # Opcodes from here have an argument:\n\nname_op('STORE_NAME', 90)       # Index in name list\nname_op('DELETE_NAME', 91)      # \"\"\ndef_op('UNPACK_SEQUENCE', 92)   # Number of tuple items\njrel_op('FOR_ITER', 93)\ndef_op('LIST_APPEND', 94)\nname_op('STORE_ATTR', 95)       # Index in name list\nname_op('DELETE_ATTR', 96)      # \"\"\nname_op('STORE_GLOBAL', 97)     # \"\"\nname_op('DELETE_GLOBAL', 98)    # \"\"\ndef_op('DUP_TOPX', 99)          # number of items to duplicate\ndef_op('LOAD_CONST', 100)       # Index in const list\nhasconst.append(100)\nname_op('LOAD_NAME', 101)       # Index in name list\ndef_op('BUILD_TUPLE', 102)      # Number of tuple items\ndef_op('BUILD_LIST', 103)       # Number of list items\ndef_op('BUILD_SET', 104)        # Number of set items\ndef_op('BUILD_MAP', 105)        # Number of dict entries (upto 255)\nname_op('LOAD_ATTR', 106)       # Index in name list\ndef_op('COMPARE_OP', 107)       # Comparison operator\nhascompare.append(107)\nname_op('IMPORT_NAME', 108)     # Index in name list\nname_op('IMPORT_FROM', 109)     # Index in name list\njrel_op('JUMP_FORWARD', 110)    # Number of bytes to skip\njabs_op('JUMP_IF_FALSE_OR_POP', 111) # Target byte offset from beginning of code\njabs_op('JUMP_IF_TRUE_OR_POP', 112)  # \"\"\njabs_op('JUMP_ABSOLUTE', 113)        # \"\"\njabs_op('POP_JUMP_IF_FALSE', 114)    # \"\"\njabs_op('POP_JUMP_IF_TRUE', 115)     # \"\"\n\nname_op('LOAD_GLOBAL', 116)     # Index in name list\n\njabs_op('CONTINUE_LOOP', 119)   # Target address\njrel_op('SETUP_LOOP', 120)      # Distance to target address\njrel_op('SETUP_EXCEPT', 121)    # \"\"\njrel_op('SETUP_FINALLY', 122)   # \"\"\n\ndef_op('LOAD_FAST', 124)        # Local variable number\nhaslocal.append(124)\ndef_op('STORE_FAST', 125)       # Local variable number\nhaslocal.append(125)\ndef_op('DELETE_FAST', 126)      # Local variable number\nhaslocal.append(126)\n\ndef_op('RAISE_VARARGS', 130)    # Number of raise arguments (1, 2, or 3)\ndef_op('CALL_FUNCTION', 131)    # #args + (#kwargs << 8)\ndef_op('MAKE_FUNCTION', 132)    # Number of args with default values\ndef_op('BUILD_SLICE', 133)      # Number of items\ndef_op('MAKE_CLOSURE', 134)\ndef_op('LOAD_CLOSURE', 135)\nhasfree.append(135)\ndef_op('LOAD_DEREF', 136)\nhasfree.append(136)\ndef_op('STORE_DEREF', 137)\nhasfree.append(137)\n\ndef_op('CALL_FUNCTION_VAR', 140)     # #args + (#kwargs << 8)\ndef_op('CALL_FUNCTION_KW', 141)      # #args + (#kwargs << 8)\ndef_op('CALL_FUNCTION_VAR_KW', 142)  # #args + (#kwargs << 8)\n\njrel_op('SETUP_WITH', 143)\n\ndef_op('EXTENDED_ARG', 145)\nEXTENDED_ARG = 145\ndef_op('SET_ADD', 146)\ndef_op('MAP_ADD', 147)\n\n# pypy modification, experimental bytecode\ndef_op('LOOKUP_METHOD', 201)          # Index in name list\nhasname.append(201)\ndef_op('CALL_METHOD', 202)            # #args not including 'self'\ndef_op('BUILD_LIST_FROM_ARG', 203)\njrel_op('JUMP_IF_NOT_DEBUG', 204)     # jump over assert statements\n\ndel def_op, name_op, jrel_op, jabs_op\n",
		"file_name": "opcode.py"
	},
	{
		"content": "#! /usr/bin/env python\n\n\"\"\"A Python debugger.\"\"\"\n\n# (See pdb.doc for documentation.)\n\nimport sys\nimport linecache\nimport cmd\nimport bdb\nfrom repr import Repr\nimport os\nimport re\nimport pprint\nimport traceback\n\n\nclass Restart(Exception):\n    \"\"\"Causes a debugger to be restarted for the debugged python program.\"\"\"\n    pass\n\n# Create a custom safe Repr instance and increase its maxstring.\n# The default of 30 truncates error messages too easily.\n_repr = Repr()\n_repr.maxstring = 200\n_saferepr = _repr.repr\n\n__all__ = [\"run\", \"pm\", \"Pdb\", \"runeval\", \"runctx\", \"runcall\", \"set_trace\",\n           \"post_mortem\", \"help\"]\n\ndef find_function(funcname, filename):\n    cre = re.compile(r'def\\s+%s\\s*[(]' % re.escape(funcname))\n    try:\n        fp = open(filename)\n    except IOError:\n        return None\n    # consumer of this info expects the first line to be 1\n    lineno = 1\n    answer = None\n    while 1:\n        line = fp.readline()\n        if line == '':\n            break\n        if cre.match(line):\n            answer = funcname, filename, lineno\n            break\n        lineno = lineno + 1\n    fp.close()\n    return answer\n\n\n# Interaction prompt line will separate file and call info from code\n# text using value of line_prefix string.  A newline and arrow may\n# be to your liking.  You can set it once pdb is imported using the\n# command \"pdb.line_prefix = '\\n% '\".\n# line_prefix = ': '    # Use this to get the old situation back\nline_prefix = '\\n-> '   # Probably a better default\n\nclass Pdb(bdb.Bdb, cmd.Cmd):\n\n    def __init__(self, completekey='tab', stdin=None, stdout=None, skip=None):\n        bdb.Bdb.__init__(self, skip=skip)\n        cmd.Cmd.__init__(self, completekey, stdin, stdout)\n        if stdout:\n            self.use_rawinput = 0\n        self.prompt = '(Pdb) '\n        self.aliases = {}\n        self.mainpyfile = ''\n        self._wait_for_mainpyfile = 0\n        # Try to load readline if it exists\n        try:\n            import readline\n        except ImportError:\n            pass\n\n        # Read $HOME/.pdbrc and ./.pdbrc\n        self.rcLines = []\n        if 'HOME' in os.environ:\n            envHome = os.environ['HOME']\n            try:\n                rcFile = open(os.path.join(envHome, \".pdbrc\"))\n            except IOError:\n                pass\n            else:\n                for line in rcFile.readlines():\n                    self.rcLines.append(line)\n                rcFile.close()\n        try:\n            rcFile = open(\".pdbrc\")\n        except IOError:\n            pass\n        else:\n            for line in rcFile.readlines():\n                self.rcLines.append(line)\n            rcFile.close()\n\n        self.commands = {} # associates a command list to breakpoint numbers\n        self.commands_doprompt = {} # for each bp num, tells if the prompt\n                                    # must be disp. after execing the cmd list\n        self.commands_silent = {} # for each bp num, tells if the stack trace\n                                  # must be disp. after execing the cmd list\n        self.commands_defining = False # True while in the process of defining\n                                       # a command list\n        self.commands_bnum = None # The breakpoint number for which we are\n                                  # defining a list\n\n    def reset(self):\n        bdb.Bdb.reset(self)\n        self.forget()\n\n    def forget(self):\n        self.lineno = None\n        self.stack = []\n        self.curindex = 0\n        self.curframe = None\n\n    def setup(self, f, t):\n        self.forget()\n        self.stack, self.curindex = self.get_stack(f, t)\n        self.curframe = self.stack[self.curindex][0]\n        # The f_locals dictionary is updated from the actual frame\n        # locals whenever the .f_locals accessor is called, so we\n        # cache it here to ensure that modifications are not overwritten.\n        self.curframe_locals = self.curframe.f_locals\n        self.execRcLines()\n\n    # Can be executed earlier than 'setup' if desired\n    def execRcLines(self):\n        if self.rcLines:\n            # Make local copy because of recursion\n            rcLines = self.rcLines\n            # executed only once\n            self.rcLines = []\n            for line in rcLines:\n                line = line[:-1]\n                if len(line) > 0 and line[0] != '#':\n                    self.onecmd(line)\n\n    # Override Bdb methods\n\n    def user_call(self, frame, argument_list):\n        \"\"\"This method is called when there is the remote possibility\n        that we ever need to stop in this function.\"\"\"\n        if self._wait_for_mainpyfile:\n            return\n        if self.stop_here(frame):\n            print >>self.stdout, '--Call--'\n            self.interaction(frame, None)\n\n    def user_line(self, frame):\n        \"\"\"This function is called when we stop or break at this line.\"\"\"\n        if self._wait_for_mainpyfile:\n            if (self.mainpyfile != self.canonic(frame.f_code.co_filename)\n                or frame.f_lineno<= 0):\n                return\n            self._wait_for_mainpyfile = 0\n        if self.bp_commands(frame):\n            self.interaction(frame, None)\n\n    def bp_commands(self,frame):\n        \"\"\"Call every command that was set for the current active breakpoint\n        (if there is one).\n\n        Returns True if the normal interaction function must be called,\n        False otherwise.\"\"\"\n        # self.currentbp is set in bdb in Bdb.break_here if a breakpoint was hit\n        if getattr(self, \"currentbp\", False) and \\\n               self.currentbp in self.commands:\n            currentbp = self.currentbp\n            self.currentbp = 0\n            lastcmd_back = self.lastcmd\n            self.setup(frame, None)\n            for line in self.commands[currentbp]:\n                self.onecmd(line)\n            self.lastcmd = lastcmd_back\n            if not self.commands_silent[currentbp]:\n                self.print_stack_entry(self.stack[self.curindex])\n            if self.commands_doprompt[currentbp]:\n                self.cmdloop()\n            self.forget()\n            return\n        return 1\n\n    def user_return(self, frame, return_value):\n        \"\"\"This function is called when a return trap is set here.\"\"\"\n        if self._wait_for_mainpyfile:\n            return\n        frame.f_locals['__return__'] = return_value\n        print >>self.stdout, '--Return--'\n        self.interaction(frame, None)\n\n    def user_exception(self, frame, exc_info):\n        \"\"\"This function is called if an exception occurs,\n        but only if we are to stop at or just below this level.\"\"\"\n        if self._wait_for_mainpyfile:\n            return\n        exc_type, exc_value, exc_traceback = exc_info\n        frame.f_locals['__exception__'] = exc_type, exc_value\n        if type(exc_type) == type(''):\n            exc_type_name = exc_type\n        else: exc_type_name = exc_type.__name__\n        print >>self.stdout, exc_type_name + ':', _saferepr(exc_value)\n        self.interaction(frame, exc_traceback)\n\n    # General interaction function\n\n    def interaction(self, frame, traceback):\n        self.setup(frame, traceback)\n        self.print_stack_entry(self.stack[self.curindex])\n        self.cmdloop()\n        self.forget()\n\n    def displayhook(self, obj):\n        \"\"\"Custom displayhook for the exec in default(), which prevents\n        assignment of the _ variable in the builtins.\n        \"\"\"\n        # reproduce the behavior of the standard displayhook, not printing None\n        if obj is not None:\n            print repr(obj)\n\n    def default(self, line):\n        if line[:1] == '!': line = line[1:]\n        locals = self.curframe_locals\n        globals = self.curframe.f_globals\n        try:\n            code = compile(line + '\\n', '<stdin>', 'single')\n            save_stdout = sys.stdout\n            save_stdin = sys.stdin\n            save_displayhook = sys.displayhook\n            try:\n                sys.stdin = self.stdin\n                sys.stdout = self.stdout\n                sys.displayhook = self.displayhook\n                exec code in globals, locals\n            finally:\n                sys.stdout = save_stdout\n                sys.stdin = save_stdin\n                sys.displayhook = save_displayhook\n        except:\n            t, v = sys.exc_info()[:2]\n            if type(t) == type(''):\n                exc_type_name = t\n            else: exc_type_name = t.__name__\n            print >>self.stdout, '***', exc_type_name + ':', v\n\n    def precmd(self, line):\n        \"\"\"Handle alias expansion and ';;' separator.\"\"\"\n        if not line.strip():\n            return line\n        args = line.split()\n        while args[0] in self.aliases:\n            line = self.aliases[args[0]]\n            ii = 1\n            for tmpArg in args[1:]:\n                line = line.replace(\"%\" + str(ii),\n                                      tmpArg)\n                ii = ii + 1\n            line = line.replace(\"%*\", ' '.join(args[1:]))\n            args = line.split()\n        # split into ';;' separated commands\n        # unless it's an alias command\n        if args[0] != 'alias':\n            marker = line.find(';;')\n            if marker >= 0:\n                # queue up everything after marker\n                next = line[marker+2:].lstrip()\n                self.cmdqueue.append(next)\n                line = line[:marker].rstrip()\n        return line\n\n    def onecmd(self, line):\n        \"\"\"Interpret the argument as though it had been typed in response\n        to the prompt.\n\n        Checks whether this line is typed at the normal prompt or in\n        a breakpoint command list definition.\n        \"\"\"\n        if not self.commands_defining:\n            return cmd.Cmd.onecmd(self, line)\n        else:\n            return self.handle_command_def(line)\n\n    def handle_command_def(self,line):\n        \"\"\"Handles one command line during command list definition.\"\"\"\n        cmd, arg, line = self.parseline(line)\n        if not cmd:\n            return\n        if cmd == 'silent':\n            self.commands_silent[self.commands_bnum] = True\n            return # continue to handle other cmd def in the cmd list\n        elif cmd == 'end':\n            self.cmdqueue = []\n            return 1 # end of cmd list\n        cmdlist = self.commands[self.commands_bnum]\n        if arg:\n            cmdlist.append(cmd+' '+arg)\n        else:\n            cmdlist.append(cmd)\n        # Determine if we must stop\n        try:\n            func = getattr(self, 'do_' + cmd)\n        except AttributeError:\n            func = self.default\n        # one of the resuming commands\n        if func.func_name in self.commands_resuming:\n            self.commands_doprompt[self.commands_bnum] = False\n            self.cmdqueue = []\n            return 1\n        return\n\n    # Command definitions, called by cmdloop()\n    # The argument is the remaining string on the command line\n    # Return true to exit from the command loop\n\n    do_h = cmd.Cmd.do_help\n\n    def do_commands(self, arg):\n        \"\"\"Defines a list of commands associated to a breakpoint.\n\n        Those commands will be executed whenever the breakpoint causes\n        the program to stop execution.\"\"\"\n        if not arg:\n            bnum = len(bdb.Breakpoint.bpbynumber)-1\n        else:\n            try:\n                bnum = int(arg)\n            except:\n                print >>self.stdout, \"Usage : commands [bnum]\\n        ...\" \\\n                                     \"\\n        end\"\n                return\n        self.commands_bnum = bnum\n        self.commands[bnum] = []\n        self.commands_doprompt[bnum] = True\n        self.commands_silent[bnum] = False\n        prompt_back = self.prompt\n        self.prompt = '(com) '\n        self.commands_defining = True\n        try:\n            self.cmdloop()\n        finally:\n            self.commands_defining = False\n            self.prompt = prompt_back\n\n    def do_break(self, arg, temporary = 0):\n        # break [ ([filename:]lineno | function) [, \"condition\"] ]\n        if not arg:\n            if self.breaks:  # There's at least one\n                print >>self.stdout, \"Num Type         Disp Enb   Where\"\n                for bp in bdb.Breakpoint.bpbynumber:\n                    if bp:\n                        bp.bpprint(self.stdout)\n            return\n        # parse arguments; comma has lowest precedence\n        # and cannot occur in filename\n        filename = None\n        lineno = None\n        cond = None\n        comma = arg.find(',')\n        if comma > 0:\n            # parse stuff after comma: \"condition\"\n            cond = arg[comma+1:].lstrip()\n            arg = arg[:comma].rstrip()\n        # parse stuff before comma: [filename:]lineno | function\n        colon = arg.rfind(':')\n        funcname = None\n        if colon >= 0:\n            filename = arg[:colon].rstrip()\n            f = self.lookupmodule(filename)\n            if not f:\n                print >>self.stdout, '*** ', repr(filename),\n                print >>self.stdout, 'not found from sys.path'\n                return\n            else:\n                filename = f\n            arg = arg[colon+1:].lstrip()\n            try:\n                lineno = int(arg)\n            except ValueError, msg:\n                print >>self.stdout, '*** Bad lineno:', arg\n                return\n        else:\n            # no colon; can be lineno or function\n            try:\n                lineno = int(arg)\n            except ValueError:\n                try:\n                    func = eval(arg,\n                                self.curframe.f_globals,\n                                self.curframe_locals)\n                except:\n                    func = arg\n                try:\n                    if hasattr(func, 'im_func'):\n                        func = func.im_func\n                    code = func.func_code\n                    #use co_name to identify the bkpt (function names\n                    #could be aliased, but co_name is invariant)\n                    funcname = code.co_name\n                    lineno = code.co_firstlineno\n                    filename = code.co_filename\n                except:\n                    # last thing to try\n                    (ok, filename, ln) = self.lineinfo(arg)\n                    if not ok:\n                        print >>self.stdout, '*** The specified object',\n                        print >>self.stdout, repr(arg),\n                        print >>self.stdout, 'is not a function'\n                        print >>self.stdout, 'or was not found along sys.path.'\n                        return\n                    funcname = ok # ok contains a function name\n                    lineno = int(ln)\n        if not filename:\n            filename = self.defaultFile()\n        # Check for reasonable breakpoint\n        line = self.checkline(filename, lineno)\n        if line:\n            # now set the break point\n            err = self.set_break(filename, line, temporary, cond, funcname)\n            if err: print >>self.stdout, '***', err\n            else:\n                bp = self.get_breaks(filename, line)[-1]\n                print >>self.stdout, \"Breakpoint %d at %s:%d\" % (bp.number,\n                                                                 bp.file,\n                                                                 bp.line)\n\n    # To be overridden in derived debuggers\n    def defaultFile(self):\n        \"\"\"Produce a reasonable default.\"\"\"\n        filename = self.curframe.f_code.co_filename\n        if filename == '<string>' and self.mainpyfile:\n            filename = self.mainpyfile\n        return filename\n\n    do_b = do_break\n\n    def do_tbreak(self, arg):\n        self.do_break(arg, 1)\n\n    def lineinfo(self, identifier):\n        failed = (None, None, None)\n        # Input is identifier, may be in single quotes\n        idstring = identifier.split(\"'\")\n        if len(idstring) == 1:\n            # not in single quotes\n            id = idstring[0].strip()\n        elif len(idstring) == 3:\n            # quoted\n            id = idstring[1].strip()\n        else:\n            return failed\n        if id == '': return failed\n        parts = id.split('.')\n        # Protection for derived debuggers\n        if parts[0] == 'self':\n            del parts[0]\n            if len(parts) == 0:\n                return failed\n        # Best first guess at file to look at\n        fname = self.defaultFile()\n        if len(parts) == 1:\n            item = parts[0]\n        else:\n            # More than one part.\n            # First is module, second is method/class\n            f = self.lookupmodule(parts[0])\n            if f:\n                fname = f\n            item = parts[1]\n        answer = find_function(item, fname)\n        return answer or failed\n\n    def checkline(self, filename, lineno):\n        \"\"\"Check whether specified line seems to be executable.\n\n        Return `lineno` if it is, 0 if not (e.g. a docstring, comment, blank\n        line or EOF). Warning: testing is not comprehensive.\n        \"\"\"\n        # this method should be callable before starting debugging, so default\n        # to \"no globals\" if there is no current frame\n        globs = self.curframe.f_globals if hasattr(self, 'curframe') else None\n        line = linecache.getline(filename, lineno, globs)\n        if not line:\n            print >>self.stdout, 'End of file'\n            return 0\n        line = line.strip()\n        # Don't allow setting breakpoint at a blank line\n        if (not line or (line[0] == '#') or\n             (line[:3] == '\"\"\"') or line[:3] == \"'''\"):\n            print >>self.stdout, '*** Blank or comment'\n            return 0\n        return lineno\n\n    def do_enable(self, arg):\n        args = arg.split()\n        for i in args:\n            try:\n                i = int(i)\n            except ValueError:\n                print >>self.stdout, 'Breakpoint index %r is not a number' % i\n                continue\n\n            if not (0 <= i < len(bdb.Breakpoint.bpbynumber)):\n                print >>self.stdout, 'No breakpoint numbered', i\n                continue\n\n            bp = bdb.Breakpoint.bpbynumber[i]\n            if bp:\n                bp.enable()\n\n    def do_disable(self, arg):\n        args = arg.split()\n        for i in args:\n            try:\n                i = int(i)\n            except ValueError:\n                print >>self.stdout, 'Breakpoint index %r is not a number' % i\n                continue\n\n            if not (0 <= i < len(bdb.Breakpoint.bpbynumber)):\n                print >>self.stdout, 'No breakpoint numbered', i\n                continue\n\n            bp = bdb.Breakpoint.bpbynumber[i]\n            if bp:\n                bp.disable()\n\n    def do_condition(self, arg):\n        # arg is breakpoint number and condition\n        args = arg.split(' ', 1)\n        try:\n            bpnum = int(args[0].strip())\n        except ValueError:\n            # something went wrong\n            print >>self.stdout, \\\n                'Breakpoint index %r is not a number' % args[0]\n            return\n        try:\n            cond = args[1]\n        except:\n            cond = None\n        try:\n            bp = bdb.Breakpoint.bpbynumber[bpnum]\n        except IndexError:\n            print >>self.stdout, 'Breakpoint index %r is not valid' % args[0]\n            return\n        if bp:\n            bp.cond = cond\n            if not cond:\n                print >>self.stdout, 'Breakpoint', bpnum,\n                print >>self.stdout, 'is now unconditional.'\n\n    def do_ignore(self,arg):\n        \"\"\"arg is bp number followed by ignore count.\"\"\"\n        args = arg.split()\n        try:\n            bpnum = int(args[0].strip())\n        except ValueError:\n            # something went wrong\n            print >>self.stdout, \\\n                'Breakpoint index %r is not a number' % args[0]\n            return\n        try:\n            count = int(args[1].strip())\n        except:\n            count = 0\n        try:\n            bp = bdb.Breakpoint.bpbynumber[bpnum]\n        except IndexError:\n            print >>self.stdout, 'Breakpoint index %r is not valid' % args[0]\n            return\n        if bp:\n            bp.ignore = count\n            if count > 0:\n                reply = 'Will ignore next '\n                if count > 1:\n                    reply = reply + '%d crossings' % count\n                else:\n                    reply = reply + '1 crossing'\n                print >>self.stdout, reply + ' of breakpoint %d.' % bpnum\n            else:\n                print >>self.stdout, 'Will stop next time breakpoint',\n                print >>self.stdout, bpnum, 'is reached.'\n\n    def do_clear(self, arg):\n        \"\"\"Three possibilities, tried in this order:\n        clear -> clear all breaks, ask for confirmation\n        clear file:lineno -> clear all breaks at file:lineno\n        clear bpno bpno ... -> clear breakpoints by number\"\"\"\n        if not arg:\n            try:\n                reply = raw_input('Clear all breaks? ')\n            except EOFError:\n                reply = 'no'\n            reply = reply.strip().lower()\n            if reply in ('y', 'yes'):\n                self.clear_all_breaks()\n            return\n        if ':' in arg:\n            # Make sure it works for \"clear C:\\foo\\bar.py:12\"\n            i = arg.rfind(':')\n            filename = arg[:i]\n            arg = arg[i+1:]\n            try:\n                lineno = int(arg)\n            except ValueError:\n                err = \"Invalid line number (%s)\" % arg\n            else:\n                err = self.clear_break(filename, lineno)\n            if err: print >>self.stdout, '***', err\n            return\n        numberlist = arg.split()\n        for i in numberlist:\n            try:\n                i = int(i)\n            except ValueError:\n                print >>self.stdout, 'Breakpoint index %r is not a number' % i\n                continue\n\n            if not (0 <= i < len(bdb.Breakpoint.bpbynumber)):\n                print >>self.stdout, 'No breakpoint numbered', i\n                continue\n            err = self.clear_bpbynumber(i)\n            if err:\n                print >>self.stdout, '***', err\n            else:\n                print >>self.stdout, 'Deleted breakpoint', i\n    do_cl = do_clear # 'c' is already an abbreviation for 'continue'\n\n    def do_where(self, arg):\n        self.print_stack_trace()\n    do_w = do_where\n    do_bt = do_where\n\n    def do_up(self, arg):\n        if self.curindex == 0:\n            print >>self.stdout, '*** Oldest frame'\n        else:\n            self.curindex = self.curindex - 1\n            self.curframe = self.stack[self.curindex][0]\n            self.curframe_locals = self.curframe.f_locals\n            self.print_stack_entry(self.stack[self.curindex])\n            self.lineno = None\n    do_u = do_up\n\n    def do_down(self, arg):\n        if self.curindex + 1 == len(self.stack):\n            print >>self.stdout, '*** Newest frame'\n        else:\n            self.curindex = self.curindex + 1\n            self.curframe = self.stack[self.curindex][0]\n            self.curframe_locals = self.curframe.f_locals\n            self.print_stack_entry(self.stack[self.curindex])\n            self.lineno = None\n    do_d = do_down\n\n    def do_until(self, arg):\n        self.set_until(self.curframe)\n        return 1\n    do_unt = do_until\n\n    def do_step(self, arg):\n        self.set_step()\n        return 1\n    do_s = do_step\n\n    def do_next(self, arg):\n        self.set_next(self.curframe)\n        return 1\n    do_n = do_next\n\n    def do_run(self, arg):\n        \"\"\"Restart program by raising an exception to be caught in the main\n        debugger loop.  If arguments were given, set them in sys.argv.\"\"\"\n        if arg:\n            import shlex\n            argv0 = sys.argv[0:1]\n            sys.argv = shlex.split(arg)\n            sys.argv[:0] = argv0\n        raise Restart\n\n    do_restart = do_run\n\n    def do_return(self, arg):\n        self.set_return(self.curframe)\n        return 1\n    do_r = do_return\n\n    def do_continue(self, arg):\n        self.set_continue()\n        return 1\n    do_c = do_cont = do_continue\n\n    def do_jump(self, arg):\n        if self.curindex + 1 != len(self.stack):\n            print >>self.stdout, \"*** You can only jump within the bottom frame\"\n            return\n        try:\n            arg = int(arg)\n        except ValueError:\n            print >>self.stdout, \"*** The 'jump' command requires a line number.\"\n        else:\n            try:\n                # Do the jump, fix up our copy of the stack, and display the\n                # new position\n                self.curframe.f_lineno = arg\n                self.stack[self.curindex] = self.stack[self.curindex][0], arg\n                self.print_stack_entry(self.stack[self.curindex])\n            except ValueError, e:\n                print >>self.stdout, '*** Jump failed:', e\n    do_j = do_jump\n\n    def do_debug(self, arg):\n        sys.settrace(None)\n        globals = self.curframe.f_globals\n        locals = self.curframe_locals\n        p = Pdb(self.completekey, self.stdin, self.stdout)\n        p.prompt = \"(%s) \" % self.prompt.strip()\n        print >>self.stdout, \"ENTERING RECURSIVE DEBUGGER\"\n        sys.call_tracing(p.run, (arg, globals, locals))\n        print >>self.stdout, \"LEAVING RECURSIVE DEBUGGER\"\n        sys.settrace(self.trace_dispatch)\n        self.lastcmd = p.lastcmd\n\n    def do_quit(self, arg):\n        self._user_requested_quit = 1\n        self.set_quit()\n        return 1\n\n    do_q = do_quit\n    do_exit = do_quit\n\n    def do_EOF(self, arg):\n        print >>self.stdout\n        self._user_requested_quit = 1\n        self.set_quit()\n        return 1\n\n    def do_args(self, arg):\n        co = self.curframe.f_code\n        dict = self.curframe_locals\n        n = co.co_argcount\n        if co.co_flags & 4: n = n+1\n        if co.co_flags & 8: n = n+1\n        for i in range(n):\n            name = co.co_varnames[i]\n            print >>self.stdout, name, '=',\n            if name in dict: print >>self.stdout, dict[name]\n            else: print >>self.stdout, \"*** undefined ***\"\n    do_a = do_args\n\n    def do_retval(self, arg):\n        if '__return__' in self.curframe_locals:\n            print >>self.stdout, self.curframe_locals['__return__']\n        else:\n            print >>self.stdout, '*** Not yet returned!'\n    do_rv = do_retval\n\n    def _getval(self, arg):\n        try:\n            return eval(arg, self.curframe.f_globals,\n                        self.curframe_locals)\n        except:\n            t, v = sys.exc_info()[:2]\n            if isinstance(t, str):\n                exc_type_name = t\n            else: exc_type_name = t.__name__\n            print >>self.stdout, '***', exc_type_name + ':', repr(v)\n            raise\n\n    def do_p(self, arg):\n        try:\n            print >>self.stdout, repr(self._getval(arg))\n        except:\n            pass\n\n    def do_pp(self, arg):\n        try:\n            pprint.pprint(self._getval(arg), self.stdout)\n        except:\n            pass\n\n    def do_list(self, arg):\n        self.lastcmd = 'list'\n        last = None\n        if arg:\n            try:\n                x = eval(arg, {}, {})\n                if type(x) == type(()):\n                    first, last = x\n                    first = int(first)\n                    last = int(last)\n                    if last < first:\n                        # Assume it's a count\n                        last = first + last\n                else:\n                    first = max(1, int(x) - 5)\n            except:\n                print >>self.stdout, '*** Error in argument:', repr(arg)\n                return\n        elif self.lineno is None:\n            first = max(1, self.curframe.f_lineno - 5)\n        else:\n            first = self.lineno + 1\n        if last is None:\n            last = first + 10\n        filename = self.curframe.f_code.co_filename\n        breaklist = self.get_file_breaks(filename)\n        try:\n            for lineno in range(first, last+1):\n                line = linecache.getline(filename, lineno,\n                                         self.curframe.f_globals)\n                if not line:\n                    print >>self.stdout, '[EOF]'\n                    break\n                else:\n                    s = repr(lineno).rjust(3)\n                    if len(s) < 4: s = s + ' '\n                    if lineno in breaklist: s = s + 'B'\n                    else: s = s + ' '\n                    if lineno == self.curframe.f_lineno:\n                        s = s + '->'\n                    print >>self.stdout, s + '\\t' + line,\n                    self.lineno = lineno\n        except KeyboardInterrupt:\n            pass\n    do_l = do_list\n\n    def do_whatis(self, arg):\n        try:\n            value = eval(arg, self.curframe.f_globals,\n                            self.curframe_locals)\n        except:\n            t, v = sys.exc_info()[:2]\n            if type(t) == type(''):\n                exc_type_name = t\n            else: exc_type_name = t.__name__\n            print >>self.stdout, '***', exc_type_name + ':', repr(v)\n            return\n        code = None\n        # Is it a function?\n        try: code = value.func_code\n        except: pass\n        if code:\n            print >>self.stdout, 'Function', code.co_name\n            return\n        # Is it an instance method?\n        try: code = value.im_func.func_code\n        except: pass\n        if code:\n            print >>self.stdout, 'Method', code.co_name\n            return\n        # None of the above...\n        print >>self.stdout, type(value)\n\n    def do_alias(self, arg):\n        args = arg.split()\n        if len(args) == 0:\n            keys = self.aliases.keys()\n            keys.sort()\n            for alias in keys:\n                print >>self.stdout, \"%s = %s\" % (alias, self.aliases[alias])\n            return\n        if args[0] in self.aliases and len(args) == 1:\n            print >>self.stdout, \"%s = %s\" % (args[0], self.aliases[args[0]])\n        else:\n            self.aliases[args[0]] = ' '.join(args[1:])\n\n    def do_unalias(self, arg):\n        args = arg.split()\n        if len(args) == 0: return\n        if args[0] in self.aliases:\n            del self.aliases[args[0]]\n\n    #list of all the commands making the program resume execution.\n    commands_resuming = ['do_continue', 'do_step', 'do_next', 'do_return',\n                         'do_quit', 'do_jump']\n\n    # Print a traceback starting at the top stack frame.\n    # The most recently entered frame is printed last;\n    # this is different from dbx and gdb, but consistent with\n    # the Python interpreter's stack trace.\n    # It is also consistent with the up/down commands (which are\n    # compatible with dbx and gdb: up moves towards 'main()'\n    # and down moves towards the most recent stack frame).\n\n    def print_stack_trace(self):\n        try:\n            for frame_lineno in self.stack:\n                self.print_stack_entry(frame_lineno)\n        except KeyboardInterrupt:\n            pass\n\n    def print_stack_entry(self, frame_lineno, prompt_prefix=line_prefix):\n        frame, lineno = frame_lineno\n        if frame is self.curframe:\n            print >>self.stdout, '>',\n        else:\n            print >>self.stdout, ' ',\n        print >>self.stdout, self.format_stack_entry(frame_lineno,\n                                                     prompt_prefix)\n\n\n    # Help methods (derived from pdb.doc)\n\n    def help_help(self):\n        self.help_h()\n\n    def help_h(self):\n        print >>self.stdout, \"\"\"h(elp)\nWithout argument, print the list of available commands.\nWith a command name as argument, print help about that command\n\"help pdb\" pipes the full documentation file to the $PAGER\n\"help exec\" gives help on the ! command\"\"\"\n\n    def help_where(self):\n        self.help_w()\n\n    def help_w(self):\n        print >>self.stdout, \"\"\"w(here)\nPrint a stack trace, with the most recent frame at the bottom.\nAn arrow indicates the \"current frame\", which determines the\ncontext of most commands.  'bt' is an alias for this command.\"\"\"\n\n    help_bt = help_w\n\n    def help_down(self):\n        self.help_d()\n\n    def help_d(self):\n        print >>self.stdout, \"\"\"d(own)\nMove the current frame one level down in the stack trace\n(to a newer frame).\"\"\"\n\n    def help_up(self):\n        self.help_u()\n\n    def help_u(self):\n        print >>self.stdout, \"\"\"u(p)\nMove the current frame one level up in the stack trace\n(to an older frame).\"\"\"\n\n    def help_break(self):\n        self.help_b()\n\n    def help_b(self):\n        print >>self.stdout, \"\"\"b(reak) ([file:]lineno | function) [, condition]\nWith a line number argument, set a break there in the current\nfile.  With a function name, set a break at first executable line\nof that function.  Without argument, list all breaks.  If a second\nargument is present, it is a string specifying an expression\nwhich must evaluate to true before the breakpoint is honored.\n\nThe line number may be prefixed with a filename and a colon,\nto specify a breakpoint in another file (probably one that\nhasn't been loaded yet).  The file is searched for on sys.path;\nthe .py suffix may be omitted.\"\"\"\n\n    def help_clear(self):\n        self.help_cl()\n\n    def help_cl(self):\n        print >>self.stdout, \"cl(ear) filename:lineno\"\n        print >>self.stdout, \"\"\"cl(ear) [bpnumber [bpnumber...]]\nWith a space separated list of breakpoint numbers, clear\nthose breakpoints.  Without argument, clear all breaks (but\nfirst ask confirmation).  With a filename:lineno argument,\nclear all breaks at that line in that file.\n\nNote that the argument is different from previous versions of\nthe debugger (in python distributions 1.5.1 and before) where\na linenumber was used instead of either filename:lineno or\nbreakpoint numbers.\"\"\"\n\n    def help_tbreak(self):\n        print >>self.stdout, \"\"\"tbreak  same arguments as break, but breakpoint\nis removed when first hit.\"\"\"\n\n    def help_enable(self):\n        print >>self.stdout, \"\"\"enable bpnumber [bpnumber ...]\nEnables the breakpoints given as a space separated list of\nbp numbers.\"\"\"\n\n    def help_disable(self):\n        print >>self.stdout, \"\"\"disable bpnumber [bpnumber ...]\nDisables the breakpoints given as a space separated list of\nbp numbers.\"\"\"\n\n    def help_ignore(self):\n        print >>self.stdout, \"\"\"ignore bpnumber count\nSets the ignore count for the given breakpoint number.  A breakpoint\nbecomes active when the ignore count is zero.  When non-zero, the\ncount is decremented each time the breakpoint is reached and the\nbreakpoint is not disabled and any associated condition evaluates\nto true.\"\"\"\n\n    def help_condition(self):\n        print >>self.stdout, \"\"\"condition bpnumber str_condition\nstr_condition is a string specifying an expression which\nmust evaluate to true before the breakpoint is honored.\nIf str_condition is absent, any existing condition is removed;\ni.e., the breakpoint is made unconditional.\"\"\"\n\n    def help_step(self):\n        self.help_s()\n\n    def help_s(self):\n        print >>self.stdout, \"\"\"s(tep)\nExecute the current line, stop at the first possible occasion\n(either in a function that is called or in the current function).\"\"\"\n\n    def help_until(self):\n        self.help_unt()\n\n    def help_unt(self):\n        print \"\"\"unt(il)\nContinue execution until the line with a number greater than the current\none is reached or until the current frame returns\"\"\"\n\n    def help_next(self):\n        self.help_n()\n\n    def help_n(self):\n        print >>self.stdout, \"\"\"n(ext)\nContinue execution until the next line in the current function\nis reached or it returns.\"\"\"\n\n    def help_return(self):\n        self.help_r()\n\n    def help_r(self):\n        print >>self.stdout, \"\"\"r(eturn)\nContinue execution until the current function returns.\"\"\"\n\n    def help_continue(self):\n        self.help_c()\n\n    def help_cont(self):\n        self.help_c()\n\n    def help_c(self):\n        print >>self.stdout, \"\"\"c(ont(inue))\nContinue execution, only stop when a breakpoint is encountered.\"\"\"\n\n    def help_jump(self):\n        self.help_j()\n\n    def help_j(self):\n        print >>self.stdout, \"\"\"j(ump) lineno\nSet the next line that will be executed.\"\"\"\n\n    def help_debug(self):\n        print >>self.stdout, \"\"\"debug code\nEnter a recursive debugger that steps through the code argument\n(which is an arbitrary expression or statement to be executed\nin the current environment).\"\"\"\n\n    def help_list(self):\n        self.help_l()\n\n    def help_l(self):\n        print >>self.stdout, \"\"\"l(ist) [first [,last]]\nList source code for the current file.\nWithout arguments, list 11 lines around the current line\nor continue the previous listing.\nWith one argument, list 11 lines starting at that line.\nWith two arguments, list the given range;\nif the second argument is less than the first, it is a count.\"\"\"\n\n    def help_args(self):\n        self.help_a()\n\n    def help_a(self):\n        print >>self.stdout, \"\"\"a(rgs)\nPrint the arguments of the current function.\"\"\"\n\n    def help_p(self):\n        print >>self.stdout, \"\"\"p expression\nPrint the value of the expression.\"\"\"\n\n    def help_pp(self):\n        print >>self.stdout, \"\"\"pp expression\nPretty-print the value of the expression.\"\"\"\n\n    def help_exec(self):\n        print >>self.stdout, \"\"\"(!) statement\nExecute the (one-line) statement in the context of\nthe current stack frame.\nThe exclamation point can be omitted unless the first word\nof the statement resembles a debugger command.\nTo assign to a global variable you must always prefix the\ncommand with a 'global' command, e.g.:\n(Pdb) global list_options; list_options = ['-l']\n(Pdb)\"\"\"\n\n    def help_run(self):\n        print \"\"\"run [args...]\nRestart the debugged python program. If a string is supplied, it is\nsplit with \"shlex\" and the result is used as the new sys.argv.\nHistory, breakpoints, actions and debugger options are preserved.\n\"restart\" is an alias for \"run\".\"\"\"\n\n    help_restart = help_run\n\n    def help_quit(self):\n        self.help_q()\n\n    def help_q(self):\n        print >>self.stdout, \"\"\"q(uit) or exit - Quit from the debugger.\nThe program being executed is aborted.\"\"\"\n\n    help_exit = help_q\n\n    def help_whatis(self):\n        print >>self.stdout, \"\"\"whatis arg\nPrints the type of the argument.\"\"\"\n\n    def help_EOF(self):\n        print >>self.stdout, \"\"\"EOF\nHandles the receipt of EOF as a command.\"\"\"\n\n    def help_alias(self):\n        print >>self.stdout, \"\"\"alias [name [command [parameter parameter ...]]]\nCreates an alias called 'name' the executes 'command'.  The command\nmust *not* be enclosed in quotes.  Replaceable parameters are\nindicated by %1, %2, and so on, while %* is replaced by all the\nparameters.  If no command is given, the current alias for name\nis shown. If no name is given, all aliases are listed.\n\nAliases may be nested and can contain anything that can be\nlegally typed at the pdb prompt.  Note!  You *can* override\ninternal pdb commands with aliases!  Those internal commands\nare then hidden until the alias is removed.  Aliasing is recursively\napplied to the first word of the command line; all other words\nin the line are left alone.\n\nSome useful aliases (especially when placed in the .pdbrc file) are:\n\n#Print instance variables (usage \"pi classInst\")\nalias pi for k in %1.__dict__.keys(): print \"%1.\",k,\"=\",%1.__dict__[k]\n\n#Print instance variables in self\nalias ps pi self\n\"\"\"\n\n    def help_unalias(self):\n        print >>self.stdout, \"\"\"unalias name\nDeletes the specified alias.\"\"\"\n\n    def help_commands(self):\n        print >>self.stdout, \"\"\"commands [bpnumber]\n(com) ...\n(com) end\n(Pdb)\n\nSpecify a list of commands for breakpoint number bpnumber.  The\ncommands themselves appear on the following lines.  Type a line\ncontaining just 'end' to terminate the commands.\n\nTo remove all commands from a breakpoint, type commands and\nfollow it immediately with  end; that is, give no commands.\n\nWith no bpnumber argument, commands refers to the last\nbreakpoint set.\n\nYou can use breakpoint commands to start your program up again.\nSimply use the continue command, or step, or any other\ncommand that resumes execution.\n\nSpecifying any command resuming execution (currently continue,\nstep, next, return, jump, quit and their abbreviations) terminates\nthe command list (as if that command was immediately followed by end).\nThis is because any time you resume execution\n(even with a simple next or step), you may encounter\nanother breakpoint--which could have its own command list, leading to\nambiguities about which list to execute.\n\n   If you use the 'silent' command in the command list, the\nusual message about stopping at a breakpoint is not printed.  This may\nbe desirable for breakpoints that are to print a specific message and\nthen continue.  If none of the other commands print anything, you\nsee no sign that the breakpoint was reached.\n\"\"\"\n\n    def help_pdb(self):\n        help()\n\n    def lookupmodule(self, filename):\n        \"\"\"Helper function for break/clear parsing -- may be overridden.\n\n        lookupmodule() translates (possibly incomplete) file or module name\n        into an absolute file name.\n        \"\"\"\n        if os.path.isabs(filename) and  os.path.exists(filename):\n            return filename\n        f = os.path.join(sys.path[0], filename)\n        if  os.path.exists(f) and self.canonic(f) == self.mainpyfile:\n            return f\n        root, ext = os.path.splitext(filename)\n        if ext == '':\n            filename = filename + '.py'\n        if os.path.isabs(filename):\n            return filename\n        for dirname in sys.path:\n            while os.path.islink(dirname):\n                dirname = os.readlink(dirname)\n            fullname = os.path.join(dirname, filename)\n            if os.path.exists(fullname):\n                return fullname\n        return None\n\n    def _runscript(self, filename):\n        # The script has to run in __main__ namespace (or imports from\n        # __main__ will break).\n        #\n        # So we clear up the __main__ and set several special variables\n        # (this gets rid of pdb's globals and cleans old variables on restarts).\n        import __main__\n        __main__.__dict__.clear()\n        __main__.__dict__.update({\"__name__\"    : \"__main__\",\n                                  \"__file__\"    : filename,\n                                  \"__builtins__\": __builtins__,\n                                 })\n\n        # When bdb sets tracing, a number of call and line events happens\n        # BEFORE debugger even reaches user's code (and the exact sequence of\n        # events depends on python version). So we take special measures to\n        # avoid stopping before we reach the main script (see user_line and\n        # user_call for details).\n        self._wait_for_mainpyfile = 1\n        self.mainpyfile = self.canonic(filename)\n        self._user_requested_quit = 0\n        statement = 'execfile(%r)' % filename\n        self.run(statement)\n\n# Simplified interface\n\ndef run(statement, globals=None, locals=None):\n    Pdb().run(statement, globals, locals)\n\ndef runeval(expression, globals=None, locals=None):\n    return Pdb().runeval(expression, globals, locals)\n\ndef runctx(statement, globals, locals):\n    # B/W compatibility\n    run(statement, globals, locals)\n\ndef runcall(*args, **kwds):\n    return Pdb().runcall(*args, **kwds)\n\ndef set_trace():\n    Pdb().set_trace(sys._getframe().f_back)\n\n# Post-Mortem interface\n\ndef post_mortem(t=None):\n    # handling the default\n    if t is None:\n        # sys.exc_info() returns (type, value, traceback) if an exception is\n        # being handled, otherwise it returns None\n        t = sys.exc_info()[2]\n        if t is None:\n            raise ValueError(\"A valid traceback must be passed if no \"\n                                               \"exception is being handled\")\n\n    p = Pdb()\n    p.reset()\n    p.interaction(None, t)\n\ndef pm():\n    post_mortem(sys.last_traceback)\n\n\n# Main program for testing\n\nTESTCMD = 'import x; x.main()'\n\ndef test():\n    run(TESTCMD)\n\n# print help\ndef help():\n    for dirname in sys.path:\n        fullname = os.path.join(dirname, 'pdb.doc')\n        if os.path.exists(fullname):\n            sts = os.system('${PAGER-more} '+fullname)\n            if sts: print '*** Pager exit status:', sts\n            break\n    else:\n        print 'Sorry, can\\'t find the help file \"pdb.doc\"',\n        print 'along the Python search path'\n\ndef main():\n    if not sys.argv[1:] or sys.argv[1] in (\"--help\", \"-h\"):\n        print \"usage: pdb.py scriptfile [arg] ...\"\n        sys.exit(2)\n\n    mainpyfile =  sys.argv[1]     # Get script filename\n    if not os.path.exists(mainpyfile):\n        print 'Error:', mainpyfile, 'does not exist'\n        sys.exit(1)\n\n    del sys.argv[0]         # Hide \"pdb.py\" from argument list\n\n    # Replace pdb's dir with script's dir in front of module search path.\n    sys.path[0] = os.path.dirname(mainpyfile)\n\n    # Note on saving/restoring sys.argv: it's a good idea when sys.argv was\n    # modified by the script being debugged. It's a bad idea when it was\n    # changed by the user from the command line. There is a \"restart\" command\n    # which allows explicit specification of command line arguments.\n    pdb = Pdb()\n    while True:\n        try:\n            pdb._runscript(mainpyfile)\n            if pdb._user_requested_quit:\n                break\n            print \"The program finished and will be restarted\"\n        except Restart:\n            print \"Restarting\", mainpyfile, \"with arguments:\"\n            print \"\\t\" + \" \".join(sys.argv[1:])\n        except SystemExit:\n            # In most cases SystemExit does not warrant a post-mortem session.\n            print \"The program exited via sys.exit(). Exit status: \",\n            print sys.exc_info()[1]\n        except:\n            traceback.print_exc()\n            print \"Uncaught exception. Entering post mortem debugging\"\n            print \"Running 'cont' or 'step' will restart the program\"\n            t = sys.exc_info()[2]\n            pdb.interaction(None, t)\n            print \"Post mortem debugger finished. The \" + mainpyfile + \\\n                  \" will be restarted\"\n\n\n# When invoked as main program, invoke the debugger on a script\nif __name__ == '__main__':\n    import pdb\n    pdb.main()\n",
		"file_name": "pdb.py"
	},
	{
		"content": "\"\"\"Debugger basics\"\"\"\n\nimport fnmatch\nimport sys\nimport os\nimport types\n\n__all__ = [\"BdbQuit\",\"Bdb\",\"Breakpoint\"]\n\nclass BdbQuit(Exception):\n    \"\"\"Exception to give up completely\"\"\"\n\n\nclass Bdb:\n\n    \"\"\"Generic Python debugger base class.\n\n    This class takes care of details of the trace facility;\n    a derived class should implement user interaction.\n    The standard debugger class (pdb.Pdb) is an example.\n    \"\"\"\n\n    def __init__(self, skip=None):\n        self.skip = set(skip) if skip else None\n        self.breaks = {}\n        self.fncache = {}\n        self.frame_returning = None\n\n    def canonic(self, filename):\n        if filename == \"<\" + filename[1:-1] + \">\":\n            return filename\n        canonic = self.fncache.get(filename)\n        if not canonic:\n            canonic = os.path.abspath(filename)\n            canonic = os.path.normcase(canonic)\n            self.fncache[filename] = canonic\n        return canonic\n\n    def reset(self):\n        import linecache\n        linecache.checkcache()\n        self.botframe = None\n        self._set_stopinfo(None, None)\n\n    def trace_dispatch(self, frame, event, arg):\n        if self.quitting:\n            return # None\n        if event == 'line':\n            return self.dispatch_line(frame)\n        if event == 'call':\n            return self.dispatch_call(frame, arg)\n        if event == 'return':\n            return self.dispatch_return(frame, arg)\n        if event == 'exception':\n            return self.dispatch_exception(frame, arg)\n        if event == 'c_call':\n            return self.trace_dispatch\n        if event == 'c_exception':\n            return self.trace_dispatch\n        if event == 'c_return':\n            return self.trace_dispatch\n        print 'bdb.Bdb.dispatch: unknown debugging event:', repr(event)\n        return self.trace_dispatch\n\n    def dispatch_line(self, frame):\n        if self.stop_here(frame) or self.break_here(frame):\n            self.user_line(frame)\n            if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    def dispatch_call(self, frame, arg):\n        # XXX 'arg' is no longer used\n        if self.botframe is None:\n            # First call of dispatch since reset()\n            self.botframe = frame.f_back # (CT) Note that this may also be None!\n            return self.trace_dispatch\n        if not (self.stop_here(frame) or self.break_anywhere(frame)):\n            # No need to trace this function\n            return # None\n        self.user_call(frame, arg)\n        if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    def dispatch_return(self, frame, arg):\n        if self.stop_here(frame) or frame == self.returnframe:\n            try:\n                self.frame_returning = frame\n                self.user_return(frame, arg)\n            finally:\n                self.frame_returning = None\n            if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    def dispatch_exception(self, frame, arg):\n        if self.stop_here(frame):\n            self.user_exception(frame, arg)\n            if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    # Normally derived classes don't override the following\n    # methods, but they may if they want to redefine the\n    # definition of stopping and breakpoints.\n\n    def is_skipped_module(self, module_name):\n        for pattern in self.skip:\n            if fnmatch.fnmatch(module_name, pattern):\n                return True\n        return False\n\n    def stop_here(self, frame):\n        # (CT) stopframe may now also be None, see dispatch_call.\n        # (CT) the former test for None is therefore removed from here.\n        if self.skip and \\\n               self.is_skipped_module(frame.f_globals.get('__name__')):\n            return False\n        if frame is self.stopframe:\n            if self.stoplineno == -1:\n                return False\n            return frame.f_lineno >= self.stoplineno\n        while frame is not None and frame is not self.stopframe:\n            if frame is self.botframe:\n                return True\n            frame = frame.f_back\n        return False\n\n    def break_here(self, frame):\n        filename = self.canonic(frame.f_code.co_filename)\n        if not filename in self.breaks:\n            return False\n        lineno = frame.f_lineno\n        if not lineno in self.breaks[filename]:\n            # The line itself has no breakpoint, but maybe the line is the\n            # first line of a function with breakpoint set by function name.\n            lineno = frame.f_code.co_firstlineno\n            if not lineno in self.breaks[filename]:\n                return False\n\n        # flag says ok to delete temp. bp\n        (bp, flag) = effective(filename, lineno, frame)\n        if bp:\n            self.currentbp = bp.number\n            if (flag and bp.temporary):\n                self.do_clear(str(bp.number))\n            return True\n        else:\n            return False\n\n    def do_clear(self, arg):\n        raise NotImplementedError, \"subclass of bdb must implement do_clear()\"\n\n    def break_anywhere(self, frame):\n        return self.canonic(frame.f_code.co_filename) in self.breaks\n\n    # Derived classes should override the user_* methods\n    # to gain control.\n\n    def user_call(self, frame, argument_list):\n        \"\"\"This method is called when there is the remote possibility\n        that we ever need to stop in this function.\"\"\"\n        pass\n\n    def user_line(self, frame):\n        \"\"\"This method is called when we stop or break at this line.\"\"\"\n        pass\n\n    def user_return(self, frame, return_value):\n        \"\"\"This method is called when a return trap is set here.\"\"\"\n        pass\n\n    def user_exception(self, frame, exc_info):\n        exc_type, exc_value, exc_traceback = exc_info\n        \"\"\"This method is called if an exception occurs,\n        but only if we are to stop at or just below this level.\"\"\"\n        pass\n\n    def _set_stopinfo(self, stopframe, returnframe, stoplineno=0):\n        self.stopframe = stopframe\n        self.returnframe = returnframe\n        self.quitting = 0\n        # stoplineno >= 0 means: stop at line >= the stoplineno\n        # stoplineno -1 means: don't stop at all\n        self.stoplineno = stoplineno\n\n    # Derived classes and clients can call the following methods\n    # to affect the stepping state.\n\n    def set_until(self, frame): #the name \"until\" is borrowed from gdb\n        \"\"\"Stop when the line with the line no greater than the current one is\n        reached or when returning from current frame\"\"\"\n        self._set_stopinfo(frame, frame, frame.f_lineno+1)\n\n    def set_step(self):\n        \"\"\"Stop after one line of code.\"\"\"\n        # Issue #13183: pdb skips frames after hitting a breakpoint and running\n        # step commands.\n        # Restore the trace function in the caller (that may not have been set\n        # for performance reasons) when returning from the current frame.\n        if self.frame_returning:\n            caller_frame = self.frame_returning.f_back\n            if caller_frame and not caller_frame.f_trace:\n                caller_frame.f_trace = self.trace_dispatch\n        self._set_stopinfo(None, None)\n\n    def set_next(self, frame):\n        \"\"\"Stop on the next line in or below the given frame.\"\"\"\n        self._set_stopinfo(frame, None)\n\n    def set_return(self, frame):\n        \"\"\"Stop when returning from the given frame.\"\"\"\n        self._set_stopinfo(frame.f_back, frame)\n\n    def set_trace(self, frame=None):\n        \"\"\"Start debugging from `frame`.\n\n        If frame is not specified, debugging starts from caller's frame.\n        \"\"\"\n        if frame is None:\n            frame = sys._getframe().f_back\n        self.reset()\n        while frame:\n            frame.f_trace = self.trace_dispatch\n            self.botframe = frame\n            frame = frame.f_back\n        self.set_step()\n        sys.settrace(self.trace_dispatch)\n\n    def set_continue(self):\n        # Don't stop except at breakpoints or when finished\n        self._set_stopinfo(self.botframe, None, -1)\n        if not self.breaks:\n            # no breakpoints; run without debugger overhead\n            sys.settrace(None)\n            frame = sys._getframe().f_back\n            while frame and frame is not self.botframe:\n                del frame.f_trace\n                frame = frame.f_back\n\n    def set_quit(self):\n        self.stopframe = self.botframe\n        self.returnframe = None\n        self.quitting = 1\n        sys.settrace(None)\n\n    # Derived classes and clients can call the following methods\n    # to manipulate breakpoints.  These methods return an\n    # error message is something went wrong, None if all is well.\n    # Set_break prints out the breakpoint line and file:lineno.\n    # Call self.get_*break*() to see the breakpoints or better\n    # for bp in Breakpoint.bpbynumber: if bp: bp.bpprint().\n\n    def set_break(self, filename, lineno, temporary=0, cond = None,\n                  funcname=None):\n        filename = self.canonic(filename)\n        import linecache # Import as late as possible\n        line = linecache.getline(filename, lineno)\n        if not line:\n            return 'Line %s:%d does not exist' % (filename,\n                                   lineno)\n        if not filename in self.breaks:\n            self.breaks[filename] = []\n        list = self.breaks[filename]\n        if not lineno in list:\n            list.append(lineno)\n        bp = Breakpoint(filename, lineno, temporary, cond, funcname)\n\n    def _prune_breaks(self, filename, lineno):\n        if (filename, lineno) not in Breakpoint.bplist:\n            self.breaks[filename].remove(lineno)\n        if not self.breaks[filename]:\n            del self.breaks[filename]\n\n    def clear_break(self, filename, lineno):\n        filename = self.canonic(filename)\n        if not filename in self.breaks:\n            return 'There are no breakpoints in %s' % filename\n        if lineno not in self.breaks[filename]:\n            return 'There is no breakpoint at %s:%d' % (filename,\n                                    lineno)\n        # If there's only one bp in the list for that file,line\n        # pair, then remove the breaks entry\n        for bp in Breakpoint.bplist[filename, lineno][:]:\n            bp.deleteMe()\n        self._prune_breaks(filename, lineno)\n\n    def clear_bpbynumber(self, arg):\n        try:\n            number = int(arg)\n        except:\n            return 'Non-numeric breakpoint number (%s)' % arg\n        try:\n            bp = Breakpoint.bpbynumber[number]\n        except IndexError:\n            return 'Breakpoint number (%d) out of range' % number\n        if not bp:\n            return 'Breakpoint (%d) already deleted' % number\n        bp.deleteMe()\n        self._prune_breaks(bp.file, bp.line)\n\n    def clear_all_file_breaks(self, filename):\n        filename = self.canonic(filename)\n        if not filename in self.breaks:\n            return 'There are no breakpoints in %s' % filename\n        for line in self.breaks[filename]:\n            blist = Breakpoint.bplist[filename, line]\n            for bp in blist:\n                bp.deleteMe()\n        del self.breaks[filename]\n\n    def clear_all_breaks(self):\n        if not self.breaks:\n            return 'There are no breakpoints'\n        for bp in Breakpoint.bpbynumber:\n            if bp:\n                bp.deleteMe()\n        self.breaks = {}\n\n    def get_break(self, filename, lineno):\n        filename = self.canonic(filename)\n        return filename in self.breaks and \\\n            lineno in self.breaks[filename]\n\n    def get_breaks(self, filename, lineno):\n        filename = self.canonic(filename)\n        return filename in self.breaks and \\\n            lineno in self.breaks[filename] and \\\n            Breakpoint.bplist[filename, lineno] or []\n\n    def get_file_breaks(self, filename):\n        filename = self.canonic(filename)\n        if filename in self.breaks:\n            return self.breaks[filename]\n        else:\n            return []\n\n    def get_all_breaks(self):\n        return self.breaks\n\n    # Derived classes and clients can call the following method\n    # to get a data structure representing a stack trace.\n\n    def get_stack(self, f, t):\n        stack = []\n        if t and t.tb_frame is f:\n            t = t.tb_next\n        while f is not None:\n            stack.append((f, f.f_lineno))\n            if f is self.botframe:\n                break\n            f = f.f_back\n        stack.reverse()\n        i = max(0, len(stack) - 1)\n        while t is not None:\n            stack.append((t.tb_frame, t.tb_lineno))\n            t = t.tb_next\n        if f is None:\n            i = max(0, len(stack) - 1)\n        return stack, i\n\n    #\n\n    def format_stack_entry(self, frame_lineno, lprefix=': '):\n        import linecache, repr\n        frame, lineno = frame_lineno\n        filename = self.canonic(frame.f_code.co_filename)\n        s = '%s(%r)' % (filename, lineno)\n        if frame.f_code.co_name:\n            s = s + frame.f_code.co_name\n        else:\n            s = s + \"<lambda>\"\n        if '__args__' in frame.f_locals:\n            args = frame.f_locals['__args__']\n        else:\n            args = None\n        if args:\n            s = s + repr.repr(args)\n        else:\n            s = s + '()'\n        if '__return__' in frame.f_locals:\n            rv = frame.f_locals['__return__']\n            s = s + '->'\n            s = s + repr.repr(rv)\n        line = linecache.getline(filename, lineno, frame.f_globals)\n        if line: s = s + lprefix + line.strip()\n        return s\n\n    # The following two methods can be called by clients to use\n    # a debugger to debug a statement, given as a string.\n\n    def run(self, cmd, globals=None, locals=None):\n        if globals is None:\n            import __main__\n            globals = __main__.__dict__\n        if locals is None:\n            locals = globals\n        self.reset()\n        sys.settrace(self.trace_dispatch)\n        if not isinstance(cmd, types.CodeType):\n            cmd = cmd+'\\n'\n        try:\n            exec cmd in globals, locals\n        except BdbQuit:\n            pass\n        finally:\n            self.quitting = 1\n            sys.settrace(None)\n\n    def runeval(self, expr, globals=None, locals=None):\n        if globals is None:\n            import __main__\n            globals = __main__.__dict__\n        if locals is None:\n            locals = globals\n        self.reset()\n        sys.settrace(self.trace_dispatch)\n        if not isinstance(expr, types.CodeType):\n            expr = expr+'\\n'\n        try:\n            return eval(expr, globals, locals)\n        except BdbQuit:\n            pass\n        finally:\n            self.quitting = 1\n            sys.settrace(None)\n\n    def runctx(self, cmd, globals, locals):\n        # B/W compatibility\n        self.run(cmd, globals, locals)\n\n    # This method is more useful to debug a single function call.\n\n    def runcall(self, func, *args, **kwds):\n        self.reset()\n        sys.settrace(self.trace_dispatch)\n        res = None\n        try:\n            res = func(*args, **kwds)\n        except BdbQuit:\n            pass\n        finally:\n            self.quitting = 1\n            sys.settrace(None)\n        return res\n\n\ndef set_trace():\n    Bdb().set_trace()\n\n\nclass Breakpoint:\n\n    \"\"\"Breakpoint class\n\n    Implements temporary breakpoints, ignore counts, disabling and\n    (re)-enabling, and conditionals.\n\n    Breakpoints are indexed by number through bpbynumber and by\n    the file,line tuple using bplist.  The former points to a\n    single instance of class Breakpoint.  The latter points to a\n    list of such instances since there may be more than one\n    breakpoint per line.\n\n    \"\"\"\n\n    # XXX Keeping state in the class is a mistake -- this means\n    # you cannot have more than one active Bdb instance.\n\n    next = 1        # Next bp to be assigned\n    bplist = {}     # indexed by (file, lineno) tuple\n    bpbynumber = [None] # Each entry is None or an instance of Bpt\n                # index 0 is unused, except for marking an\n                # effective break .... see effective()\n\n    def __init__(self, file, line, temporary=0, cond=None, funcname=None):\n        self.funcname = funcname\n        # Needed if funcname is not None.\n        self.func_first_executable_line = None\n        self.file = file    # This better be in canonical form!\n        self.line = line\n        self.temporary = temporary\n        self.cond = cond\n        self.enabled = 1\n        self.ignore = 0\n        self.hits = 0\n        self.number = Breakpoint.next\n        Breakpoint.next = Breakpoint.next + 1\n        # Build the two lists\n        self.bpbynumber.append(self)\n        if (file, line) in self.bplist:\n            self.bplist[file, line].append(self)\n        else:\n            self.bplist[file, line] = [self]\n\n\n    def deleteMe(self):\n        index = (self.file, self.line)\n        self.bpbynumber[self.number] = None   # No longer in list\n        self.bplist[index].remove(self)\n        if not self.bplist[index]:\n            # No more bp for this f:l combo\n            del self.bplist[index]\n\n    def enable(self):\n        self.enabled = 1\n\n    def disable(self):\n        self.enabled = 0\n\n    def bpprint(self, out=None):\n        if out is None:\n            out = sys.stdout\n        if self.temporary:\n            disp = 'del  '\n        else:\n            disp = 'keep '\n        if self.enabled:\n            disp = disp + 'yes  '\n        else:\n            disp = disp + 'no   '\n        print >>out, '%-4dbreakpoint   %s at %s:%d' % (self.number, disp,\n                                                       self.file, self.line)\n        if self.cond:\n            print >>out, '\\tstop only if %s' % (self.cond,)\n        if self.ignore:\n            print >>out, '\\tignore next %d hits' % (self.ignore)\n        if (self.hits):\n            if (self.hits > 1): ss = 's'\n            else: ss = ''\n            print >>out, ('\\tbreakpoint already hit %d time%s' %\n                          (self.hits, ss))\n\n# -----------end of Breakpoint class----------\n\ndef checkfuncname(b, frame):\n    \"\"\"Check whether we should break here because of `b.funcname`.\"\"\"\n    if not b.funcname:\n        # Breakpoint was set via line number.\n        if b.line != frame.f_lineno:\n            # Breakpoint was set at a line with a def statement and the function\n            # defined is called: don't break.\n            return False\n        return True\n\n    # Breakpoint set via function name.\n\n    if frame.f_code.co_name != b.funcname:\n        # It's not a function call, but rather execution of def statement.\n        return False\n\n    # We are in the right frame.\n    if not b.func_first_executable_line:\n        # The function is entered for the 1st time.\n        b.func_first_executable_line = frame.f_lineno\n\n    if  b.func_first_executable_line != frame.f_lineno:\n        # But we are not at the first line number: don't break.\n        return False\n    return True\n\n# Determines if there is an effective (active) breakpoint at this\n# line of code.  Returns breakpoint number or 0 if none\ndef effective(file, line, frame):\n    \"\"\"Determine which breakpoint for this file:line is to be acted upon.\n\n    Called only if we know there is a bpt at this\n    location.  Returns breakpoint that was triggered and a flag\n    that indicates if it is ok to delete a temporary bp.\n\n    \"\"\"\n    possibles = Breakpoint.bplist[file,line]\n    for i in range(0, len(possibles)):\n        b = possibles[i]\n        if b.enabled == 0:\n            continue\n        if not checkfuncname(b, frame):\n            continue\n        # Count every hit when bp is enabled\n        b.hits = b.hits + 1\n        if not b.cond:\n            # If unconditional, and ignoring,\n            # go on to next, else break\n            if b.ignore > 0:\n                b.ignore = b.ignore -1\n                continue\n            else:\n                # breakpoint and marker that's ok\n                # to delete if temporary\n                return (b,1)\n        else:\n            # Conditional bp.\n            # Ignore count applies only to those bpt hits where the\n            # condition evaluates to true.\n            try:\n                val = eval(b.cond, frame.f_globals,\n                       frame.f_locals)\n                if val:\n                    if b.ignore > 0:\n                        b.ignore = b.ignore -1\n                        # continue\n                    else:\n                        return (b,1)\n                # else:\n                #   continue\n            except:\n                # if eval fails, most conservative\n                # thing is to stop on breakpoint\n                # regardless of ignore count.\n                # Don't delete temporary,\n                # as another hint to user.\n                return (b,0)\n    return (None, None)\n\n# -------------------- testing --------------------\n\nclass Tdb(Bdb):\n    def user_call(self, frame, args):\n        name = frame.f_code.co_name\n        if not name: name = '???'\n        print '+++ call', name, args\n    def user_line(self, frame):\n        import linecache\n        name = frame.f_code.co_name\n        if not name: name = '???'\n        fn = self.canonic(frame.f_code.co_filename)\n        line = linecache.getline(fn, frame.f_lineno, frame.f_globals)\n        print '+++', fn, frame.f_lineno, name, ':', line.strip()\n    def user_return(self, frame, retval):\n        print '+++ return', retval\n    def user_exception(self, frame, exc_stuff):\n        print '+++ exception', exc_stuff\n        self.set_continue()\n\ndef foo(n):\n    print 'foo(', n, ')'\n    x = bar(n*10)\n    print 'bar returned', x\n\ndef bar(a):\n    print 'bar(', a, ')'\n    return a/2\n\ndef test():\n    t = Tdb()\n    t.run('import bdb; bdb.foo(10)')\n\n# end\n",
		"file_name": "bdb.py"
	},
	{
		"content": "\"\"\"Filename matching with shell patterns.\n\nfnmatch(FILENAME, PATTERN) matches according to the local convention.\nfnmatchcase(FILENAME, PATTERN) always takes case in account.\n\nThe functions operate by translating the pattern into a regular\nexpression.  They cache the compiled regular expressions for speed.\n\nThe function translate(PATTERN) returns a regular expression\ncorresponding to PATTERN.  (It does not compile it.)\n\"\"\"\n\nimport re\n\n__all__ = [\"filter\", \"fnmatch\", \"fnmatchcase\", \"translate\"]\n\n_cache = {}\n_MAXCACHE = 100\n\ndef _purge():\n    \"\"\"Clear the pattern cache\"\"\"\n    _cache.clear()\n\ndef fnmatch(name, pat):\n    \"\"\"Test whether FILENAME matches PATTERN.\n\n    Patterns are Unix shell style:\n\n    *       matches everything\n    ?       matches any single character\n    [seq]   matches any character in seq\n    [!seq]  matches any char not in seq\n\n    An initial period in FILENAME is not special.\n    Both FILENAME and PATTERN are first case-normalized\n    if the operating system requires it.\n    If you don't want this, use fnmatchcase(FILENAME, PATTERN).\n    \"\"\"\n\n    import os\n    name = os.path.normcase(name)\n    pat = os.path.normcase(pat)\n    return fnmatchcase(name, pat)\n\ndef filter(names, pat):\n    \"\"\"Return the subset of the list NAMES that match PAT\"\"\"\n    import os,posixpath\n    result=[]\n    pat=os.path.normcase(pat)\n    if not pat in _cache:\n        res = translate(pat)\n        if len(_cache) >= _MAXCACHE:\n            _cache.clear()\n        _cache[pat] = re.compile(res)\n    match=_cache[pat].match\n    if os.path is posixpath:\n        # normcase on posix is NOP. Optimize it away from the loop.\n        for name in names:\n            if match(name):\n                result.append(name)\n    else:\n        for name in names:\n            if match(os.path.normcase(name)):\n                result.append(name)\n    return result\n\ndef fnmatchcase(name, pat):\n    \"\"\"Test whether FILENAME matches PATTERN, including case.\n\n    This is a version of fnmatch() which doesn't case-normalize\n    its arguments.\n    \"\"\"\n\n    if not pat in _cache:\n        res = translate(pat)\n        if len(_cache) >= _MAXCACHE:\n            _cache.clear()\n        _cache[pat] = re.compile(res)\n    return _cache[pat].match(name) is not None\n\ndef translate(pat):\n    \"\"\"Translate a shell PATTERN to a regular expression.\n\n    There is no way to quote meta-characters.\n    \"\"\"\n\n    i, n = 0, len(pat)\n    res = ''\n    while i < n:\n        c = pat[i]\n        i = i+1\n        if c == '*':\n            res = res + '.*'\n        elif c == '?':\n            res = res + '.'\n        elif c == '[':\n            j = i\n            if j < n and pat[j] == '!':\n                j = j+1\n            if j < n and pat[j] == ']':\n                j = j+1\n            while j < n and pat[j] != ']':\n                j = j+1\n            if j >= n:\n                res = res + '\\\\['\n            else:\n                stuff = pat[i:j].replace('\\\\','\\\\\\\\')\n                i = j+1\n                if stuff[0] == '!':\n                    stuff = '^' + stuff[1:]\n                elif stuff[0] == '^':\n                    stuff = '\\\\' + stuff\n                res = '%s[%s]' % (res, stuff)\n        else:\n            res = res + re.escape(c)\n    return res + '\\Z(?ms)'\n",
		"file_name": "fnmatch.py"
	},
	{
		"content": "\"\"\"A generic class to build line-oriented command interpreters.\n\nInterpreters constructed with this class obey the following conventions:\n\n1. End of file on input is processed as the command 'EOF'.\n2. A command is parsed out of each line by collecting the prefix composed\n   of characters in the identchars member.\n3. A command `foo' is dispatched to a method 'do_foo()'; the do_ method\n   is passed a single argument consisting of the remainder of the line.\n4. Typing an empty line repeats the last command.  (Actually, it calls the\n   method `emptyline', which may be overridden in a subclass.)\n5. There is a predefined `help' method.  Given an argument `topic', it\n   calls the command `help_topic'.  With no arguments, it lists all topics\n   with defined help_ functions, broken into up to three topics; documented\n   commands, miscellaneous help topics, and undocumented commands.\n6. The command '?' is a synonym for `help'.  The command '!' is a synonym\n   for `shell', if a do_shell method exists.\n7. If completion is enabled, completing commands will be done automatically,\n   and completing of commands args is done by calling complete_foo() with\n   arguments text, line, begidx, endidx.  text is string we are matching\n   against, all returned matches must begin with it.  line is the current\n   input line (lstripped), begidx and endidx are the beginning and end\n   indexes of the text being matched, which could be used to provide\n   different completion depending upon which position the argument is in.\n\nThe `default' method may be overridden to intercept commands for which there\nis no do_ method.\n\nThe `completedefault' method may be overridden to intercept completions for\ncommands that have no complete_ method.\n\nThe data member `self.ruler' sets the character used to draw separator lines\nin the help messages.  If empty, no ruler line is drawn.  It defaults to \"=\".\n\nIf the value of `self.intro' is nonempty when the cmdloop method is called,\nit is printed out on interpreter startup.  This value may be overridden\nvia an optional argument to the cmdloop() method.\n\nThe data members `self.doc_header', `self.misc_header', and\n`self.undoc_header' set the headers used for the help function's\nlistings of documented functions, miscellaneous topics, and undocumented\nfunctions respectively.\n\nThese interpreters use raw_input; thus, if the readline module is loaded,\nthey automatically support Emacs-like command history and editing features.\n\"\"\"\n\nimport string\n\n__all__ = [\"Cmd\"]\n\nPROMPT = '(Cmd) '\nIDENTCHARS = string.ascii_letters + string.digits + '_'\n\nclass Cmd:\n    \"\"\"A simple framework for writing line-oriented command interpreters.\n\n    These are often useful for test harnesses, administrative tools, and\n    prototypes that will later be wrapped in a more sophisticated interface.\n\n    A Cmd instance or subclass instance is a line-oriented interpreter\n    framework.  There is no good reason to instantiate Cmd itself; rather,\n    it's useful as a superclass of an interpreter class you define yourself\n    in order to inherit Cmd's methods and encapsulate action methods.\n\n    \"\"\"\n    prompt = PROMPT\n    identchars = IDENTCHARS\n    ruler = '='\n    lastcmd = ''\n    intro = None\n    doc_leader = \"\"\n    doc_header = \"Documented commands (type help <topic>):\"\n    misc_header = \"Miscellaneous help topics:\"\n    undoc_header = \"Undocumented commands:\"\n    nohelp = \"*** No help on %s\"\n    use_rawinput = 1\n\n    def __init__(self, completekey='tab', stdin=None, stdout=None):\n        \"\"\"Instantiate a line-oriented interpreter framework.\n\n        The optional argument 'completekey' is the readline name of a\n        completion key; it defaults to the Tab key. If completekey is\n        not None and the readline module is available, command completion\n        is done automatically. The optional arguments stdin and stdout\n        specify alternate input and output file objects; if not specified,\n        sys.stdin and sys.stdout are used.\n\n        \"\"\"\n        import sys\n        if stdin is not None:\n            self.stdin = stdin\n        else:\n            self.stdin = sys.stdin\n        if stdout is not None:\n            self.stdout = stdout\n        else:\n            self.stdout = sys.stdout\n        self.cmdqueue = []\n        self.completekey = completekey\n\n    def cmdloop(self, intro=None):\n        \"\"\"Repeatedly issue a prompt, accept input, parse an initial prefix\n        off the received input, and dispatch to action methods, passing them\n        the remainder of the line as argument.\n\n        \"\"\"\n\n        self.preloop()\n        if self.use_rawinput and self.completekey:\n            try:\n                import readline\n                self.old_completer = readline.get_completer()\n                readline.set_completer(self.complete)\n                readline.parse_and_bind(self.completekey+\": complete\")\n            except ImportError:\n                pass\n        try:\n            if intro is not None:\n                self.intro = intro\n            if self.intro:\n                self.stdout.write(str(self.intro)+\"\\n\")\n            stop = None\n            while not stop:\n                if self.cmdqueue:\n                    line = self.cmdqueue.pop(0)\n                else:\n                    if self.use_rawinput:\n                        try:\n                            line = raw_input(self.prompt)\n                        except EOFError:\n                            line = 'EOF'\n                    else:\n                        self.stdout.write(self.prompt)\n                        self.stdout.flush()\n                        line = self.stdin.readline()\n                        if not len(line):\n                            line = 'EOF'\n                        else:\n                            line = line.rstrip('\\r\\n')\n                line = self.precmd(line)\n                stop = self.onecmd(line)\n                stop = self.postcmd(stop, line)\n            self.postloop()\n        finally:\n            if self.use_rawinput and self.completekey:\n                try:\n                    import readline\n                    readline.set_completer(self.old_completer)\n                except ImportError:\n                    pass\n\n\n    def precmd(self, line):\n        \"\"\"Hook method executed just before the command line is\n        interpreted, but after the input prompt is generated and issued.\n\n        \"\"\"\n        return line\n\n    def postcmd(self, stop, line):\n        \"\"\"Hook method executed just after a command dispatch is finished.\"\"\"\n        return stop\n\n    def preloop(self):\n        \"\"\"Hook method executed once when the cmdloop() method is called.\"\"\"\n        pass\n\n    def postloop(self):\n        \"\"\"Hook method executed once when the cmdloop() method is about to\n        return.\n\n        \"\"\"\n        pass\n\n    def parseline(self, line):\n        \"\"\"Parse the line into a command name and a string containing\n        the arguments.  Returns a tuple containing (command, args, line).\n        'command' and 'args' may be None if the line couldn't be parsed.\n        \"\"\"\n        line = line.strip()\n        if not line:\n            return None, None, line\n        elif line[0] == '?':\n            line = 'help ' + line[1:]\n        elif line[0] == '!':\n            if hasattr(self, 'do_shell'):\n                line = 'shell ' + line[1:]\n            else:\n                return None, None, line\n        i, n = 0, len(line)\n        while i < n and line[i] in self.identchars: i = i+1\n        cmd, arg = line[:i], line[i:].strip()\n        return cmd, arg, line\n\n    def onecmd(self, line):\n        \"\"\"Interpret the argument as though it had been typed in response\n        to the prompt.\n\n        This may be overridden, but should not normally need to be;\n        see the precmd() and postcmd() methods for useful execution hooks.\n        The return value is a flag indicating whether interpretation of\n        commands by the interpreter should stop.\n\n        \"\"\"\n        cmd, arg, line = self.parseline(line)\n        if not line:\n            return self.emptyline()\n        if cmd is None:\n            return self.default(line)\n        self.lastcmd = line\n        if line == 'EOF' :\n            self.lastcmd = ''\n        if cmd == '':\n            return self.default(line)\n        else:\n            try:\n                func = getattr(self, 'do_' + cmd)\n            except AttributeError:\n                return self.default(line)\n            return func(arg)\n\n    def emptyline(self):\n        \"\"\"Called when an empty line is entered in response to the prompt.\n\n        If this method is not overridden, it repeats the last nonempty\n        command entered.\n\n        \"\"\"\n        if self.lastcmd:\n            return self.onecmd(self.lastcmd)\n\n    def default(self, line):\n        \"\"\"Called on an input line when the command prefix is not recognized.\n\n        If this method is not overridden, it prints an error message and\n        returns.\n\n        \"\"\"\n        self.stdout.write('*** Unknown syntax: %s\\n'%line)\n\n    def completedefault(self, *ignored):\n        \"\"\"Method called to complete an input line when no command-specific\n        complete_*() method is available.\n\n        By default, it returns an empty list.\n\n        \"\"\"\n        return []\n\n    def completenames(self, text, *ignored):\n        dotext = 'do_'+text\n        return [a[3:] for a in self.get_names() if a.startswith(dotext)]\n\n    def complete(self, text, state):\n        \"\"\"Return the next possible completion for 'text'.\n\n        If a command has not been entered, then complete against command list.\n        Otherwise try to call complete_<command> to get list of completions.\n        \"\"\"\n        if state == 0:\n            import readline\n            origline = readline.get_line_buffer()\n            line = origline.lstrip()\n            stripped = len(origline) - len(line)\n            begidx = readline.get_begidx() - stripped\n            endidx = readline.get_endidx() - stripped\n            if begidx>0:\n                cmd, args, foo = self.parseline(line)\n                if cmd == '':\n                    compfunc = self.completedefault\n                else:\n                    try:\n                        compfunc = getattr(self, 'complete_' + cmd)\n                    except AttributeError:\n                        compfunc = self.completedefault\n            else:\n                compfunc = self.completenames\n            self.completion_matches = compfunc(text, line, begidx, endidx)\n        try:\n            return self.completion_matches[state]\n        except IndexError:\n            return None\n\n    def get_names(self):\n        # This method used to pull in base class attributes\n        # at a time dir() didn't do it yet.\n        return dir(self.__class__)\n\n    def complete_help(self, *args):\n        commands = set(self.completenames(*args))\n        topics = set(a[5:] for a in self.get_names()\n                     if a.startswith('help_' + args[0]))\n        return list(commands | topics)\n\n    def do_help(self, arg):\n        'List available commands with \"help\" or detailed help with \"help cmd\".'\n        if arg:\n            # XXX check arg syntax\n            try:\n                func = getattr(self, 'help_' + arg)\n            except AttributeError:\n                try:\n                    doc=getattr(self, 'do_' + arg).__doc__\n                    if doc:\n                        self.stdout.write(\"%s\\n\"%str(doc))\n                        return\n                except AttributeError:\n                    pass\n                self.stdout.write(\"%s\\n\"%str(self.nohelp % (arg,)))\n                return\n            func()\n        else:\n            names = self.get_names()\n            cmds_doc = []\n            cmds_undoc = []\n            help = {}\n            for name in names:\n                if name[:5] == 'help_':\n                    help[name[5:]]=1\n            names.sort()\n            # There can be duplicates if routines overridden\n            prevname = ''\n            for name in names:\n                if name[:3] == 'do_':\n                    if name == prevname:\n                        continue\n                    prevname = name\n                    cmd=name[3:]\n                    if cmd in help:\n                        cmds_doc.append(cmd)\n                        del help[cmd]\n                    elif getattr(self, name).__doc__:\n                        cmds_doc.append(cmd)\n                    else:\n                        cmds_undoc.append(cmd)\n            self.stdout.write(\"%s\\n\"%str(self.doc_leader))\n            self.print_topics(self.doc_header,   cmds_doc,   15,80)\n            self.print_topics(self.misc_header,  help.keys(),15,80)\n            self.print_topics(self.undoc_header, cmds_undoc, 15,80)\n\n    def print_topics(self, header, cmds, cmdlen, maxcol):\n        if cmds:\n            self.stdout.write(\"%s\\n\"%str(header))\n            if self.ruler:\n                self.stdout.write(\"%s\\n\"%str(self.ruler * len(header)))\n            self.columnize(cmds, maxcol-1)\n            self.stdout.write(\"\\n\")\n\n    def columnize(self, list, displaywidth=80):\n        \"\"\"Display a list of strings as a compact set of columns.\n\n        Each column is only as wide as necessary.\n        Columns are separated by two spaces (one was not legible enough).\n        \"\"\"\n        if not list:\n            self.stdout.write(\"<empty>\\n\")\n            return\n        nonstrings = [i for i in range(len(list))\n                        if not isinstance(list[i], str)]\n        if nonstrings:\n            raise TypeError, (\"list[i] not a string for i in %s\" %\n                              \", \".join(map(str, nonstrings)))\n        size = len(list)\n        if size == 1:\n            self.stdout.write('%s\\n'%str(list[0]))\n            return\n        # Try every row count from 1 upwards\n        for nrows in range(1, len(list)):\n            ncols = (size+nrows-1) // nrows\n            colwidths = []\n            totwidth = -2\n            for col in range(ncols):\n                colwidth = 0\n                for row in range(nrows):\n                    i = row + nrows*col\n                    if i >= size:\n                        break\n                    x = list[i]\n                    colwidth = max(colwidth, len(x))\n                colwidths.append(colwidth)\n                totwidth += colwidth + 2\n                if totwidth > displaywidth:\n                    break\n            if totwidth <= displaywidth:\n                break\n        else:\n            nrows = len(list)\n            ncols = 1\n            colwidths = [0]\n        for row in range(nrows):\n            texts = []\n            for col in range(ncols):\n                i = row + nrows*col\n                if i >= size:\n                    x = \"\"\n                else:\n                    x = list[i]\n                texts.append(x)\n            while texts and not texts[-1]:\n                del texts[-1]\n            for col in range(len(texts)):\n                texts[col] = texts[col].ljust(colwidths[col])\n            self.stdout.write(\"%s\\n\"%str(\"  \".join(texts)))\n",
		"file_name": "cmd.py"
	},
	{
		"content": "#  Author:      Fred L. Drake, Jr.\n#               fdrake@acm.org\n#\n#  This is a simple little module I wrote to make life easier.  I didn't\n#  see anything quite like it in the library, though I may have overlooked\n#  something.  I wrote this when I was trying to read some heavily nested\n#  tuples with fairly non-descriptive content.  This is modeled very much\n#  after Lisp/Scheme - style pretty-printing of lists.  If you find it\n#  useful, thank small children who sleep at night.\n\n\"\"\"Support to pretty-print lists, tuples, & dictionaries recursively.\n\nVery simple, but useful, especially in debugging data structures.\n\nClasses\n-------\n\nPrettyPrinter()\n    Handle pretty-printing operations onto a stream using a configured\n    set of formatting parameters.\n\nFunctions\n---------\n\npformat()\n    Format a Python object into a pretty-printed representation.\n\npprint()\n    Pretty-print a Python object to a stream [default is sys.stdout].\n\nsaferepr()\n    Generate a 'standard' repr()-like value, but protect against recursive\n    data structures.\n\n\"\"\"\n\nimport sys as _sys\nimport warnings\n\ntry:\n    from cStringIO import StringIO as _StringIO\nexcept ImportError:\n    from StringIO import StringIO as _StringIO\n\n__all__ = [\"pprint\",\"pformat\",\"isreadable\",\"isrecursive\",\"saferepr\",\n           \"PrettyPrinter\"]\n\n# cache these for faster access:\n_commajoin = \", \".join\n_id = id\n_len = len\n_type = type\n\n\ndef pprint(object, stream=None, indent=1, width=80, depth=None):\n    \"\"\"Pretty-print a Python object to a stream [default is sys.stdout].\"\"\"\n    printer = PrettyPrinter(\n        stream=stream, indent=indent, width=width, depth=depth)\n    printer.pprint(object)\n\ndef pformat(object, indent=1, width=80, depth=None):\n    \"\"\"Format a Python object into a pretty-printed representation.\"\"\"\n    return PrettyPrinter(indent=indent, width=width, depth=depth).pformat(object)\n\ndef saferepr(object):\n    \"\"\"Version of repr() which can handle recursive data structures.\"\"\"\n    return _safe_repr(object, {}, None, 0)[0]\n\ndef isreadable(object):\n    \"\"\"Determine if saferepr(object) is readable by eval().\"\"\"\n    return _safe_repr(object, {}, None, 0)[1]\n\ndef isrecursive(object):\n    \"\"\"Determine if object requires a recursive representation.\"\"\"\n    return _safe_repr(object, {}, None, 0)[2]\n\ndef _sorted(iterable):\n    with warnings.catch_warnings():\n        if _sys.py3kwarning:\n            warnings.filterwarnings(\"ignore\", \"comparing unequal types \"\n                                    \"not supported\", DeprecationWarning)\n        return sorted(iterable)\n\nclass PrettyPrinter:\n    def __init__(self, indent=1, width=80, depth=None, stream=None):\n        \"\"\"Handle pretty printing operations onto a stream using a set of\n        configured parameters.\n\n        indent\n            Number of spaces to indent for each level of nesting.\n\n        width\n            Attempted maximum number of columns in the output.\n\n        depth\n            The maximum depth to print out nested structures.\n\n        stream\n            The desired output stream.  If omitted (or false), the standard\n            output stream available at construction will be used.\n\n        \"\"\"\n        indent = int(indent)\n        width = int(width)\n        assert indent >= 0, \"indent must be >= 0\"\n        assert depth is None or depth > 0, \"depth must be > 0\"\n        assert width, \"width must be != 0\"\n        self._depth = depth\n        self._indent_per_level = indent\n        self._width = width\n        if stream is not None:\n            self._stream = stream\n        else:\n            self._stream = _sys.stdout\n\n    def pprint(self, object):\n        self._format(object, self._stream, 0, 0, {}, 0)\n        self._stream.write(\"\\n\")\n\n    def pformat(self, object):\n        sio = _StringIO()\n        self._format(object, sio, 0, 0, {}, 0)\n        return sio.getvalue()\n\n    def isrecursive(self, object):\n        return self.format(object, {}, 0, 0)[2]\n\n    def isreadable(self, object):\n        s, readable, recursive = self.format(object, {}, 0, 0)\n        return readable and not recursive\n\n    def _format(self, object, stream, indent, allowance, context, level):\n        level = level + 1\n        objid = _id(object)\n        if objid in context:\n            stream.write(_recursion(object))\n            self._recursive = True\n            self._readable = False\n            return\n        rep = self._repr(object, context, level - 1)\n        typ = _type(object)\n        sepLines = _len(rep) > (self._width - 1 - indent - allowance)\n        write = stream.write\n\n        if self._depth and level > self._depth:\n            write(rep)\n            return\n\n        r = getattr(typ, \"__repr__\", None)\n        if issubclass(typ, dict) and r == dict.__repr__:\n            write('{')\n            if self._indent_per_level > 1:\n                write((self._indent_per_level - 1) * ' ')\n            length = _len(object)\n            if length:\n                context[objid] = 1\n                indent = indent + self._indent_per_level\n                items = _sorted(object.items())\n                key, ent = items[0]\n                rep = self._repr(key, context, level)\n                write(rep)\n                write(': ')\n                self._format(ent, stream, indent + _len(rep) + 2,\n                              allowance + 1, context, level)\n                if length > 1:\n                    for key, ent in items[1:]:\n                        rep = self._repr(key, context, level)\n                        if sepLines:\n                            write(',\\n%s%s: ' % (' '*indent, rep))\n                        else:\n                            write(', %s: ' % rep)\n                        self._format(ent, stream, indent + _len(rep) + 2,\n                                      allowance + 1, context, level)\n                indent = indent - self._indent_per_level\n                del context[objid]\n            write('}')\n            return\n\n        if ((issubclass(typ, list) and r == list.__repr__) or\n            (issubclass(typ, tuple) and r == tuple.__repr__) or\n            (issubclass(typ, set) and r == set.__repr__) or\n            (issubclass(typ, frozenset) and r == frozenset.__repr__)\n           ):\n            length = _len(object)\n            if issubclass(typ, list):\n                write('[')\n                endchar = ']'\n            elif issubclass(typ, tuple):\n                write('(')\n                endchar = ')'\n            else:\n                if not length:\n                    write(rep)\n                    return\n                write(typ.__name__)\n                write('([')\n                endchar = '])'\n                indent += len(typ.__name__) + 1\n                object = _sorted(object)\n            if self._indent_per_level > 1 and sepLines:\n                write((self._indent_per_level - 1) * ' ')\n            if length:\n                context[objid] = 1\n                indent = indent + self._indent_per_level\n                self._format(object[0], stream, indent, allowance + 1,\n                             context, level)\n                if length > 1:\n                    for ent in object[1:]:\n                        if sepLines:\n                            write(',\\n' + ' '*indent)\n                        else:\n                            write(', ')\n                        self._format(ent, stream, indent,\n                                      allowance + 1, context, level)\n                indent = indent - self._indent_per_level\n                del context[objid]\n            if issubclass(typ, tuple) and length == 1:\n                write(',')\n            write(endchar)\n            return\n\n        write(rep)\n\n    def _repr(self, object, context, level):\n        repr, readable, recursive = self.format(object, context.copy(),\n                                                self._depth, level)\n        if not readable:\n            self._readable = False\n        if recursive:\n            self._recursive = True\n        return repr\n\n    def format(self, object, context, maxlevels, level):\n        \"\"\"Format object for a specific context, returning a string\n        and flags indicating whether the representation is 'readable'\n        and whether the object represents a recursive construct.\n        \"\"\"\n        return _safe_repr(object, context, maxlevels, level)\n\n\n# Return triple (repr_string, isreadable, isrecursive).\n\ndef _safe_repr(object, context, maxlevels, level):\n    typ = _type(object)\n    if typ is str:\n        if 'locale' not in _sys.modules:\n            return repr(object), True, False\n        if \"'\" in object and '\"' not in object:\n            closure = '\"'\n            quotes = {'\"': '\\\\\"'}\n        else:\n            closure = \"'\"\n            quotes = {\"'\": \"\\\\'\"}\n        qget = quotes.get\n        sio = _StringIO()\n        write = sio.write\n        for char in object:\n            if char.isalpha():\n                write(char)\n            else:\n                write(qget(char, repr(char)[1:-1]))\n        return (\"%s%s%s\" % (closure, sio.getvalue(), closure)), True, False\n\n    r = getattr(typ, \"__repr__\", None)\n    if issubclass(typ, dict) and r == dict.__repr__:\n        if not object:\n            return \"{}\", True, False\n        objid = _id(object)\n        if maxlevels and level >= maxlevels:\n            return \"{...}\", False, objid in context\n        if objid in context:\n            return _recursion(object), False, True\n        context[objid] = 1\n        readable = True\n        recursive = False\n        components = []\n        append = components.append\n        level += 1\n        saferepr = _safe_repr\n        for k, v in _sorted(object.items()):\n            krepr, kreadable, krecur = saferepr(k, context, maxlevels, level)\n            vrepr, vreadable, vrecur = saferepr(v, context, maxlevels, level)\n            append(\"%s: %s\" % (krepr, vrepr))\n            readable = readable and kreadable and vreadable\n            if krecur or vrecur:\n                recursive = True\n        del context[objid]\n        return \"{%s}\" % _commajoin(components), readable, recursive\n\n    if (issubclass(typ, list) and r == list.__repr__) or \\\n       (issubclass(typ, tuple) and r == tuple.__repr__):\n        if issubclass(typ, list):\n            if not object:\n                return \"[]\", True, False\n            format = \"[%s]\"\n        elif _len(object) == 1:\n            format = \"(%s,)\"\n        else:\n            if not object:\n                return \"()\", True, False\n            format = \"(%s)\"\n        objid = _id(object)\n        if maxlevels and level >= maxlevels:\n            return format % \"...\", False, objid in context\n        if objid in context:\n            return _recursion(object), False, True\n        context[objid] = 1\n        readable = True\n        recursive = False\n        components = []\n        append = components.append\n        level += 1\n        for o in object:\n            orepr, oreadable, orecur = _safe_repr(o, context, maxlevels, level)\n            append(orepr)\n            if not oreadable:\n                readable = False\n            if orecur:\n                recursive = True\n        del context[objid]\n        return format % _commajoin(components), readable, recursive\n\n    rep = repr(object)\n    return rep, (rep and not rep.startswith('<')), False\n\n\ndef _recursion(object):\n    return (\"<Recursion on %s with id=%s>\"\n            % (_type(object).__name__, _id(object)))\n\n\ndef _perfcheck(object=None):\n    import time\n    if object is None:\n        object = [(\"string\", (1, 2), [3, 4], {5: 6, 7: 8})] * 100000\n    p = PrettyPrinter()\n    t1 = time.time()\n    _safe_repr(object, {}, None, 0)\n    t2 = time.time()\n    p.pformat(object)\n    t3 = time.time()\n    print \"_safe_repr:\", t2 - t1\n    print \"pformat:\", t3 - t2\n\nif __name__ == \"__main__\":\n    _perfcheck()\n",
		"file_name": "pprint.py"
	},
	{
		"content": "# -*- coding: utf-8 -*-\n\"\"\"A lexical analyzer class for simple shell-like syntaxes.\"\"\"\n\n# Module and documentation by Eric S. Raymond, 21 Dec 1998\n# Input stacking and error message cleanup added by ESR, March 2000\n# push_source() and pop_source() made explicit by ESR, January 2001.\n# Posix compliance, split(), string arguments, and\n# iterator interface by Gustavo Niemeyer, April 2003.\n\nimport os.path\nimport sys\nfrom collections import deque\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\n__all__ = [\"shlex\", \"split\"]\n\nclass shlex:\n    \"A lexical analyzer class for simple shell-like syntaxes.\"\n    def __init__(self, instream=None, infile=None, posix=False):\n        if isinstance(instream, basestring):\n            instream = StringIO(instream)\n        if instream is not None:\n            self.instream = instream\n            self.infile = infile\n        else:\n            self.instream = sys.stdin\n            self.infile = None\n        self.posix = posix\n        if posix:\n            self.eof = None\n        else:\n            self.eof = ''\n        self.commenters = '#'\n        self.wordchars = ('abcdfeghijklmnopqrstuvwxyz'\n                          'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_')\n        if self.posix:\n            self.wordchars += ('\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u00f8\u00f9\u00fa\u00fb\u00fc\u00fd\u00fe\u00ff'\n                               '\u00c0\u00c1\u00c2\u00c3\u00c4\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u00d8\u00d9\u00da\u00db\u00dc\u00dd\u00de')\n        self.whitespace = ' \\t\\r\\n'\n        self.whitespace_split = False\n        self.quotes = '\\'\"'\n        self.escape = '\\\\'\n        self.escapedquotes = '\"'\n        self.state = ' '\n        self.pushback = deque()\n        self.lineno = 1\n        self.debug = 0\n        self.token = ''\n        self.filestack = deque()\n        self.source = None\n        if self.debug:\n            print 'shlex: reading from %s, line %d' \\\n                  % (self.instream, self.lineno)\n\n    def push_token(self, tok):\n        \"Push a token onto the stack popped by the get_token method\"\n        if self.debug >= 1:\n            print \"shlex: pushing token \" + repr(tok)\n        self.pushback.appendleft(tok)\n\n    def push_source(self, newstream, newfile=None):\n        \"Push an input source onto the lexer's input source stack.\"\n        if isinstance(newstream, basestring):\n            newstream = StringIO(newstream)\n        self.filestack.appendleft((self.infile, self.instream, self.lineno))\n        self.infile = newfile\n        self.instream = newstream\n        self.lineno = 1\n        if self.debug:\n            if newfile is not None:\n                print 'shlex: pushing to file %s' % (self.infile,)\n            else:\n                print 'shlex: pushing to stream %s' % (self.instream,)\n\n    def pop_source(self):\n        \"Pop the input source stack.\"\n        self.instream.close()\n        (self.infile, self.instream, self.lineno) = self.filestack.popleft()\n        if self.debug:\n            print 'shlex: popping to %s, line %d' \\\n                  % (self.instream, self.lineno)\n        self.state = ' '\n\n    def get_token(self):\n        \"Get a token from the input stream (or from stack if it's nonempty)\"\n        if self.pushback:\n            tok = self.pushback.popleft()\n            if self.debug >= 1:\n                print \"shlex: popping token \" + repr(tok)\n            return tok\n        # No pushback.  Get a token.\n        raw = self.read_token()\n        # Handle inclusions\n        if self.source is not None:\n            while raw == self.source:\n                spec = self.sourcehook(self.read_token())\n                if spec:\n                    (newfile, newstream) = spec\n                    self.push_source(newstream, newfile)\n                raw = self.get_token()\n        # Maybe we got EOF instead?\n        while raw == self.eof:\n            if not self.filestack:\n                return self.eof\n            else:\n                self.pop_source()\n                raw = self.get_token()\n        # Neither inclusion nor EOF\n        if self.debug >= 1:\n            if raw != self.eof:\n                print \"shlex: token=\" + repr(raw)\n            else:\n                print \"shlex: token=EOF\"\n        return raw\n\n    def read_token(self):\n        quoted = False\n        escapedstate = ' '\n        while True:\n            nextchar = self.instream.read(1)\n            if nextchar == '\\n':\n                self.lineno = self.lineno + 1\n            if self.debug >= 3:\n                print \"shlex: in state\", repr(self.state), \\\n                      \"I see character:\", repr(nextchar)\n            if self.state is None:\n                self.token = ''        # past end of file\n                break\n            elif self.state == ' ':\n                if not nextchar:\n                    self.state = None  # end of file\n                    break\n                elif nextchar in self.whitespace:\n                    if self.debug >= 2:\n                        print \"shlex: I see whitespace in whitespace state\"\n                    if self.token or (self.posix and quoted):\n                        break   # emit current token\n                    else:\n                        continue\n                elif nextchar in self.commenters:\n                    self.instream.readline()\n                    self.lineno = self.lineno + 1\n                elif self.posix and nextchar in self.escape:\n                    escapedstate = 'a'\n                    self.state = nextchar\n                elif nextchar in self.wordchars:\n                    self.token = nextchar\n                    self.state = 'a'\n                elif nextchar in self.quotes:\n                    if not self.posix:\n                        self.token = nextchar\n                    self.state = nextchar\n                elif self.whitespace_split:\n                    self.token = nextchar\n                    self.state = 'a'\n                else:\n                    self.token = nextchar\n                    if self.token or (self.posix and quoted):\n                        break   # emit current token\n                    else:\n                        continue\n            elif self.state in self.quotes:\n                quoted = True\n                if not nextchar:      # end of file\n                    if self.debug >= 2:\n                        print \"shlex: I see EOF in quotes state\"\n                    # XXX what error should be raised here?\n                    raise ValueError, \"No closing quotation\"\n                if nextchar == self.state:\n                    if not self.posix:\n                        self.token = self.token + nextchar\n                        self.state = ' '\n                        break\n                    else:\n                        self.state = 'a'\n                elif self.posix and nextchar in self.escape and \\\n                     self.state in self.escapedquotes:\n                    escapedstate = self.state\n                    self.state = nextchar\n                else:\n                    self.token = self.token + nextchar\n            elif self.state in self.escape:\n                if not nextchar:      # end of file\n                    if self.debug >= 2:\n                        print \"shlex: I see EOF in escape state\"\n                    # XXX what error should be raised here?\n                    raise ValueError, \"No escaped character\"\n                # In posix shells, only the quote itself or the escape\n                # character may be escaped within quotes.\n                if escapedstate in self.quotes and \\\n                   nextchar != self.state and nextchar != escapedstate:\n                    self.token = self.token + self.state\n                self.token = self.token + nextchar\n                self.state = escapedstate\n            elif self.state == 'a':\n                if not nextchar:\n                    self.state = None   # end of file\n                    break\n                elif nextchar in self.whitespace:\n                    if self.debug >= 2:\n                        print \"shlex: I see whitespace in word state\"\n                    self.state = ' '\n                    if self.token or (self.posix and quoted):\n                        break   # emit current token\n                    else:\n                        continue\n                elif nextchar in self.commenters:\n                    self.instream.readline()\n                    self.lineno = self.lineno + 1\n                    if self.posix:\n                        self.state = ' '\n                        if self.token or (self.posix and quoted):\n                            break   # emit current token\n                        else:\n                            continue\n                elif self.posix and nextchar in self.quotes:\n                    self.state = nextchar\n                elif self.posix and nextchar in self.escape:\n                    escapedstate = 'a'\n                    self.state = nextchar\n                elif nextchar in self.wordchars or nextchar in self.quotes \\\n                    or self.whitespace_split:\n                    self.token = self.token + nextchar\n                else:\n                    self.pushback.appendleft(nextchar)\n                    if self.debug >= 2:\n                        print \"shlex: I see punctuation in word state\"\n                    self.state = ' '\n                    if self.token:\n                        break   # emit current token\n                    else:\n                        continue\n        result = self.token\n        self.token = ''\n        if self.posix and not quoted and result == '':\n            result = None\n        if self.debug > 1:\n            if result:\n                print \"shlex: raw token=\" + repr(result)\n            else:\n                print \"shlex: raw token=EOF\"\n        return result\n\n    def sourcehook(self, newfile):\n        \"Hook called on a filename to be sourced.\"\n        if newfile[0] == '\"':\n            newfile = newfile[1:-1]\n        # This implements cpp-like semantics for relative-path inclusion.\n        if isinstance(self.infile, basestring) and not os.path.isabs(newfile):\n            newfile = os.path.join(os.path.dirname(self.infile), newfile)\n        return (newfile, open(newfile, \"r\"))\n\n    def error_leader(self, infile=None, lineno=None):\n        \"Emit a C-compiler-like, Emacs-friendly error-message leader.\"\n        if infile is None:\n            infile = self.infile\n        if lineno is None:\n            lineno = self.lineno\n        return \"\\\"%s\\\", line %d: \" % (infile, lineno)\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        token = self.get_token()\n        if token == self.eof:\n            raise StopIteration\n        return token\n\ndef split(s, comments=False, posix=True):\n    lex = shlex(s, posix=posix)\n    lex.whitespace_split = True\n    if not comments:\n        lex.commenters = ''\n    return list(lex)\n\nif __name__ == '__main__':\n    if len(sys.argv) == 1:\n        lexer = shlex()\n    else:\n        file = sys.argv[1]\n        lexer = shlex(open(file), file)\n    while 1:\n        tt = lexer.get_token()\n        if tt:\n            print \"Token: \" + repr(tt)\n        else:\n            break\n",
		"file_name": "shlex.py"
	},
	{
		"content": "\"\"\"\nPython unit testing framework, based on Erich Gamma's JUnit and Kent Beck's\nSmalltalk testing framework.\n\nThis module contains the core framework classes that form the basis of\nspecific test cases and suites (TestCase, TestSuite etc.), and also a\ntext-based utility class for running the tests and reporting the results\n (TextTestRunner).\n\nSimple usage:\n\n    import unittest\n\n    class IntegerArithmeticTestCase(unittest.TestCase):\n        def testAdd(self):  ## test method names begin 'test*'\n            self.assertEqual((1 + 2), 3)\n            self.assertEqual(0 + 1, 1)\n        def testMultiply(self):\n            self.assertEqual((0 * 10), 0)\n            self.assertEqual((5 * 8), 40)\n\n    if __name__ == '__main__':\n        unittest.main()\n\nFurther information is available in the bundled documentation, and from\n\n  http://docs.python.org/library/unittest.html\n\nCopyright (c) 1999-2003 Steve Purcell\nCopyright (c) 2003-2010 Python Software Foundation\nThis module is free software, and you may redistribute it and/or modify\nit under the same terms as Python itself, so long as this copyright message\nand disclaimer are retained in their original form.\n\nIN NO EVENT SHALL THE AUTHOR BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT,\nSPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OF\nTHIS CODE, EVEN IF THE AUTHOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH\nDAMAGE.\n\nTHE AUTHOR SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\nPARTICULAR PURPOSE.  THE CODE PROVIDED HEREUNDER IS ON AN \"AS IS\" BASIS,\nAND THERE IS NO OBLIGATION WHATSOEVER TO PROVIDE MAINTENANCE,\nSUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.\n\"\"\"\n\n__all__ = ['TestResult', 'TestCase', 'TestSuite',\n           'TextTestRunner', 'TestLoader', 'FunctionTestCase', 'main',\n           'defaultTestLoader', 'SkipTest', 'skip', 'skipIf', 'skipUnless',\n           'expectedFailure', 'TextTestResult', 'installHandler',\n           'registerResult', 'removeResult', 'removeHandler']\n\n# Expose obsolete functions for backwards compatibility\n__all__.extend(['getTestCaseNames', 'makeSuite', 'findTestCases'])\n\n__unittest = True\n\nfrom .result import TestResult\nfrom .case import (TestCase, FunctionTestCase, SkipTest, skip, skipIf,\n                   skipUnless, expectedFailure)\nfrom .suite import BaseTestSuite, TestSuite\nfrom .loader import (TestLoader, defaultTestLoader, makeSuite, getTestCaseNames,\n                     findTestCases)\nfrom .main import TestProgram, main\nfrom .runner import TextTestRunner, TextTestResult\nfrom .signals import installHandler, registerResult, removeResult, removeHandler\n\n# deprecated\n_TextTestResult = TextTestResult\n",
		"file_name": "unittest/__init__.py"
	},
	{
		"content": "\"\"\"Test case implementation\"\"\"\n\nimport collections\nimport sys\nimport functools\nimport difflib\nimport pprint\nimport re\nimport types\nimport warnings\n\nfrom . import result\nfrom .util import (\n    strclass, safe_repr, unorderable_list_difference,\n    _count_diff_all_purpose, _count_diff_hashable\n)\n\n\n__unittest = True\n\n\nDIFF_OMITTED = ('\\nDiff is %s characters long. '\n                 'Set self.maxDiff to None to see it.')\n\nclass SkipTest(Exception):\n    \"\"\"\n    Raise this exception in a test to skip it.\n\n    Usually you can use TestCase.skipTest() or one of the skipping decorators\n    instead of raising this directly.\n    \"\"\"\n    pass\n\nclass _ExpectedFailure(Exception):\n    \"\"\"\n    Raise this when a test is expected to fail.\n\n    This is an implementation detail.\n    \"\"\"\n\n    def __init__(self, exc_info):\n        super(_ExpectedFailure, self).__init__()\n        self.exc_info = exc_info\n\nclass _UnexpectedSuccess(Exception):\n    \"\"\"\n    The test was supposed to fail, but it didn't!\n    \"\"\"\n    pass\n\ndef _id(obj):\n    return obj\n\ndef skip(reason):\n    \"\"\"\n    Unconditionally skip a test.\n    \"\"\"\n    def decorator(test_item):\n        if not isinstance(test_item, (type, types.ClassType)):\n            @functools.wraps(test_item)\n            def skip_wrapper(*args, **kwargs):\n                raise SkipTest(reason)\n            test_item = skip_wrapper\n\n        test_item.__unittest_skip__ = True\n        test_item.__unittest_skip_why__ = reason\n        return test_item\n    return decorator\n\ndef skipIf(condition, reason):\n    \"\"\"\n    Skip a test if the condition is true.\n    \"\"\"\n    if condition:\n        return skip(reason)\n    return _id\n\ndef skipUnless(condition, reason):\n    \"\"\"\n    Skip a test unless the condition is true.\n    \"\"\"\n    if not condition:\n        return skip(reason)\n    return _id\n\n\ndef expectedFailure(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            func(*args, **kwargs)\n        except Exception:\n            raise _ExpectedFailure(sys.exc_info())\n        raise _UnexpectedSuccess\n    return wrapper\n\n\nclass _AssertRaisesContext(object):\n    \"\"\"A context manager used to implement TestCase.assertRaises* methods.\"\"\"\n\n    def __init__(self, expected, test_case, expected_regexp=None):\n        self.expected = expected\n        self.failureException = test_case.failureException\n        self.expected_regexp = expected_regexp\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, tb):\n        if exc_type is None:\n            try:\n                exc_name = self.expected.__name__\n            except AttributeError:\n                exc_name = str(self.expected)\n            raise self.failureException(\n                \"{0} not raised\".format(exc_name))\n        if not issubclass(exc_type, self.expected):\n            # let unexpected exceptions pass through\n            return False\n        self.exception = exc_value # store for later retrieval\n        if self.expected_regexp is None:\n            return True\n\n        expected_regexp = self.expected_regexp\n        if not expected_regexp.search(str(exc_value)):\n            raise self.failureException('\"%s\" does not match \"%s\"' %\n                     (expected_regexp.pattern, str(exc_value)))\n        return True\n\n\nclass TestCase(object):\n    \"\"\"A class whose instances are single test cases.\n\n    By default, the test code itself should be placed in a method named\n    'runTest'.\n\n    If the fixture may be used for many test cases, create as\n    many test methods as are needed. When instantiating such a TestCase\n    subclass, specify in the constructor arguments the name of the test method\n    that the instance is to execute.\n\n    Test authors should subclass TestCase for their own tests. Construction\n    and deconstruction of the test's environment ('fixture') can be\n    implemented by overriding the 'setUp' and 'tearDown' methods respectively.\n\n    If it is necessary to override the __init__ method, the base class\n    __init__ method must always be called. It is important that subclasses\n    should not change the signature of their __init__ method, since instances\n    of the classes are instantiated automatically by parts of the framework\n    in order to be run.\n\n    When subclassing TestCase, you can set these attributes:\n    * failureException: determines which exception will be raised when\n        the instance's assertion methods fail; test methods raising this\n        exception will be deemed to have 'failed' rather than 'errored'.\n    * longMessage: determines whether long messages (including repr of\n        objects used in assert methods) will be printed on failure in *addition*\n        to any explicit message passed.\n    * maxDiff: sets the maximum length of a diff in failure messages\n        by assert methods using difflib. It is looked up as an instance\n        attribute so can be configured by individual tests if required.\n    \"\"\"\n\n    failureException = AssertionError\n\n    longMessage = False\n\n    maxDiff = 80*8\n\n    # If a string is longer than _diffThreshold, use normal comparison instead\n    # of difflib.  See #11763.\n    _diffThreshold = 2**16\n\n    # Attribute used by TestSuite for classSetUp\n\n    _classSetupFailed = False\n\n    def __init__(self, methodName='runTest'):\n        \"\"\"Create an instance of the class that will use the named test\n           method when executed. Raises a ValueError if the instance does\n           not have a method with the specified name.\n        \"\"\"\n        self._testMethodName = methodName\n        self._resultForDoCleanups = None\n        try:\n            testMethod = getattr(self, methodName)\n        except AttributeError:\n            raise ValueError(\"no such test method in %s: %s\" %\n                  (self.__class__, methodName))\n        self._testMethodDoc = testMethod.__doc__\n        self._cleanups = []\n\n        # Map types to custom assertEqual functions that will compare\n        # instances of said type in more detail to generate a more useful\n        # error message.\n        self._type_equality_funcs = {}\n        self.addTypeEqualityFunc(dict, 'assertDictEqual')\n        self.addTypeEqualityFunc(list, 'assertListEqual')\n        self.addTypeEqualityFunc(tuple, 'assertTupleEqual')\n        self.addTypeEqualityFunc(set, 'assertSetEqual')\n        self.addTypeEqualityFunc(frozenset, 'assertSetEqual')\n        try:\n            self.addTypeEqualityFunc(unicode, 'assertMultiLineEqual')\n        except NameError:\n            # No unicode support in this build\n            pass\n\n    def addTypeEqualityFunc(self, typeobj, function):\n        \"\"\"Add a type specific assertEqual style function to compare a type.\n\n        This method is for use by TestCase subclasses that need to register\n        their own type equality functions to provide nicer error messages.\n\n        Args:\n            typeobj: The data type to call this function on when both values\n                    are of the same type in assertEqual().\n            function: The callable taking two arguments and an optional\n                    msg= argument that raises self.failureException with a\n                    useful error message when the two arguments are not equal.\n        \"\"\"\n        self._type_equality_funcs[typeobj] = function\n\n    def addCleanup(self, function, *args, **kwargs):\n        \"\"\"Add a function, with arguments, to be called when the test is\n        completed. Functions added are called on a LIFO basis and are\n        called after tearDown on test failure or success.\n\n        Cleanup items are called even if setUp fails (unlike tearDown).\"\"\"\n        self._cleanups.append((function, args, kwargs))\n\n    def setUp(self):\n        \"Hook method for setting up the test fixture before exercising it.\"\n        pass\n\n    def tearDown(self):\n        \"Hook method for deconstructing the test fixture after testing it.\"\n        pass\n\n    @classmethod\n    def setUpClass(cls):\n        \"Hook method for setting up class fixture before running tests in the class.\"\n\n    @classmethod\n    def tearDownClass(cls):\n        \"Hook method for deconstructing the class fixture after running all tests in the class.\"\n\n    def countTestCases(self):\n        return 1\n\n    def defaultTestResult(self):\n        return result.TestResult()\n\n    def shortDescription(self):\n        \"\"\"Returns a one-line description of the test, or None if no\n        description has been provided.\n\n        The default implementation of this method returns the first line of\n        the specified test method's docstring.\n        \"\"\"\n        doc = self._testMethodDoc\n        return doc and doc.split(\"\\n\")[0].strip() or None\n\n\n    def id(self):\n        return \"%s.%s\" % (strclass(self.__class__), self._testMethodName)\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self._testMethodName == other._testMethodName\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((type(self), self._testMethodName))\n\n    def __str__(self):\n        return \"%s (%s)\" % (self._testMethodName, strclass(self.__class__))\n\n    def __repr__(self):\n        return \"<%s testMethod=%s>\" % \\\n               (strclass(self.__class__), self._testMethodName)\n\n    def _addSkip(self, result, reason):\n        addSkip = getattr(result, 'addSkip', None)\n        if addSkip is not None:\n            addSkip(self, reason)\n        else:\n            warnings.warn(\"TestResult has no addSkip method, skips not reported\",\n                          RuntimeWarning, 2)\n            result.addSuccess(self)\n\n    def run(self, result=None):\n        orig_result = result\n        if result is None:\n            result = self.defaultTestResult()\n            startTestRun = getattr(result, 'startTestRun', None)\n            if startTestRun is not None:\n                startTestRun()\n\n        self._resultForDoCleanups = result\n        result.startTest(self)\n\n        testMethod = getattr(self, self._testMethodName)\n        if (getattr(self.__class__, \"__unittest_skip__\", False) or\n            getattr(testMethod, \"__unittest_skip__\", False)):\n            # If the class or method was skipped.\n            try:\n                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')\n                            or getattr(testMethod, '__unittest_skip_why__', ''))\n                self._addSkip(result, skip_why)\n            finally:\n                result.stopTest(self)\n            return\n        try:\n            success = False\n            try:\n                self.setUp()\n            except SkipTest as e:\n                self._addSkip(result, str(e))\n            except KeyboardInterrupt:\n                raise\n            except:\n                result.addError(self, sys.exc_info())\n            else:\n                try:\n                    testMethod()\n                except KeyboardInterrupt:\n                    raise\n                except self.failureException:\n                    result.addFailure(self, sys.exc_info())\n                except _ExpectedFailure as e:\n                    addExpectedFailure = getattr(result, 'addExpectedFailure', None)\n                    if addExpectedFailure is not None:\n                        addExpectedFailure(self, e.exc_info)\n                    else:\n                        warnings.warn(\"TestResult has no addExpectedFailure method, reporting as passes\",\n                                      RuntimeWarning)\n                        result.addSuccess(self)\n                except _UnexpectedSuccess:\n                    addUnexpectedSuccess = getattr(result, 'addUnexpectedSuccess', None)\n                    if addUnexpectedSuccess is not None:\n                        addUnexpectedSuccess(self)\n                    else:\n                        warnings.warn(\"TestResult has no addUnexpectedSuccess method, reporting as failures\",\n                                      RuntimeWarning)\n                        result.addFailure(self, sys.exc_info())\n                except SkipTest as e:\n                    self._addSkip(result, str(e))\n                except:\n                    result.addError(self, sys.exc_info())\n                else:\n                    success = True\n\n                try:\n                    self.tearDown()\n                except KeyboardInterrupt:\n                    raise\n                except:\n                    result.addError(self, sys.exc_info())\n                    success = False\n\n            cleanUpSuccess = self.doCleanups()\n            success = success and cleanUpSuccess\n            if success:\n                result.addSuccess(self)\n        finally:\n            result.stopTest(self)\n            if orig_result is None:\n                stopTestRun = getattr(result, 'stopTestRun', None)\n                if stopTestRun is not None:\n                    stopTestRun()\n\n    def doCleanups(self):\n        \"\"\"Execute all cleanup functions. Normally called for you after\n        tearDown.\"\"\"\n        result = self._resultForDoCleanups\n        ok = True\n        while self._cleanups:\n            function, args, kwargs = self._cleanups.pop(-1)\n            try:\n                function(*args, **kwargs)\n            except KeyboardInterrupt:\n                raise\n            except:\n                ok = False\n                result.addError(self, sys.exc_info())\n        return ok\n\n    def __call__(self, *args, **kwds):\n        return self.run(*args, **kwds)\n\n    def debug(self):\n        \"\"\"Run the test without collecting errors in a TestResult\"\"\"\n        self.setUp()\n        getattr(self, self._testMethodName)()\n        self.tearDown()\n        while self._cleanups:\n            function, args, kwargs = self._cleanups.pop(-1)\n            function(*args, **kwargs)\n\n    def skipTest(self, reason):\n        \"\"\"Skip this test.\"\"\"\n        raise SkipTest(reason)\n\n    def fail(self, msg=None):\n        \"\"\"Fail immediately, with the given message.\"\"\"\n        raise self.failureException(msg)\n\n    def assertFalse(self, expr, msg=None):\n        \"\"\"Check that the expression is false.\"\"\"\n        if expr:\n            msg = self._formatMessage(msg, \"%s is not false\" % safe_repr(expr))\n            raise self.failureException(msg)\n\n    def assertTrue(self, expr, msg=None):\n        \"\"\"Check that the expression is true.\"\"\"\n        if not expr:\n            msg = self._formatMessage(msg, \"%s is not true\" % safe_repr(expr))\n            raise self.failureException(msg)\n\n    def _formatMessage(self, msg, standardMsg):\n        \"\"\"Honour the longMessage attribute when generating failure messages.\n        If longMessage is False this means:\n        * Use only an explicit message if it is provided\n        * Otherwise use the standard message for the assert\n\n        If longMessage is True:\n        * Use the standard message\n        * If an explicit message is provided, plus ' : ' and the explicit message\n        \"\"\"\n        if not self.longMessage:\n            return msg or standardMsg\n        if msg is None:\n            return standardMsg\n        try:\n            # don't switch to '{}' formatting in Python 2.X\n            # it changes the way unicode input is handled\n            return '%s : %s' % (standardMsg, msg)\n        except UnicodeDecodeError:\n            return  '%s : %s' % (safe_repr(standardMsg), safe_repr(msg))\n\n\n    def assertRaises(self, excClass, callableObj=None, *args, **kwargs):\n        \"\"\"Fail unless an exception of class excClass is raised\n           by callableObj when invoked with arguments args and keyword\n           arguments kwargs. If a different type of exception is\n           raised, it will not be caught, and the test case will be\n           deemed to have suffered an error, exactly as for an\n           unexpected exception.\n\n           If called with callableObj omitted or None, will return a\n           context object used like this::\n\n                with self.assertRaises(SomeException):\n                    do_something()\n\n           The context manager keeps a reference to the exception as\n           the 'exception' attribute. This allows you to inspect the\n           exception after the assertion::\n\n               with self.assertRaises(SomeException) as cm:\n                   do_something()\n               the_exception = cm.exception\n               self.assertEqual(the_exception.error_code, 3)\n        \"\"\"\n        context = _AssertRaisesContext(excClass, self)\n        if callableObj is None:\n            return context\n        with context:\n            callableObj(*args, **kwargs)\n\n    def _getAssertEqualityFunc(self, first, second):\n        \"\"\"Get a detailed comparison function for the types of the two args.\n\n        Returns: A callable accepting (first, second, msg=None) that will\n        raise a failure exception if first != second with a useful human\n        readable error message for those types.\n        \"\"\"\n        #\n        # NOTE(gregory.p.smith): I considered isinstance(first, type(second))\n        # and vice versa.  I opted for the conservative approach in case\n        # subclasses are not intended to be compared in detail to their super\n        # class instances using a type equality func.  This means testing\n        # subtypes won't automagically use the detailed comparison.  Callers\n        # should use their type specific assertSpamEqual method to compare\n        # subclasses if the detailed comparison is desired and appropriate.\n        # See the discussion in http://bugs.python.org/issue2578.\n        #\n        if type(first) is type(second):\n            asserter = self._type_equality_funcs.get(type(first))\n            if asserter is not None:\n                if isinstance(asserter, basestring):\n                    asserter = getattr(self, asserter)\n                return asserter\n\n        return self._baseAssertEqual\n\n    def _baseAssertEqual(self, first, second, msg=None):\n        \"\"\"The default assertEqual implementation, not type specific.\"\"\"\n        if not first == second:\n            standardMsg = '%s != %s' % (safe_repr(first), safe_repr(second))\n            msg = self._formatMessage(msg, standardMsg)\n            raise self.failureException(msg)\n\n    def assertEqual(self, first, second, msg=None):\n        \"\"\"Fail if the two objects are unequal as determined by the '=='\n           operator.\n        \"\"\"\n        assertion_func = self._getAssertEqualityFunc(first, second)\n        assertion_func(first, second, msg=msg)\n\n    def assertNotEqual(self, first, second, msg=None):\n        \"\"\"Fail if the two objects are equal as determined by the '!='\n           operator.\n        \"\"\"\n        if not first != second:\n            msg = self._formatMessage(msg, '%s == %s' % (safe_repr(first),\n                                                          safe_repr(second)))\n            raise self.failureException(msg)\n\n\n    def assertAlmostEqual(self, first, second, places=None, msg=None, delta=None):\n        \"\"\"Fail if the two objects are unequal as determined by their\n           difference rounded to the given number of decimal places\n           (default 7) and comparing to zero, or by comparing that the\n           between the two objects is more than the given delta.\n\n           Note that decimal places (from zero) are usually not the same\n           as significant digits (measured from the most signficant digit).\n\n           If the two objects compare equal then they will automatically\n           compare almost equal.\n        \"\"\"\n        if first == second:\n            # shortcut\n            return\n        if delta is not None and places is not None:\n            raise TypeError(\"specify delta or places not both\")\n\n        if delta is not None:\n            if abs(first - second) <= delta:\n                return\n\n            standardMsg = '%s != %s within %s delta' % (safe_repr(first),\n                                                        safe_repr(second),\n                                                        safe_repr(delta))\n        else:\n            if places is None:\n                places = 7\n\n            if round(abs(second-first), places) == 0:\n                return\n\n            standardMsg = '%s != %s within %r places' % (safe_repr(first),\n                                                          safe_repr(second),\n                                                          places)\n        msg = self._formatMessage(msg, standardMsg)\n        raise self.failureException(msg)\n\n    def assertNotAlmostEqual(self, first, second, places=None, msg=None, delta=None):\n        \"\"\"Fail if the two objects are equal as determined by their\n           difference rounded to the given number of decimal places\n           (default 7) and comparing to zero, or by comparing that the\n           between the two objects is less than the given delta.\n\n           Note that decimal places (from zero) are usually not the same\n           as significant digits (measured from the most signficant digit).\n\n           Objects that are equal automatically fail.\n        \"\"\"\n        if delta is not None and places is not None:\n            raise TypeError(\"specify delta or places not both\")\n        if delta is not None:\n            if not (first == second) and abs(first - second) > delta:\n                return\n            standardMsg = '%s == %s within %s delta' % (safe_repr(first),\n                                                        safe_repr(second),\n                                                        safe_repr(delta))\n        else:\n            if places is None:\n                places = 7\n            if not (first == second) and round(abs(second-first), places) != 0:\n                return\n            standardMsg = '%s == %s within %r places' % (safe_repr(first),\n                                                         safe_repr(second),\n                                                         places)\n\n        msg = self._formatMessage(msg, standardMsg)\n        raise self.failureException(msg)\n\n    # Synonyms for assertion methods\n\n    # The plurals are undocumented.  Keep them that way to discourage use.\n    # Do not add more.  Do not remove.\n    # Going through a deprecation cycle on these would annoy many people.\n    assertEquals = assertEqual\n    assertNotEquals = assertNotEqual\n    assertAlmostEquals = assertAlmostEqual\n    assertNotAlmostEquals = assertNotAlmostEqual\n    assert_ = assertTrue\n\n    # These fail* assertion method names are pending deprecation and will\n    # be a DeprecationWarning in 3.2; http://bugs.python.org/issue2578\n    def _deprecate(original_func):\n        def deprecated_func(*args, **kwargs):\n            warnings.warn(\n                'Please use {0} instead.'.format(original_func.__name__),\n                PendingDeprecationWarning, 2)\n            return original_func(*args, **kwargs)\n        return deprecated_func\n\n    failUnlessEqual = _deprecate(assertEqual)\n    failIfEqual = _deprecate(assertNotEqual)\n    failUnlessAlmostEqual = _deprecate(assertAlmostEqual)\n    failIfAlmostEqual = _deprecate(assertNotAlmostEqual)\n    failUnless = _deprecate(assertTrue)\n    failUnlessRaises = _deprecate(assertRaises)\n    failIf = _deprecate(assertFalse)\n\n    def assertSequenceEqual(self, seq1, seq2, msg=None, seq_type=None):\n        \"\"\"An equality assertion for ordered sequences (like lists and tuples).\n\n        For the purposes of this function, a valid ordered sequence type is one\n        which can be indexed, has a length, and has an equality operator.\n\n        Args:\n            seq1: The first sequence to compare.\n            seq2: The second sequence to compare.\n            seq_type: The expected datatype of the sequences, or None if no\n                    datatype should be enforced.\n            msg: Optional message to use on failure instead of a list of\n                    differences.\n        \"\"\"\n        if seq_type is not None:\n            seq_type_name = seq_type.__name__\n            if not isinstance(seq1, seq_type):\n                raise self.failureException('First sequence is not a %s: %s'\n                                        % (seq_type_name, safe_repr(seq1)))\n            if not isinstance(seq2, seq_type):\n                raise self.failureException('Second sequence is not a %s: %s'\n                                        % (seq_type_name, safe_repr(seq2)))\n        else:\n            seq_type_name = \"sequence\"\n\n        differing = None\n        try:\n            len1 = len(seq1)\n        except (TypeError, NotImplementedError):\n            differing = 'First %s has no length.    Non-sequence?' % (\n                    seq_type_name)\n\n        if differing is None:\n            try:\n                len2 = len(seq2)\n            except (TypeError, NotImplementedError):\n                differing = 'Second %s has no length.    Non-sequence?' % (\n                        seq_type_name)\n\n        if differing is None:\n            if seq1 == seq2:\n                return\n\n            seq1_repr = safe_repr(seq1)\n            seq2_repr = safe_repr(seq2)\n            if len(seq1_repr) > 30:\n                seq1_repr = seq1_repr[:30] + '...'\n            if len(seq2_repr) > 30:\n                seq2_repr = seq2_repr[:30] + '...'\n            elements = (seq_type_name.capitalize(), seq1_repr, seq2_repr)\n            differing = '%ss differ: %s != %s\\n' % elements\n\n            for i in xrange(min(len1, len2)):\n                try:\n                    item1 = seq1[i]\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('\\nUnable to index element %d of first %s\\n' %\n                                 (i, seq_type_name))\n                    break\n\n                try:\n                    item2 = seq2[i]\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('\\nUnable to index element %d of second %s\\n' %\n                                 (i, seq_type_name))\n                    break\n\n                if item1 != item2:\n                    differing += ('\\nFirst differing element %d:\\n%s\\n%s\\n' %\n                                 (i, item1, item2))\n                    break\n            else:\n                if (len1 == len2 and seq_type is None and\n                    type(seq1) != type(seq2)):\n                    # The sequences are the same, but have differing types.\n                    return\n\n            if len1 > len2:\n                differing += ('\\nFirst %s contains %d additional '\n                             'elements.\\n' % (seq_type_name, len1 - len2))\n                try:\n                    differing += ('First extra element %d:\\n%s\\n' %\n                                  (len2, seq1[len2]))\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('Unable to index element %d '\n                                  'of first %s\\n' % (len2, seq_type_name))\n            elif len1 < len2:\n                differing += ('\\nSecond %s contains %d additional '\n                             'elements.\\n' % (seq_type_name, len2 - len1))\n                try:\n                    differing += ('First extra element %d:\\n%s\\n' %\n                                  (len1, seq2[len1]))\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('Unable to index element %d '\n                                  'of second %s\\n' % (len1, seq_type_name))\n        standardMsg = differing\n        diffMsg = '\\n' + '\\n'.join(\n            difflib.ndiff(pprint.pformat(seq1).splitlines(),\n                          pprint.pformat(seq2).splitlines()))\n        standardMsg = self._truncateMessage(standardMsg, diffMsg)\n        msg = self._formatMessage(msg, standardMsg)\n        self.fail(msg)\n\n    def _truncateMessage(self, message, diff):\n        max_diff = self.maxDiff\n        if max_diff is None or len(diff) <= max_diff:\n            return message + diff\n        return message + (DIFF_OMITTED % len(diff))\n\n    def assertListEqual(self, list1, list2, msg=None):\n        \"\"\"A list-specific equality assertion.\n\n        Args:\n            list1: The first list to compare.\n            list2: The second list to compare.\n            msg: Optional message to use on failure instead of a list of\n                    differences.\n\n        \"\"\"\n        self.assertSequenceEqual(list1, list2, msg, seq_type=list)\n\n    def assertTupleEqual(self, tuple1, tuple2, msg=None):\n        \"\"\"A tuple-specific equality assertion.\n\n        Args:\n            tuple1: The first tuple to compare.\n            tuple2: The second tuple to compare.\n            msg: Optional message to use on failure instead of a list of\n                    differences.\n        \"\"\"\n        self.assertSequenceEqual(tuple1, tuple2, msg, seq_type=tuple)\n\n    def assertSetEqual(self, set1, set2, msg=None):\n        \"\"\"A set-specific equality assertion.\n\n        Args:\n            set1: The first set to compare.\n            set2: The second set to compare.\n            msg: Optional message to use on failure instead of a list of\n                    differences.\n\n        assertSetEqual uses ducktyping to support different types of sets, and\n        is optimized for sets specifically (parameters must support a\n        difference method).\n        \"\"\"\n        try:\n            difference1 = set1.difference(set2)\n        except TypeError, e:\n            self.fail('invalid type when attempting set difference: %s' % e)\n        except AttributeError, e:\n            self.fail('first argument does not support set difference: %s' % e)\n\n        try:\n            difference2 = set2.difference(set1)\n        except TypeError, e:\n            self.fail('invalid type when attempting set difference: %s' % e)\n        except AttributeError, e:\n            self.fail('second argument does not support set difference: %s' % e)\n\n        if not (difference1 or difference2):\n            return\n\n        lines = []\n        if difference1:\n            lines.append('Items in the first set but not the second:')\n            for item in difference1:\n                lines.append(repr(item))\n        if difference2:\n            lines.append('Items in the second set but not the first:')\n            for item in difference2:\n                lines.append(repr(item))\n\n        standardMsg = '\\n'.join(lines)\n        self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIn(self, member, container, msg=None):\n        \"\"\"Just like self.assertTrue(a in b), but with a nicer default message.\"\"\"\n        if member not in container:\n            standardMsg = '%s not found in %s' % (safe_repr(member),\n                                                  safe_repr(container))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertNotIn(self, member, container, msg=None):\n        \"\"\"Just like self.assertTrue(a not in b), but with a nicer default message.\"\"\"\n        if member in container:\n            standardMsg = '%s unexpectedly found in %s' % (safe_repr(member),\n                                                        safe_repr(container))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIs(self, expr1, expr2, msg=None):\n        \"\"\"Just like self.assertTrue(a is b), but with a nicer default message.\"\"\"\n        if expr1 is not expr2:\n            standardMsg = '%s is not %s' % (safe_repr(expr1),\n                                             safe_repr(expr2))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIsNot(self, expr1, expr2, msg=None):\n        \"\"\"Just like self.assertTrue(a is not b), but with a nicer default message.\"\"\"\n        if expr1 is expr2:\n            standardMsg = 'unexpectedly identical: %s' % (safe_repr(expr1),)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertDictEqual(self, d1, d2, msg=None):\n        self.assertIsInstance(d1, dict, 'First argument is not a dictionary')\n        self.assertIsInstance(d2, dict, 'Second argument is not a dictionary')\n\n        if d1 != d2:\n            standardMsg = '%s != %s' % (safe_repr(d1, True), safe_repr(d2, True))\n            diff = ('\\n' + '\\n'.join(difflib.ndiff(\n                           pprint.pformat(d1).splitlines(),\n                           pprint.pformat(d2).splitlines())))\n            standardMsg = self._truncateMessage(standardMsg, diff)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertDictContainsSubset(self, expected, actual, msg=None):\n        \"\"\"Checks whether actual is a superset of expected.\"\"\"\n        missing = []\n        mismatched = []\n        for key, value in expected.iteritems():\n            if key not in actual:\n                missing.append(key)\n            elif value != actual[key]:\n                mismatched.append('%s, expected: %s, actual: %s' %\n                                  (safe_repr(key), safe_repr(value),\n                                   safe_repr(actual[key])))\n\n        if not (missing or mismatched):\n            return\n\n        standardMsg = ''\n        if missing:\n            standardMsg = 'Missing: %s' % ','.join(safe_repr(m) for m in\n                                                    missing)\n        if mismatched:\n            if standardMsg:\n                standardMsg += '; '\n            standardMsg += 'Mismatched values: %s' % ','.join(mismatched)\n\n        self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertItemsEqual(self, expected_seq, actual_seq, msg=None):\n        \"\"\"An unordered sequence specific comparison. It asserts that\n        actual_seq and expected_seq have the same element counts.\n        Equivalent to::\n\n            self.assertEqual(Counter(iter(actual_seq)),\n                             Counter(iter(expected_seq)))\n\n        Asserts that each element has the same count in both sequences.\n        Example:\n            - [0, 1, 1] and [1, 0, 1] compare equal.\n            - [0, 0, 1] and [0, 1] compare unequal.\n        \"\"\"\n        first_seq, second_seq = list(expected_seq), list(actual_seq)\n        with warnings.catch_warnings():\n            if sys.py3kwarning:\n                # Silence Py3k warning raised during the sorting\n                for _msg in [\"(code|dict|type) inequality comparisons\",\n                             \"builtin_function_or_method order comparisons\",\n                             \"comparing unequal types\"]:\n                    warnings.filterwarnings(\"ignore\", _msg, DeprecationWarning)\n            try:\n                first = collections.Counter(first_seq)\n                second = collections.Counter(second_seq)\n            except TypeError:\n                # Handle case with unhashable elements\n                differences = _count_diff_all_purpose(first_seq, second_seq)\n            else:\n                if first == second:\n                    return\n                differences = _count_diff_hashable(first_seq, second_seq)\n\n        if differences:\n            standardMsg = 'Element counts were not equal:\\n'\n            lines = ['First has %d, Second has %d:  %r' % diff for diff in differences]\n            diffMsg = '\\n'.join(lines)\n            standardMsg = self._truncateMessage(standardMsg, diffMsg)\n            msg = self._formatMessage(msg, standardMsg)\n            self.fail(msg)\n\n    def assertMultiLineEqual(self, first, second, msg=None):\n        \"\"\"Assert that two multi-line strings are equal.\"\"\"\n        self.assertIsInstance(first, basestring,\n                'First argument is not a string')\n        self.assertIsInstance(second, basestring,\n                'Second argument is not a string')\n\n        if first != second:\n            # don't use difflib if the strings are too long\n            if (len(first) > self._diffThreshold or\n                len(second) > self._diffThreshold):\n                self._baseAssertEqual(first, second, msg)\n            firstlines = first.splitlines(True)\n            secondlines = second.splitlines(True)\n            if len(firstlines) == 1 and first.strip('\\r\\n') == first:\n                firstlines = [first + '\\n']\n                secondlines = [second + '\\n']\n            standardMsg = '%s != %s' % (safe_repr(first, True),\n                                        safe_repr(second, True))\n            diff = '\\n' + ''.join(difflib.ndiff(firstlines, secondlines))\n            standardMsg = self._truncateMessage(standardMsg, diff)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertLess(self, a, b, msg=None):\n        \"\"\"Just like self.assertTrue(a < b), but with a nicer default message.\"\"\"\n        if not a < b:\n            standardMsg = '%s not less than %s' % (safe_repr(a), safe_repr(b))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertLessEqual(self, a, b, msg=None):\n        \"\"\"Just like self.assertTrue(a <= b), but with a nicer default message.\"\"\"\n        if not a <= b:\n            standardMsg = '%s not less than or equal to %s' % (safe_repr(a), safe_repr(b))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertGreater(self, a, b, msg=None):\n        \"\"\"Just like self.assertTrue(a > b), but with a nicer default message.\"\"\"\n        if not a > b:\n            standardMsg = '%s not greater than %s' % (safe_repr(a), safe_repr(b))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertGreaterEqual(self, a, b, msg=None):\n        \"\"\"Just like self.assertTrue(a >= b), but with a nicer default message.\"\"\"\n        if not a >= b:\n            standardMsg = '%s not greater than or equal to %s' % (safe_repr(a), safe_repr(b))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIsNone(self, obj, msg=None):\n        \"\"\"Same as self.assertTrue(obj is None), with a nicer default message.\"\"\"\n        if obj is not None:\n            standardMsg = '%s is not None' % (safe_repr(obj),)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIsNotNone(self, obj, msg=None):\n        \"\"\"Included for symmetry with assertIsNone.\"\"\"\n        if obj is None:\n            standardMsg = 'unexpectedly None'\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIsInstance(self, obj, cls, msg=None):\n        \"\"\"Same as self.assertTrue(isinstance(obj, cls)), with a nicer\n        default message.\"\"\"\n        if not isinstance(obj, cls):\n            standardMsg = '%s is not an instance of %r' % (safe_repr(obj), cls)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertNotIsInstance(self, obj, cls, msg=None):\n        \"\"\"Included for symmetry with assertIsInstance.\"\"\"\n        if isinstance(obj, cls):\n            standardMsg = '%s is an instance of %r' % (safe_repr(obj), cls)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertRaisesRegexp(self, expected_exception, expected_regexp,\n                           callable_obj=None, *args, **kwargs):\n        \"\"\"Asserts that the message in a raised exception matches a regexp.\n\n        Args:\n            expected_exception: Exception class expected to be raised.\n            expected_regexp: Regexp (re pattern object or string) expected\n                    to be found in error message.\n            callable_obj: Function to be called.\n            args: Extra args.\n            kwargs: Extra kwargs.\n        \"\"\"\n        if expected_regexp is not None:\n            expected_regexp = re.compile(expected_regexp)\n        context = _AssertRaisesContext(expected_exception, self, expected_regexp)\n        if callable_obj is None:\n            return context\n        with context:\n            callable_obj(*args, **kwargs)\n\n    def assertRegexpMatches(self, text, expected_regexp, msg=None):\n        \"\"\"Fail the test unless the text matches the regular expression.\"\"\"\n        if isinstance(expected_regexp, basestring):\n            expected_regexp = re.compile(expected_regexp)\n        if not expected_regexp.search(text):\n            msg = msg or \"Regexp didn't match\"\n            msg = '%s: %r not found in %r' % (msg, expected_regexp.pattern, text)\n            raise self.failureException(msg)\n\n    def assertNotRegexpMatches(self, text, unexpected_regexp, msg=None):\n        \"\"\"Fail the test if the text matches the regular expression.\"\"\"\n        if isinstance(unexpected_regexp, basestring):\n            unexpected_regexp = re.compile(unexpected_regexp)\n        match = unexpected_regexp.search(text)\n        if match:\n            msg = msg or \"Regexp matched\"\n            msg = '%s: %r matches %r in %r' % (msg,\n                                               text[match.start():match.end()],\n                                               unexpected_regexp.pattern,\n                                               text)\n            raise self.failureException(msg)\n\n\nclass FunctionTestCase(TestCase):\n    \"\"\"A test case that wraps a test function.\n\n    This is useful for slipping pre-existing test functions into the\n    unittest framework. Optionally, set-up and tidy-up functions can be\n    supplied. As with TestCase, the tidy-up ('tearDown') function will\n    always be called if the set-up ('setUp') function ran successfully.\n    \"\"\"\n\n    def __init__(self, testFunc, setUp=None, tearDown=None, description=None):\n        super(FunctionTestCase, self).__init__()\n        self._setUpFunc = setUp\n        self._tearDownFunc = tearDown\n        self._testFunc = testFunc\n        self._description = description\n\n    def setUp(self):\n        if self._setUpFunc is not None:\n            self._setUpFunc()\n\n    def tearDown(self):\n        if self._tearDownFunc is not None:\n            self._tearDownFunc()\n\n    def runTest(self):\n        self._testFunc()\n\n    def id(self):\n        return self._testFunc.__name__\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n\n        return self._setUpFunc == other._setUpFunc and \\\n               self._tearDownFunc == other._tearDownFunc and \\\n               self._testFunc == other._testFunc and \\\n               self._description == other._description\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((type(self), self._setUpFunc, self._tearDownFunc,\n                     self._testFunc, self._description))\n\n    def __str__(self):\n        return \"%s (%s)\" % (strclass(self.__class__),\n                            self._testFunc.__name__)\n\n    def __repr__(self):\n        return \"<%s tec=%s>\" % (strclass(self.__class__),\n                                     self._testFunc)\n\n    def shortDescription(self):\n        if self._description is not None:\n            return self._description\n        doc = self._testFunc.__doc__\n        return doc and doc.split(\"\\n\")[0].strip() or None\n",
		"file_name": "unittest/case.py"
	},
	{
		"content": "\"\"\"Test result object\"\"\"\n\nimport os\nimport sys\nimport traceback\n\nfrom StringIO import StringIO\n\nfrom . import util\nfrom functools import wraps\n\n__unittest = True\n\ndef failfast(method):\n    @wraps(method)\n    def inner(self, *args, **kw):\n        if getattr(self, 'failfast', False):\n            self.stop()\n        return method(self, *args, **kw)\n    return inner\n\nSTDOUT_LINE = '\\nStdout:\\n%s'\nSTDERR_LINE = '\\nStderr:\\n%s'\n\n\nclass TestResult(object):\n    \"\"\"Holder for test result information.\n\n    Test results are automatically managed by the TestCase and TestSuite\n    classes, and do not need to be explicitly manipulated by writers of tests.\n\n    Each instance holds the total number of tests run, and collections of\n    failures and errors that occurred among those test runs. The collections\n    contain tuples of (testcase, exceptioninfo), where exceptioninfo is the\n    formatted traceback of the error that occurred.\n    \"\"\"\n    _previousTestClass = None\n    _testRunEntered = False\n    _moduleSetUpFailed = False\n    def __init__(self, stream=None, descriptions=None, verbosity=None):\n        self.failfast = False\n        self.failures = []\n        self.errors = []\n        self.testsRun = 0\n        self.skipped = []\n        self.expectedFailures = []\n        self.unexpectedSuccesses = []\n        self.shouldStop = False\n        self.buffer = False\n        self._stdout_buffer = None\n        self._stderr_buffer = None\n        self._original_stdout = sys.stdout\n        self._original_stderr = sys.stderr\n        self._mirrorOutput = False\n\n    def printErrors(self):\n        \"Called by TestRunner after test run\"\n\n    def startTest(self, test):\n        \"Called when the given test is about to be run\"\n        self.testsRun += 1\n        self._mirrorOutput = False\n        self._setupStdout()\n\n    def _setupStdout(self):\n        if self.buffer:\n            if self._stderr_buffer is None:\n                self._stderr_buffer = StringIO()\n                self._stdout_buffer = StringIO()\n            sys.stdout = self._stdout_buffer\n            sys.stderr = self._stderr_buffer\n\n    def startTestRun(self):\n        \"\"\"Called once before any tests are executed.\n\n        See startTest for a method called before each test.\n        \"\"\"\n\n    def stopTest(self, test):\n        \"\"\"Called when the given test has been run\"\"\"\n        self._restoreStdout()\n        self._mirrorOutput = False\n\n    def _restoreStdout(self):\n        if self.buffer:\n            if self._mirrorOutput:\n                output = sys.stdout.getvalue()\n                error = sys.stderr.getvalue()\n                if output:\n                    if not output.endswith('\\n'):\n                        output += '\\n'\n                    self._original_stdout.write(STDOUT_LINE % output)\n                if error:\n                    if not error.endswith('\\n'):\n                        error += '\\n'\n                    self._original_stderr.write(STDERR_LINE % error)\n\n            sys.stdout = self._original_stdout\n            sys.stderr = self._original_stderr\n            self._stdout_buffer.seek(0)\n            self._stdout_buffer.truncate()\n            self._stderr_buffer.seek(0)\n            self._stderr_buffer.truncate()\n\n    def stopTestRun(self):\n        \"\"\"Called once after all tests are executed.\n\n        See stopTest for a method called after each test.\n        \"\"\"\n\n    @failfast\n    def addError(self, test, err):\n        \"\"\"Called when an error has occurred. 'err' is a tuple of values as\n        returned by sys.exc_info().\n        \"\"\"\n        self.errors.append((test, self._exc_info_to_string(err, test)))\n        self._mirrorOutput = True\n\n    @failfast\n    def addFailure(self, test, err):\n        \"\"\"Called when an error has occurred. 'err' is a tuple of values as\n        returned by sys.exc_info().\"\"\"\n        self.failures.append((test, self._exc_info_to_string(err, test)))\n        self._mirrorOutput = True\n\n    def addSuccess(self, test):\n        \"Called when a test has completed successfully\"\n        pass\n\n    def addSkip(self, test, reason):\n        \"\"\"Called when a test is skipped.\"\"\"\n        self.skipped.append((test, reason))\n\n    def addExpectedFailure(self, test, err):\n        \"\"\"Called when an expected failure/error occured.\"\"\"\n        self.expectedFailures.append(\n            (test, self._exc_info_to_string(err, test)))\n\n    @failfast\n    def addUnexpectedSuccess(self, test):\n        \"\"\"Called when a test was expected to fail, but succeed.\"\"\"\n        self.unexpectedSuccesses.append(test)\n\n    def wasSuccessful(self):\n        \"Tells whether or not this result was a success\"\n        return len(self.failures) == len(self.errors) == 0\n\n    def stop(self):\n        \"Indicates that the tests should be aborted\"\n        self.shouldStop = True\n\n    def _exc_info_to_string(self, err, test):\n        \"\"\"Converts a sys.exc_info()-style tuple of values into a string.\"\"\"\n        exctype, value, tb = err\n        # Skip test runner traceback levels\n        while tb and self._is_relevant_tb_level(tb):\n            tb = tb.tb_next\n\n        if exctype is test.failureException:\n            # Skip assert*() traceback levels\n            length = self._count_relevant_tb_levels(tb)\n            msgLines = traceback.format_exception(exctype, value, tb, length)\n        else:\n            msgLines = traceback.format_exception(exctype, value, tb)\n\n        if self.buffer:\n            output = sys.stdout.getvalue()\n            error = sys.stderr.getvalue()\n            if output:\n                if not output.endswith('\\n'):\n                    output += '\\n'\n                msgLines.append(STDOUT_LINE % output)\n            if error:\n                if not error.endswith('\\n'):\n                    error += '\\n'\n                msgLines.append(STDERR_LINE % error)\n        return ''.join(msgLines)\n\n\n    def _is_relevant_tb_level(self, tb):\n        return '__unittest' in tb.tb_frame.f_globals\n\n    def _count_relevant_tb_levels(self, tb):\n        length = 0\n        while tb and not self._is_relevant_tb_level(tb):\n            length += 1\n            tb = tb.tb_next\n        return length\n\n    def __repr__(self):\n        return (\"<%s run=%i errors=%i failures=%i>\" %\n               (util.strclass(self.__class__), self.testsRun, len(self.errors),\n                len(self.failures)))\n",
		"file_name": "unittest/result.py"
	},
	{
		"content": "\"\"\"Various utility functions.\"\"\"\nfrom collections import namedtuple, OrderedDict\n\n\n__unittest = True\n\n_MAX_LENGTH = 80\ndef safe_repr(obj, short=False):\n    try:\n        result = repr(obj)\n    except Exception:\n        result = object.__repr__(obj)\n    if not short or len(result) < _MAX_LENGTH:\n        return result\n    return result[:_MAX_LENGTH] + ' [truncated]...'\n\n\ndef strclass(cls):\n    return \"%s.%s\" % (cls.__module__, cls.__name__)\n\ndef sorted_list_difference(expected, actual):\n    \"\"\"Finds elements in only one or the other of two, sorted input lists.\n\n    Returns a two-element tuple of lists.    The first list contains those\n    elements in the \"expected\" list but not in the \"actual\" list, and the\n    second contains those elements in the \"actual\" list but not in the\n    \"expected\" list.    Duplicate elements in either input list are ignored.\n    \"\"\"\n    i = j = 0\n    missing = []\n    unexpected = []\n    while True:\n        try:\n            e = expected[i]\n            a = actual[j]\n            if e < a:\n                missing.append(e)\n                i += 1\n                while expected[i] == e:\n                    i += 1\n            elif e > a:\n                unexpected.append(a)\n                j += 1\n                while actual[j] == a:\n                    j += 1\n            else:\n                i += 1\n                try:\n                    while expected[i] == e:\n                        i += 1\n                finally:\n                    j += 1\n                    while actual[j] == a:\n                        j += 1\n        except IndexError:\n            missing.extend(expected[i:])\n            unexpected.extend(actual[j:])\n            break\n    return missing, unexpected\n\n\ndef unorderable_list_difference(expected, actual, ignore_duplicate=False):\n    \"\"\"Same behavior as sorted_list_difference but\n    for lists of unorderable items (like dicts).\n\n    As it does a linear search per item (remove) it\n    has O(n*n) performance.\n    \"\"\"\n    missing = []\n    unexpected = []\n    while expected:\n        item = expected.pop()\n        try:\n            actual.remove(item)\n        except ValueError:\n            missing.append(item)\n        if ignore_duplicate:\n            for lst in expected, actual:\n                try:\n                    while True:\n                        lst.remove(item)\n                except ValueError:\n                    pass\n    if ignore_duplicate:\n        while actual:\n            item = actual.pop()\n            unexpected.append(item)\n            try:\n                while True:\n                    actual.remove(item)\n            except ValueError:\n                pass\n        return missing, unexpected\n\n    # anything left in actual is unexpected\n    return missing, actual\n\n_Mismatch = namedtuple('Mismatch', 'actual expected value')\n\ndef _count_diff_all_purpose(actual, expected):\n    'Returns list of (cnt_act, cnt_exp, elem) triples where the counts differ'\n    # elements need not be hashable\n    s, t = list(actual), list(expected)\n    m, n = len(s), len(t)\n    NULL = object()\n    result = []\n    for i, elem in enumerate(s):\n        if elem is NULL:\n            continue\n        cnt_s = cnt_t = 0\n        for j in range(i, m):\n            if s[j] == elem:\n                cnt_s += 1\n                s[j] = NULL\n        for j, other_elem in enumerate(t):\n            if other_elem == elem:\n                cnt_t += 1\n                t[j] = NULL\n        if cnt_s != cnt_t:\n            diff = _Mismatch(cnt_s, cnt_t, elem)\n            result.append(diff)\n\n    for i, elem in enumerate(t):\n        if elem is NULL:\n            continue\n        cnt_t = 0\n        for j in range(i, n):\n            if t[j] == elem:\n                cnt_t += 1\n                t[j] = NULL\n        diff = _Mismatch(0, cnt_t, elem)\n        result.append(diff)\n    return result\n\ndef _ordered_count(iterable):\n    'Return dict of element counts, in the order they were first seen'\n    c = OrderedDict()\n    for elem in iterable:\n        c[elem] = c.get(elem, 0) + 1\n    return c\n\ndef _count_diff_hashable(actual, expected):\n    'Returns list of (cnt_act, cnt_exp, elem) triples where the counts differ'\n    # elements must be hashable\n    s, t = _ordered_count(actual), _ordered_count(expected)\n    result = []\n    for elem, cnt_s in s.items():\n        cnt_t = t.get(elem, 0)\n        if cnt_s != cnt_t:\n            diff = _Mismatch(cnt_s, cnt_t, elem)\n            result.append(diff)\n    for elem, cnt_t in t.items():\n        if elem not in s:\n            diff = _Mismatch(0, cnt_t, elem)\n            result.append(diff)\n    return result\n",
		"file_name": "unittest/util.py"
	},
	{
		"content": "\"\"\"Loading unittests.\"\"\"\n\nimport os\nimport re\nimport sys\nimport traceback\nimport types\n\nfrom functools import cmp_to_key as _CmpToKey\nfrom fnmatch import fnmatch\n\nfrom . import case, suite\n\n__unittest = True\n\n# what about .pyc or .pyo (etc)\n# we would need to avoid loading the same tests multiple times\n# from '.py', '.pyc' *and* '.pyo'\nVALID_MODULE_NAME = re.compile(r'[_a-z]\\w*\\.py$', re.IGNORECASE)\n\n\ndef _make_failed_import_test(name, suiteClass):\n    message = 'Failed to import test module: %s\\n%s' % (name, traceback.format_exc())\n    return _make_failed_test('ModuleImportFailure', name, ImportError(message),\n                             suiteClass)\n\ndef _make_failed_load_tests(name, exception, suiteClass):\n    return _make_failed_test('LoadTestsFailure', name, exception, suiteClass)\n\ndef _make_failed_test(classname, methodname, exception, suiteClass):\n    def testFailure(self):\n        raise exception\n    attrs = {methodname: testFailure}\n    TestClass = type(classname, (case.TestCase,), attrs)\n    return suiteClass((TestClass(methodname),))\n\n\nclass TestLoader(object):\n    \"\"\"\n    This class is responsible for loading tests according to various criteria\n    and returning them wrapped in a TestSuite\n    \"\"\"\n    testMethodPrefix = 'test'\n    sortTestMethodsUsing = cmp\n    suiteClass = suite.TestSuite\n    _top_level_dir = None\n\n    def loadTestsFromTestCase(self, testCaseClass):\n        \"\"\"Return a suite of all tests cases contained in testCaseClass\"\"\"\n        if issubclass(testCaseClass, suite.TestSuite):\n            raise TypeError(\"Test cases should not be derived from TestSuite.\" \\\n                                \" Maybe you meant to derive from TestCase?\")\n        testCaseNames = self.getTestCaseNames(testCaseClass)\n        if not testCaseNames and hasattr(testCaseClass, 'runTest'):\n            testCaseNames = ['runTest']\n        loaded_suite = self.suiteClass(map(testCaseClass, testCaseNames))\n        return loaded_suite\n\n    def loadTestsFromModule(self, module, use_load_tests=True):\n        \"\"\"Return a suite of all tests cases contained in the given module\"\"\"\n        tests = []\n        for name in dir(module):\n            obj = getattr(module, name)\n            if isinstance(obj, type) and issubclass(obj, case.TestCase):\n                tests.append(self.loadTestsFromTestCase(obj))\n\n        load_tests = getattr(module, 'load_tests', None)\n        tests = self.suiteClass(tests)\n        if use_load_tests and load_tests is not None:\n            try:\n                return load_tests(self, tests, None)\n            except Exception, e:\n                return _make_failed_load_tests(module.__name__, e,\n                                               self.suiteClass)\n        return tests\n\n    def loadTestsFromName(self, name, module=None):\n        \"\"\"Return a suite of all tests cases given a string specifier.\n\n        The name may resolve either to a module, a test case class, a\n        test method within a test case class, or a callable object which\n        returns a TestCase or TestSuite instance.\n\n        The method optionally resolves the names relative to a given module.\n        \"\"\"\n        parts = name.split('.')\n        if module is None:\n            parts_copy = parts[:]\n            while parts_copy:\n                try:\n                    module = __import__('.'.join(parts_copy))\n                    break\n                except ImportError:\n                    del parts_copy[-1]\n                    if not parts_copy:\n                        raise\n            parts = parts[1:]\n        obj = module\n        for part in parts:\n            parent, obj = obj, getattr(obj, part)\n\n        if isinstance(obj, types.ModuleType):\n            return self.loadTestsFromModule(obj)\n        elif isinstance(obj, type) and issubclass(obj, case.TestCase):\n            return self.loadTestsFromTestCase(obj)\n        elif (isinstance(obj, types.UnboundMethodType) and\n              isinstance(parent, type) and\n              issubclass(parent, case.TestCase)):\n            name = parts[-1]\n            inst = parent(name)\n            return self.suiteClass([inst])\n        elif isinstance(obj, suite.TestSuite):\n            return obj\n        elif hasattr(obj, '__call__'):\n            test = obj()\n            if isinstance(test, suite.TestSuite):\n                return test\n            elif isinstance(test, case.TestCase):\n                return self.suiteClass([test])\n            else:\n                raise TypeError(\"calling %s returned %s, not a test\" %\n                                (obj, test))\n        else:\n            raise TypeError(\"don't know how to make test from: %s\" % obj)\n\n    def loadTestsFromNames(self, names, module=None):\n        \"\"\"Return a suite of all tests cases found using the given sequence\n        of string specifiers. See 'loadTestsFromName()'.\n        \"\"\"\n        suites = [self.loadTestsFromName(name, module) for name in names]\n        return self.suiteClass(suites)\n\n    def getTestCaseNames(self, testCaseClass):\n        \"\"\"Return a sorted sequence of method names found within testCaseClass\n        \"\"\"\n        def isTestMethod(attrname, testCaseClass=testCaseClass,\n                         prefix=self.testMethodPrefix):\n            return attrname.startswith(prefix) and \\\n                hasattr(getattr(testCaseClass, attrname), '__call__')\n        testFnNames = filter(isTestMethod, dir(testCaseClass))\n        if self.sortTestMethodsUsing:\n            testFnNames.sort(key=_CmpToKey(self.sortTestMethodsUsing))\n        return testFnNames\n\n    def discover(self, start_dir, pattern='test*.py', top_level_dir=None):\n        \"\"\"Find and return all test modules from the specified start\n        directory, recursing into subdirectories to find them. Only test files\n        that match the pattern will be loaded. (Using shell style pattern\n        matching.)\n\n        All test modules must be importable from the top level of the project.\n        If the start directory is not the top level directory then the top\n        level directory must be specified separately.\n\n        If a test package name (directory with '__init__.py') matches the\n        pattern then the package will be checked for a 'load_tests' function. If\n        this exists then it will be called with loader, tests, pattern.\n\n        If load_tests exists then discovery does  *not* recurse into the package,\n        load_tests is responsible for loading all tests in the package.\n\n        The pattern is deliberately not stored as a loader attribute so that\n        packages can continue discovery themselves. top_level_dir is stored so\n        load_tests does not need to pass this argument in to loader.discover().\n        \"\"\"\n        set_implicit_top = False\n        if top_level_dir is None and self._top_level_dir is not None:\n            # make top_level_dir optional if called from load_tests in a package\n            top_level_dir = self._top_level_dir\n        elif top_level_dir is None:\n            set_implicit_top = True\n            top_level_dir = start_dir\n\n        top_level_dir = os.path.abspath(top_level_dir)\n\n        if not top_level_dir in sys.path:\n            # all test modules must be importable from the top level directory\n            # should we *unconditionally* put the start directory in first\n            # in sys.path to minimise likelihood of conflicts between installed\n            # modules and development versions?\n            sys.path.insert(0, top_level_dir)\n        self._top_level_dir = top_level_dir\n\n        is_not_importable = False\n        if os.path.isdir(os.path.abspath(start_dir)):\n            start_dir = os.path.abspath(start_dir)\n            if start_dir != top_level_dir:\n                is_not_importable = not os.path.isfile(os.path.join(start_dir, '__init__.py'))\n        else:\n            # support for discovery from dotted module names\n            try:\n                __import__(start_dir)\n            except ImportError:\n                is_not_importable = True\n            else:\n                the_module = sys.modules[start_dir]\n                top_part = start_dir.split('.')[0]\n                start_dir = os.path.abspath(os.path.dirname((the_module.__file__)))\n                if set_implicit_top:\n                    self._top_level_dir = self._get_directory_containing_module(top_part)\n                    sys.path.remove(top_level_dir)\n\n        if is_not_importable:\n            raise ImportError('Start directory is not importable: %r' % start_dir)\n\n        tests = list(self._find_tests(start_dir, pattern))\n        return self.suiteClass(tests)\n\n    def _get_directory_containing_module(self, module_name):\n        module = sys.modules[module_name]\n        full_path = os.path.abspath(module.__file__)\n\n        if os.path.basename(full_path).lower().startswith('__init__.py'):\n            return os.path.dirname(os.path.dirname(full_path))\n        else:\n            # here we have been given a module rather than a package - so\n            # all we can do is search the *same* directory the module is in\n            # should an exception be raised instead\n            return os.path.dirname(full_path)\n\n    def _get_name_from_path(self, path):\n        path = os.path.splitext(os.path.normpath(path))[0]\n\n        _relpath = os.path.relpath(path, self._top_level_dir)\n        assert not os.path.isabs(_relpath), \"Path must be within the project\"\n        assert not _relpath.startswith('..'), \"Path must be within the project\"\n\n        name = _relpath.replace(os.path.sep, '.')\n        return name\n\n    def _get_module_from_name(self, name):\n        __import__(name)\n        return sys.modules[name]\n\n    def _match_path(self, path, full_path, pattern):\n        # override this method to use alternative matching strategy\n        return fnmatch(path, pattern)\n\n    def _find_tests(self, start_dir, pattern):\n        \"\"\"Used by discovery. Yields test suites it loads.\"\"\"\n        paths = os.listdir(start_dir)\n\n        for path in paths:\n            full_path = os.path.join(start_dir, path)\n            if os.path.isfile(full_path):\n                if not VALID_MODULE_NAME.match(path):\n                    # valid Python identifiers only\n                    continue\n                if not self._match_path(path, full_path, pattern):\n                    continue\n                # if the test file matches, load it\n                name = self._get_name_from_path(full_path)\n                try:\n                    module = self._get_module_from_name(name)\n                except:\n                    yield _make_failed_import_test(name, self.suiteClass)\n                else:\n                    mod_file = os.path.abspath(getattr(module, '__file__', full_path))\n                    realpath = os.path.splitext(os.path.realpath(mod_file))[0]\n                    fullpath_noext = os.path.splitext(os.path.realpath(full_path))[0]\n                    if realpath.lower() != fullpath_noext.lower():\n                        module_dir = os.path.dirname(realpath)\n                        mod_name = os.path.splitext(os.path.basename(full_path))[0]\n                        expected_dir = os.path.dirname(full_path)\n                        msg = (\"%r module incorrectly imported from %r. Expected %r. \"\n                               \"Is this module globally installed?\")\n                        raise ImportError(msg % (mod_name, module_dir, expected_dir))\n                    yield self.loadTestsFromModule(module)\n            elif os.path.isdir(full_path):\n                if not os.path.isfile(os.path.join(full_path, '__init__.py')):\n                    continue\n\n                load_tests = None\n                tests = None\n                if fnmatch(path, pattern):\n                    # only check load_tests if the package directory itself matches the filter\n                    name = self._get_name_from_path(full_path)\n                    package = self._get_module_from_name(name)\n                    load_tests = getattr(package, 'load_tests', None)\n                    tests = self.loadTestsFromModule(package, use_load_tests=False)\n\n                if load_tests is None:\n                    if tests is not None:\n                        # tests loaded from package file\n                        yield tests\n                    # recurse into the package\n                    for test in self._find_tests(full_path, pattern):\n                        yield test\n                else:\n                    try:\n                        yield load_tests(self, tests, pattern)\n                    except Exception, e:\n                        yield _make_failed_load_tests(package.__name__, e,\n                                                      self.suiteClass)\n\ndefaultTestLoader = TestLoader()\n\n\ndef _makeLoader(prefix, sortUsing, suiteClass=None):\n    loader = TestLoader()\n    loader.sortTestMethodsUsing = sortUsing\n    loader.testMethodPrefix = prefix\n    if suiteClass:\n        loader.suiteClass = suiteClass\n    return loader\n\ndef getTestCaseNames(testCaseClass, prefix, sortUsing=cmp):\n    return _makeLoader(prefix, sortUsing).getTestCaseNames(testCaseClass)\n\ndef makeSuite(testCaseClass, prefix='test', sortUsing=cmp,\n              suiteClass=suite.TestSuite):\n    return _makeLoader(prefix, sortUsing, suiteClass).loadTestsFromTestCase(testCaseClass)\n\ndef findTestCases(module, prefix='test', sortUsing=cmp,\n                  suiteClass=suite.TestSuite):\n    return _makeLoader(prefix, sortUsing, suiteClass).loadTestsFromModule(module)\n",
		"file_name": "unittest/loader.py"
	},
	{
		"content": "\"\"\"TestSuite\"\"\"\n\nimport sys\n\nfrom . import case\nfrom . import util\n\n__unittest = True\n\n\ndef _call_if_exists(parent, attr):\n    func = getattr(parent, attr, lambda: None)\n    func()\n\n\nclass BaseTestSuite(object):\n    \"\"\"A simple test suite that doesn't provide class or module shared fixtures.\n    \"\"\"\n    def __init__(self, tests=()):\n        self._tests = []\n        self.addTests(tests)\n\n    def __repr__(self):\n        return \"<%s tests=%s>\" % (util.strclass(self.__class__), list(self))\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return list(self) == list(other)\n\n    def __ne__(self, other):\n        return not self == other\n\n    # Can't guarantee hash invariant, so flag as unhashable\n    __hash__ = None\n\n    def __iter__(self):\n        return iter(self._tests)\n\n    def countTestCases(self):\n        cases = 0\n        for test in self:\n            cases += test.countTestCases()\n        return cases\n\n    def addTest(self, test):\n        # sanity checks\n        if not hasattr(test, '__call__'):\n            raise TypeError(\"{} is not callable\".format(repr(test)))\n        if isinstance(test, type) and issubclass(test,\n                                                 (case.TestCase, TestSuite)):\n            raise TypeError(\"TestCases and TestSuites must be instantiated \"\n                            \"before passing them to addTest()\")\n        self._tests.append(test)\n\n    def addTests(self, tests):\n        if isinstance(tests, basestring):\n            raise TypeError(\"tests must be an iterable of tests, not a string\")\n        for test in tests:\n            self.addTest(test)\n\n    def run(self, result):\n        for test in self:\n            if result.shouldStop:\n                break\n            test(result)\n        return result\n\n    def __call__(self, *args, **kwds):\n        return self.run(*args, **kwds)\n\n    def debug(self):\n        \"\"\"Run the tests without collecting errors in a TestResult\"\"\"\n        for test in self:\n            test.debug()\n\n\nclass TestSuite(BaseTestSuite):\n    \"\"\"A test suite is a composite test consisting of a number of TestCases.\n\n    For use, create an instance of TestSuite, then add test case instances.\n    When all tests have been added, the suite can be passed to a test\n    runner, such as TextTestRunner. It will run the individual test cases\n    in the order in which they were added, aggregating the results. When\n    subclassing, do not forget to call the base class constructor.\n    \"\"\"\n\n    def run(self, result, debug=False):\n        topLevel = False\n        if getattr(result, '_testRunEntered', False) is False:\n            result._testRunEntered = topLevel = True\n\n        for test in self:\n            if result.shouldStop:\n                break\n\n            if _isnotsuite(test):\n                self._tearDownPreviousClass(test, result)\n                self._handleModuleFixture(test, result)\n                self._handleClassSetUp(test, result)\n                result._previousTestClass = test.__class__\n\n                if (getattr(test.__class__, '_classSetupFailed', False) or\n                    getattr(result, '_moduleSetUpFailed', False)):\n                    continue\n\n            if not debug:\n                test(result)\n            else:\n                test.debug()\n\n        if topLevel:\n            self._tearDownPreviousClass(None, result)\n            self._handleModuleTearDown(result)\n            result._testRunEntered = False\n        return result\n\n    def debug(self):\n        \"\"\"Run the tests without collecting errors in a TestResult\"\"\"\n        debug = _DebugResult()\n        self.run(debug, True)\n\n    ################################\n\n    def _handleClassSetUp(self, test, result):\n        previousClass = getattr(result, '_previousTestClass', None)\n        currentClass = test.__class__\n        if currentClass == previousClass:\n            return\n        if result._moduleSetUpFailed:\n            return\n        if getattr(currentClass, \"__unittest_skip__\", False):\n            return\n\n        try:\n            currentClass._classSetupFailed = False\n        except TypeError:\n            # test may actually be a function\n            # so its class will be a builtin-type\n            pass\n\n        setUpClass = getattr(currentClass, 'setUpClass', None)\n        if setUpClass is not None:\n            _call_if_exists(result, '_setupStdout')\n            try:\n                setUpClass()\n            except Exception as e:\n                if isinstance(result, _DebugResult):\n                    raise\n                currentClass._classSetupFailed = True\n                className = util.strclass(currentClass)\n                errorName = 'setUpClass (%s)' % className\n                self._addClassOrModuleLevelException(result, e, errorName)\n            finally:\n                _call_if_exists(result, '_restoreStdout')\n\n    def _get_previous_module(self, result):\n        previousModule = None\n        previousClass = getattr(result, '_previousTestClass', None)\n        if previousClass is not None:\n            previousModule = previousClass.__module__\n        return previousModule\n\n\n    def _handleModuleFixture(self, test, result):\n        previousModule = self._get_previous_module(result)\n        currentModule = test.__class__.__module__\n        if currentModule == previousModule:\n            return\n\n        self._handleModuleTearDown(result)\n\n        result._moduleSetUpFailed = False\n        try:\n            module = sys.modules[currentModule]\n        except KeyError:\n            return\n        setUpModule = getattr(module, 'setUpModule', None)\n        if setUpModule is not None:\n            _call_if_exists(result, '_setupStdout')\n            try:\n                setUpModule()\n            except Exception, e:\n                if isinstance(result, _DebugResult):\n                    raise\n                result._moduleSetUpFailed = True\n                errorName = 'setUpModule (%s)' % currentModule\n                self._addClassOrModuleLevelException(result, e, errorName)\n            finally:\n                _call_if_exists(result, '_restoreStdout')\n\n    def _addClassOrModuleLevelException(self, result, exception, errorName):\n        error = _ErrorHolder(errorName)\n        addSkip = getattr(result, 'addSkip', None)\n        if addSkip is not None and isinstance(exception, case.SkipTest):\n            addSkip(error, str(exception))\n        else:\n            result.addError(error, sys.exc_info())\n\n    def _handleModuleTearDown(self, result):\n        previousModule = self._get_previous_module(result)\n        if previousModule is None:\n            return\n        if result._moduleSetUpFailed:\n            return\n\n        try:\n            module = sys.modules[previousModule]\n        except KeyError:\n            return\n\n        tearDownModule = getattr(module, 'tearDownModule', None)\n        if tearDownModule is not None:\n            _call_if_exists(result, '_setupStdout')\n            try:\n                tearDownModule()\n            except Exception as e:\n                if isinstance(result, _DebugResult):\n                    raise\n                errorName = 'tearDownModule (%s)' % previousModule\n                self._addClassOrModuleLevelException(result, e, errorName)\n            finally:\n                _call_if_exists(result, '_restoreStdout')\n\n    def _tearDownPreviousClass(self, test, result):\n        previousClass = getattr(result, '_previousTestClass', None)\n        currentClass = test.__class__\n        if currentClass == previousClass:\n            return\n        if getattr(previousClass, '_classSetupFailed', False):\n            return\n        if getattr(result, '_moduleSetUpFailed', False):\n            return\n        if getattr(previousClass, \"__unittest_skip__\", False):\n            return\n\n        tearDownClass = getattr(previousClass, 'tearDownClass', None)\n        if tearDownClass is not None:\n            _call_if_exists(result, '_setupStdout')\n            try:\n                tearDownClass()\n            except Exception, e:\n                if isinstance(result, _DebugResult):\n                    raise\n                className = util.strclass(previousClass)\n                errorName = 'tearDownClass (%s)' % className\n                self._addClassOrModuleLevelException(result, e, errorName)\n            finally:\n                _call_if_exists(result, '_restoreStdout')\n\n\nclass _ErrorHolder(object):\n    \"\"\"\n    Placeholder for a TestCase inside a result. As far as a TestResult\n    is concerned, this looks exactly like a unit test. Used to insert\n    arbitrary errors into a test suite run.\n    \"\"\"\n    # Inspired by the ErrorHolder from Twisted:\n    # http://twistedmatrix.com/trac/browser/trunk/twisted/trial/runner.py\n\n    # attribute used by TestResult._exc_info_to_string\n    failureException = None\n\n    def __init__(self, description):\n        self.description = description\n\n    def id(self):\n        return self.description\n\n    def shortDescription(self):\n        return None\n\n    def __repr__(self):\n        return \"<ErrorHolder description=%r>\" % (self.description,)\n\n    def __str__(self):\n        return self.id()\n\n    def run(self, result):\n        # could call result.addError(...) - but this test-like object\n        # shouldn't be run anyway\n        pass\n\n    def __call__(self, result):\n        return self.run(result)\n\n    def countTestCases(self):\n        return 0\n\ndef _isnotsuite(test):\n    \"A crude way to tell apart testcases and suites with duck-typing\"\n    try:\n        iter(test)\n    except TypeError:\n        return True\n    return False\n\n\nclass _DebugResult(object):\n    \"Used by the TestSuite to hold previous class when running in debug.\"\n    _previousTestClass = None\n    _moduleSetUpFailed = False\n    shouldStop = False\n",
		"file_name": "unittest/suite.py"
	},
	{
		"content": "\"\"\"Unittest main program\"\"\"\n\nimport sys\nimport os\nimport types\n\nfrom . import loader, runner\nfrom .signals import installHandler\n\n__unittest = True\n\nFAILFAST     = \"  -f, --failfast   Stop on first failure\\n\"\nCATCHBREAK   = \"  -c, --catch      Catch control-C and display results\\n\"\nBUFFEROUTPUT = \"  -b, --buffer     Buffer stdout and stderr during test runs\\n\"\n\nUSAGE_AS_MAIN = \"\"\"\\\nUsage: %(progName)s [options] [tests]\n\nOptions:\n  -h, --help       Show this message\n  -v, --verbose    Verbose output\n  -q, --quiet      Minimal output\n%(failfast)s%(catchbreak)s%(buffer)s\nExamples:\n  %(progName)s test_module               - run tests from test_module\n  %(progName)s module.TestClass          - run tests from module.TestClass\n  %(progName)s module.Class.test_method  - run specified test method\n\n[tests] can be a list of any number of test modules, classes and test\nmethods.\n\nAlternative Usage: %(progName)s discover [options]\n\nOptions:\n  -v, --verbose    Verbose output\n%(failfast)s%(catchbreak)s%(buffer)s  -s directory     Directory to start discovery ('.' default)\n  -p pattern       Pattern to match test files ('test*.py' default)\n  -t directory     Top level directory of project (default to\n                   start directory)\n\nFor test discovery all test modules must be importable from the top\nlevel directory of the project.\n\"\"\"\n\nUSAGE_FROM_MODULE = \"\"\"\\\nUsage: %(progName)s [options] [test] [...]\n\nOptions:\n  -h, --help       Show this message\n  -v, --verbose    Verbose output\n  -q, --quiet      Minimal output\n%(failfast)s%(catchbreak)s%(buffer)s\nExamples:\n  %(progName)s                               - run default set of tests\n  %(progName)s MyTestSuite                   - run suite 'MyTestSuite'\n  %(progName)s MyTestCase.testSomething      - run MyTestCase.testSomething\n  %(progName)s MyTestCase                    - run all 'test*' test methods\n                                               in MyTestCase\n\"\"\"\n\n\n\nclass TestProgram(object):\n    \"\"\"A command-line program that runs a set of tests; this is primarily\n       for making test modules conveniently executable.\n    \"\"\"\n    USAGE = USAGE_FROM_MODULE\n\n    # defaults for testing\n    failfast = catchbreak = buffer = progName = None\n\n    def __init__(self, module='__main__', defaultTest=None, argv=None,\n                    testRunner=None, testLoader=loader.defaultTestLoader,\n                    exit=True, verbosity=1, failfast=None, catchbreak=None,\n                    buffer=None):\n        if isinstance(module, basestring):\n            self.module = __import__(module)\n            for part in module.split('.')[1:]:\n                self.module = getattr(self.module, part)\n        else:\n            self.module = module\n        if argv is None:\n            argv = sys.argv\n\n        self.exit = exit\n        self.failfast = failfast\n        self.catchbreak = catchbreak\n        self.verbosity = verbosity\n        self.buffer = buffer\n        self.defaultTest = defaultTest\n        self.testRunner = testRunner\n        self.testLoader = testLoader\n        self.progName = os.path.basename(argv[0])\n        self.parseArgs(argv)\n        self.runTests()\n\n    def usageExit(self, msg=None):\n        if msg:\n            print msg\n        usage = {'progName': self.progName, 'catchbreak': '', 'failfast': '',\n                 'buffer': ''}\n        if self.failfast != False:\n            usage['failfast'] = FAILFAST\n        if self.catchbreak != False:\n            usage['catchbreak'] = CATCHBREAK\n        if self.buffer != False:\n            usage['buffer'] = BUFFEROUTPUT\n        print self.USAGE % usage\n        sys.exit(2)\n\n    def parseArgs(self, argv):\n        if len(argv) > 1 and argv[1].lower() == 'discover':\n            self._do_discovery(argv[2:])\n            return\n\n        import getopt\n        long_opts = ['help', 'verbose', 'quiet', 'failfast', 'catch', 'buffer']\n        try:\n            options, args = getopt.getopt(argv[1:], 'hHvqfcb', long_opts)\n            for opt, value in options:\n                if opt in ('-h','-H','--help'):\n                    self.usageExit()\n                if opt in ('-q','--quiet'):\n                    self.verbosity = 0\n                if opt in ('-v','--verbose'):\n                    self.verbosity = 2\n                if opt in ('-f','--failfast'):\n                    if self.failfast is None:\n                        self.failfast = True\n                    # Should this raise an exception if -f is not valid?\n                if opt in ('-c','--catch'):\n                    if self.catchbreak is None:\n                        self.catchbreak = True\n                    # Should this raise an exception if -c is not valid?\n                if opt in ('-b','--buffer'):\n                    if self.buffer is None:\n                        self.buffer = True\n                    # Should this raise an exception if -b is not valid?\n            if len(args) == 0 and self.defaultTest is None:\n                # createTests will load tests from self.module\n                self.testNames = None\n            elif len(args) > 0:\n                self.testNames = args\n                if __name__ == '__main__':\n                    # to support python -m unittest ...\n                    self.module = None\n            else:\n                self.testNames = (self.defaultTest,)\n            self.createTests()\n        except getopt.error, msg:\n            self.usageExit(msg)\n\n    def createTests(self):\n        if self.testNames is None:\n            self.test = self.testLoader.loadTestsFromModule(self.module)\n        else:\n            self.test = self.testLoader.loadTestsFromNames(self.testNames,\n                                                           self.module)\n\n    def _do_discovery(self, argv, Loader=None):\n        if Loader is None:\n            Loader = lambda: self.testLoader\n\n        # handle command line args for test discovery\n        self.progName = '%s discover' % self.progName\n        import optparse\n        parser = optparse.OptionParser()\n        parser.prog = self.progName\n        parser.add_option('-v', '--verbose', dest='verbose', default=False,\n                          help='Verbose output', action='store_true')\n        if self.failfast != False:\n            parser.add_option('-f', '--failfast', dest='failfast', default=False,\n                              help='Stop on first fail or error',\n                              action='store_true')\n        if self.catchbreak != False:\n            parser.add_option('-c', '--catch', dest='catchbreak', default=False,\n                              help='Catch ctrl-C and display results so far',\n                              action='store_true')\n        if self.buffer != False:\n            parser.add_option('-b', '--buffer', dest='buffer', default=False,\n                              help='Buffer stdout and stderr during tests',\n                              action='store_true')\n        parser.add_option('-s', '--start-directory', dest='start', default='.',\n                          help=\"Directory to start discovery ('.' default)\")\n        parser.add_option('-p', '--pattern', dest='pattern', default='test*.py',\n                          help=\"Pattern to match tests ('test*.py' default)\")\n        parser.add_option('-t', '--top-level-directory', dest='top', default=None,\n                          help='Top level directory of project (defaults to start directory)')\n\n        options, args = parser.parse_args(argv)\n        if len(args) > 3:\n            self.usageExit()\n\n        for name, value in zip(('start', 'pattern', 'top'), args):\n            setattr(options, name, value)\n\n        # only set options from the parsing here\n        # if they weren't set explicitly in the constructor\n        if self.failfast is None:\n            self.failfast = options.failfast\n        if self.catchbreak is None:\n            self.catchbreak = options.catchbreak\n        if self.buffer is None:\n            self.buffer = options.buffer\n\n        if options.verbose:\n            self.verbosity = 2\n\n        start_dir = options.start\n        pattern = options.pattern\n        top_level_dir = options.top\n\n        loader = Loader()\n        self.test = loader.discover(start_dir, pattern, top_level_dir)\n\n    def runTests(self):\n        if self.catchbreak:\n            installHandler()\n        if self.testRunner is None:\n            self.testRunner = runner.TextTestRunner\n        if isinstance(self.testRunner, (type, types.ClassType)):\n            try:\n                testRunner = self.testRunner(verbosity=self.verbosity,\n                                             failfast=self.failfast,\n                                             buffer=self.buffer)\n            except TypeError:\n                # didn't accept the verbosity, buffer or failfast arguments\n                testRunner = self.testRunner()\n        else:\n            # it is assumed to be a TestRunner instance\n            testRunner = self.testRunner\n        self.result = testRunner.run(self.test)\n        if self.exit:\n            sys.exit(not self.result.wasSuccessful())\n\nmain = TestProgram\n",
		"file_name": "unittest/main.py"
	},
	{
		"content": "\"\"\"Running tests\"\"\"\n\nimport sys\nimport time\n\nfrom . import result\nfrom .signals import registerResult\n\n__unittest = True\n\n\nclass _WritelnDecorator(object):\n    \"\"\"Used to decorate file-like objects with a handy 'writeln' method\"\"\"\n    def __init__(self,stream):\n        self.stream = stream\n\n    def __getattr__(self, attr):\n        if attr in ('stream', '__getstate__'):\n            raise AttributeError(attr)\n        return getattr(self.stream,attr)\n\n    def writeln(self, arg=None):\n        if arg:\n            self.write(arg)\n        self.write('\\n') # text-mode streams translate to \\r\\n if needed\n\n\nclass TextTestResult(result.TestResult):\n    \"\"\"A test result class that can print formatted text results to a stream.\n\n    Used by TextTestRunner.\n    \"\"\"\n    separator1 = '=' * 70\n    separator2 = '-' * 70\n\n    def __init__(self, stream, descriptions, verbosity):\n        super(TextTestResult, self).__init__(stream, descriptions, verbosity)\n        self.stream = stream\n        self.showAll = verbosity > 1\n        self.dots = verbosity == 1\n        self.descriptions = descriptions\n\n    def getDescription(self, test):\n        doc_first_line = test.shortDescription()\n        if self.descriptions and doc_first_line:\n            return '\\n'.join((str(test), doc_first_line))\n        else:\n            return str(test)\n\n    def startTest(self, test):\n        super(TextTestResult, self).startTest(test)\n        if self.showAll:\n            self.stream.write(self.getDescription(test))\n            self.stream.write(\" ... \")\n            self.stream.flush()\n\n    def addSuccess(self, test):\n        super(TextTestResult, self).addSuccess(test)\n        if self.showAll:\n            self.stream.writeln(\"ok\")\n        elif self.dots:\n            self.stream.write('.')\n            self.stream.flush()\n\n    def addError(self, test, err):\n        super(TextTestResult, self).addError(test, err)\n        if self.showAll:\n            self.stream.writeln(\"ERROR\")\n        elif self.dots:\n            self.stream.write('E')\n            self.stream.flush()\n\n    def addFailure(self, test, err):\n        super(TextTestResult, self).addFailure(test, err)\n        if self.showAll:\n            self.stream.writeln(\"FAIL\")\n        elif self.dots:\n            self.stream.write('F')\n            self.stream.flush()\n\n    def addSkip(self, test, reason):\n        super(TextTestResult, self).addSkip(test, reason)\n        if self.showAll:\n            self.stream.writeln(\"skipped {0!r}\".format(reason))\n        elif self.dots:\n            self.stream.write(\"s\")\n            self.stream.flush()\n\n    def addExpectedFailure(self, test, err):\n        super(TextTestResult, self).addExpectedFailure(test, err)\n        if self.showAll:\n            self.stream.writeln(\"expected failure\")\n        elif self.dots:\n            self.stream.write(\"x\")\n            self.stream.flush()\n\n    def addUnexpectedSuccess(self, test):\n        super(TextTestResult, self).addUnexpectedSuccess(test)\n        if self.showAll:\n            self.stream.writeln(\"unexpected success\")\n        elif self.dots:\n            self.stream.write(\"u\")\n            self.stream.flush()\n\n    def printErrors(self):\n        if self.dots or self.showAll:\n            self.stream.writeln()\n        self.printErrorList('ERROR', self.errors)\n        self.printErrorList('FAIL', self.failures)\n\n    def printErrorList(self, flavour, errors):\n        for test, err in errors:\n            self.stream.writeln(self.separator1)\n            self.stream.writeln(\"%s: %s\" % (flavour,self.getDescription(test)))\n            self.stream.writeln(self.separator2)\n            self.stream.writeln(\"%s\" % err)\n\n\nclass TextTestRunner(object):\n    \"\"\"A test runner class that displays results in textual form.\n\n    It prints out the names of tests as they are run, errors as they\n    occur, and a summary of the results at the end of the test run.\n    \"\"\"\n    resultclass = TextTestResult\n\n    def __init__(self, stream=sys.stderr, descriptions=True, verbosity=1,\n                 failfast=False, buffer=False, resultclass=None):\n        self.stream = _WritelnDecorator(stream)\n        self.descriptions = descriptions\n        self.verbosity = verbosity\n        self.failfast = failfast\n        self.buffer = buffer\n        if resultclass is not None:\n            self.resultclass = resultclass\n\n    def _makeResult(self):\n        return self.resultclass(self.stream, self.descriptions, self.verbosity)\n\n    def run(self, test):\n        \"Run the given test case or test suite.\"\n        result = self._makeResult()\n        registerResult(result)\n        result.failfast = self.failfast\n        result.buffer = self.buffer\n        startTime = time.time()\n        startTestRun = getattr(result, 'startTestRun', None)\n        if startTestRun is not None:\n            startTestRun()\n        try:\n            test(result)\n        finally:\n            stopTestRun = getattr(result, 'stopTestRun', None)\n            if stopTestRun is not None:\n                stopTestRun()\n        stopTime = time.time()\n        timeTaken = stopTime - startTime\n        result.printErrors()\n        if hasattr(result, 'separator2'):\n            self.stream.writeln(result.separator2)\n        run = result.testsRun\n        self.stream.writeln(\"Ran %d test%s in %.3fs\" %\n                            (run, run != 1 and \"s\" or \"\", timeTaken))\n        self.stream.writeln()\n\n        expectedFails = unexpectedSuccesses = skipped = 0\n        try:\n            results = map(len, (result.expectedFailures,\n                                result.unexpectedSuccesses,\n                                result.skipped))\n        except AttributeError:\n            pass\n        else:\n            expectedFails, unexpectedSuccesses, skipped = results\n\n        infos = []\n        if not result.wasSuccessful():\n            self.stream.write(\"FAILED\")\n            failed, errored = map(len, (result.failures, result.errors))\n            if failed:\n                infos.append(\"failures=%d\" % failed)\n            if errored:\n                infos.append(\"errors=%d\" % errored)\n        else:\n            self.stream.write(\"OK\")\n        if skipped:\n            infos.append(\"skipped=%d\" % skipped)\n        if expectedFails:\n            infos.append(\"expected failures=%d\" % expectedFails)\n        if unexpectedSuccesses:\n            infos.append(\"unexpected successes=%d\" % unexpectedSuccesses)\n        if infos:\n            self.stream.writeln(\" (%s)\" % (\", \".join(infos),))\n        else:\n            self.stream.write(\"\\n\")\n        return result\n",
		"file_name": "unittest/runner.py"
	},
	{
		"content": "import signal\nimport weakref\n\nfrom functools import wraps\n\n__unittest = True\n\n\nclass _InterruptHandler(object):\n    def __init__(self, default_handler):\n        self.called = False\n        self.original_handler = default_handler\n        if isinstance(default_handler, int):\n            if default_handler == signal.SIG_DFL:\n                # Pretend it's signal.default_int_handler instead.\n                default_handler = signal.default_int_handler\n            elif default_handler == signal.SIG_IGN:\n                # Not quite the same thing as SIG_IGN, but the closest we\n                # can make it: do nothing.\n                def default_handler(unused_signum, unused_frame):\n                    pass\n            else:\n                raise TypeError(\"expected SIGINT signal handler to be \"\n                                \"signal.SIG_IGN, signal.SIG_DFL, or a \"\n                                \"callable object\")\n        self.default_handler = default_handler\n\n    def __call__(self, signum, frame):\n        installed_handler = signal.getsignal(signal.SIGINT)\n        if installed_handler is not self:\n            # if we aren't the installed handler, then delegate immediately\n            # to the default handler\n            self.default_handler(signum, frame)\n\n        if self.called:\n            self.default_handler(signum, frame)\n        self.called = True\n        for result in _results.keys():\n            result.stop()\n\n_results = weakref.WeakKeyDictionary()\ndef registerResult(result):\n    _results[result] = 1\n\ndef removeResult(result):\n    return bool(_results.pop(result, None))\n\n_interrupt_handler = None\ndef installHandler():\n    global _interrupt_handler\n    if _interrupt_handler is None:\n        default_handler = signal.getsignal(signal.SIGINT)\n        _interrupt_handler = _InterruptHandler(default_handler)\n        signal.signal(signal.SIGINT, _interrupt_handler)\n\n\ndef removeHandler(method=None):\n    if method is not None:\n        @wraps(method)\n        def inner(*args, **kwargs):\n            initial = signal.getsignal(signal.SIGINT)\n            removeHandler()\n            try:\n                return method(*args, **kwargs)\n            finally:\n                signal.signal(signal.SIGINT, initial)\n        return inner\n\n    global _interrupt_handler\n    if _interrupt_handler is not None:\n        signal.signal(signal.SIGINT, _interrupt_handler.original_handler)\n",
		"file_name": "unittest/signals.py"
	},
	{
		"content": "#! /usr/bin/env python\n\n\"\"\"Keywords (from \"graminit.c\")\n\nThis file is automatically generated; please don't muck it up!\n\nTo update the symbols in this file, 'cd' to the top directory of\nthe python source tree after building the interpreter and run:\n\n    ./python Lib/keyword.py\n\"\"\"\n\n__all__ = [\"iskeyword\", \"kwlist\"]\n\nkwlist = [\n#--start keywords--\n        'and',\n        'as',\n        'assert',\n        'break',\n        'class',\n        'continue',\n        'def',\n        'del',\n        'elif',\n        'else',\n        'except',\n        'exec',\n        'finally',\n        'for',\n        'from',\n        'global',\n        'if',\n        'import',\n        'in',\n        'is',\n        'lambda',\n        'not',\n        'or',\n        'pass',\n        'print',\n        'raise',\n        'return',\n        'try',\n        'while',\n        'with',\n        'yield',\n#--end keywords--\n        ]\n\niskeyword = frozenset(kwlist).__contains__\n\ndef main():\n    import sys, re\n\n    args = sys.argv[1:]\n    iptfile = args and args[0] or \"Python/graminit.c\"\n    if len(args) > 1: optfile = args[1]\n    else: optfile = \"Lib/keyword.py\"\n\n    # scan the source file for keywords\n    fp = open(iptfile)\n    strprog = re.compile('\"([^\"]+)\"')\n    lines = []\n    for line in fp:\n        if '{1, \"' in line:\n            match = strprog.search(line)\n            if match:\n                lines.append(\"        '\" + match.group(1) + \"',\\n\")\n    fp.close()\n    lines.sort()\n\n    # load the output skeleton from the target\n    fp = open(optfile)\n    format = fp.readlines()\n    fp.close()\n\n    # insert the lines of keywords\n    try:\n        start = format.index(\"#--start keywords--\\n\") + 1\n        end = format.index(\"#--end keywords--\\n\")\n        format[start:end] = lines\n    except ValueError:\n        sys.stderr.write(\"target does not contain format markers\\n\")\n        sys.exit(1)\n\n    # write the output file\n    fp = open(optfile, 'w')\n    fp.write(''.join(format))\n    fp.close()\n\nif __name__ == \"__main__\":\n    main()\n",
		"file_name": "keyword.py"
	},
	{
		"content": "#\n# Reimplementation of cPickle, mostly as a copy of pickle.py\n#\n\nfrom pickle import Pickler, dump, dumps, PickleError, PicklingError, UnpicklingError, _EmptyClass\nfrom pickle import __doc__, __version__, format_version, compatible_formats\nfrom types import *\nfrom copy_reg import dispatch_table\nfrom copy_reg import _extension_registry, _inverted_registry, _extension_cache\nimport marshal, struct, sys\n\ntry: from __pypy__ import builtinify\nexcept ImportError: builtinify = lambda f: f\n\n# These are purely informational; no code uses these.\nformat_version = \"2.0\"                  # File format version we write\ncompatible_formats = [\"1.0\",            # Original protocol 0\n                      \"1.1\",            # Protocol 0 with INST added\n                      \"1.2\",            # Original protocol 1\n                      \"1.3\",            # Protocol 1 with BINFLOAT added\n                      \"2.0\",            # Protocol 2\n                      ]                 # Old format versions we can read\n\n# Keep in synch with cPickle.  This is the highest protocol number we\n# know how to read.\nHIGHEST_PROTOCOL = 2\n\nBadPickleGet = KeyError\nUnpickleableError = PicklingError\n\nMARK            = ord('(')   # push special markobject on stack\nSTOP            = ord('.')   # every pickle ends with STOP\nPOP             = ord('0')   # discard topmost stack item\nPOP_MARK        = ord('1')   # discard stack top through topmost markobject\nDUP             = ord('2')   # duplicate top stack item\nFLOAT           = ord('F')   # push float object; decimal string argument\nINT             = ord('I')   # push integer or bool; decimal string argument\nBININT          = ord('J')   # push four-byte signed int\nBININT1         = ord('K')   # push 1-byte unsigned int\nLONG            = ord('L')   # push long; decimal string argument\nBININT2         = ord('M')   # push 2-byte unsigned int\nNONE            = ord('N')   # push None\nPERSID          = ord('P')   # push persistent object; id is taken from string arg\nBINPERSID       = ord('Q')   #  \"       \"         \"  ;  \"  \"   \"     \"  stack\nREDUCE          = ord('R')   # apply callable to argtuple, both on stack\nSTRING          = ord('S')   # push string; NL-terminated string argument\nBINSTRING       = ord('T')   # push string; counted binary string argument\nSHORT_BINSTRING = ord('U')   #  \"     \"   ;    \"      \"       \"      \" < 256 bytes\nUNICODE         = ord('V')   # push Unicode string; raw-unicode-escaped'd argument\nBINUNICODE      = ord('X')   #   \"     \"       \"  ; counted UTF-8 string argument\nAPPEND          = ord('a')   # append stack top to list below it\nBUILD           = ord('b')   # call __setstate__ or __dict__.update()\nGLOBAL          = ord('c')   # push self.find_class(modname, name); 2 string args\nDICT            = ord('d')   # build a dict from stack items\nEMPTY_DICT      = ord('}')   # push empty dict\nAPPENDS         = ord('e')   # extend list on stack by topmost stack slice\nGET             = ord('g')   # push item from memo on stack; index is string arg\nBINGET          = ord('h')   #   \"    \"    \"    \"   \"   \"  ;   \"    \" 1-byte arg\nINST            = ord('i')   # build & push class instance\nLONG_BINGET     = ord('j')   # push item from memo on stack; index is 4-byte arg\nLIST            = ord('l')   # build list from topmost stack items\nEMPTY_LIST      = ord(']')   # push empty list\nOBJ             = ord('o')   # build & push class instance\nPUT             = ord('p')   # store stack top in memo; index is string arg\nBINPUT          = ord('q')   #   \"     \"    \"   \"   \" ;   \"    \" 1-byte arg\nLONG_BINPUT     = ord('r')   #   \"     \"    \"   \"   \" ;   \"    \" 4-byte arg\nSETITEM         = ord('s')   # add key+value pair to dict\nTUPLE           = ord('t')   # build tuple from topmost stack items\nEMPTY_TUPLE     = ord(')')   # push empty tuple\nSETITEMS        = ord('u')   # modify dict by adding topmost key+value pairs\nBINFLOAT        = ord('G')   # push float; arg is 8-byte float encoding\n\nTRUE            = 'I01\\n'  # not an opcode; see INT docs in pickletools.py\nFALSE           = 'I00\\n'  # not an opcode; see INT docs in pickletools.py\n\n# Protocol 2\n\nPROTO           = ord('\\x80')  # identify pickle protocol\nNEWOBJ          = ord('\\x81')  # build object by applying cls.__new__ to argtuple\nEXT1            = ord('\\x82')  # push object from extension registry; 1-byte index\nEXT2            = ord('\\x83')  # ditto, but 2-byte index\nEXT4            = ord('\\x84')  # ditto, but 4-byte index\nTUPLE1          = ord('\\x85')  # build 1-tuple from stack top\nTUPLE2          = ord('\\x86')  # build 2-tuple from two topmost stack items\nTUPLE3          = ord('\\x87')  # build 3-tuple from three topmost stack items\nNEWTRUE         = ord('\\x88')  # push True\nNEWFALSE        = ord('\\x89')  # push False\nLONG1           = ord('\\x8a')  # push long from < 256 bytes\nLONG4           = ord('\\x8b')  # push really big long\n\n_tuplesize2code = [EMPTY_TUPLE, TUPLE1, TUPLE2, TUPLE3]\n\n\n# ____________________________________________________________\n# XXX some temporary dark magic to produce pickled dumps that are\n#     closer to the ones produced by cPickle in CPython\n\nfrom pickle import StringIO\n\nPythonPickler = Pickler\nclass Pickler(PythonPickler):\n    def __init__(self, *args, **kw):\n        self.__f = None\n        if len(args) == 1 and isinstance(args[0], int):\n            self.__f = StringIO()\n            PythonPickler.__init__(self, self.__f, args[0], **kw)\n        else:\n            PythonPickler.__init__(self, *args, **kw)\n\n    def memoize(self, obj):\n        self.memo[id(None)] = None   # cPickle starts counting at one\n        return PythonPickler.memoize(self, obj)\n\n    def getvalue(self):\n        return self.__f and self.__f.getvalue()\n\n@builtinify\ndef dump(obj, file, protocol=None):\n    Pickler(file, protocol).dump(obj)\n\n@builtinify\ndef dumps(obj, protocol=None):\n    file = StringIO()\n    Pickler(file, protocol).dump(obj)\n    return file.getvalue()\n\n# Why use struct.pack() for pickling but marshal.loads() for\n# unpickling?  struct.pack() is 40% faster than marshal.dumps(), but\n# marshal.loads() is twice as fast as struct.unpack()!\nmloads = marshal.loads\n\n# Unpickling machinery\n\nclass _Stack(list):\n    def pop(self, index=-1):\n        try:\n            return list.pop(self, index)\n        except IndexError:\n            raise UnpicklingError(\"unpickling stack underflow\")\n\nclass Unpickler(object):\n\n    def __init__(self, file):\n        \"\"\"This takes a file-like object for reading a pickle data stream.\n\n        The protocol version of the pickle is detected automatically, so no\n        proto argument is needed.\n\n        The file-like object must have two methods, a read() method that\n        takes an integer argument, and a readline() method that requires no\n        arguments.  Both methods should return a string.  Thus file-like\n        object can be a file object opened for reading, a StringIO object,\n        or any other custom object that meets this interface.\n        \"\"\"\n        self.readline = file.readline\n        self.read = file.read\n        self.memo = {}\n\n    def load(self):\n        \"\"\"Read a pickled object representation from the open file.\n\n        Return the reconstituted object hierarchy specified in the file.\n        \"\"\"\n        self.mark = object() # any new unique object\n        self.stack = _Stack()\n        self.append = self.stack.append\n        try:\n            key = ord(self.read(1))\n            while key != STOP:\n                self.dispatch[key](self)\n                key = ord(self.read(1))\n        except TypeError:\n            if self.read(1) == '':\n                raise EOFError\n            raise\n        return self.stack.pop()\n\n    # Return largest index k such that self.stack[k] is self.mark.\n    # If the stack doesn't contain a mark, eventually raises IndexError.\n    # This could be sped by maintaining another stack, of indices at which\n    # the mark appears.  For that matter, the latter stack would suffice,\n    # and we wouldn't need to push mark objects on self.stack at all.\n    # Doing so is probably a good thing, though, since if the pickle is\n    # corrupt (or hostile) we may get a clue from finding self.mark embedded\n    # in unpickled objects.\n    def marker(self):\n        k = len(self.stack)-1\n        while self.stack[k] is not self.mark: k -= 1\n        return k\n\n    dispatch = {}\n\n    def load_proto(self):\n        proto = ord(self.read(1))\n        if not 0 <= proto <= 2:\n            raise ValueError, \"unsupported pickle protocol: %d\" % proto\n    dispatch[PROTO] = load_proto\n\n    def load_persid(self):\n        pid = self.readline()[:-1]\n        self.append(self.persistent_load(pid))\n    dispatch[PERSID] = load_persid\n\n    def load_binpersid(self):\n        pid = self.stack.pop()\n        self.append(self.persistent_load(pid))\n    dispatch[BINPERSID] = load_binpersid\n\n    def load_none(self):\n        self.append(None)\n    dispatch[NONE] = load_none\n\n    def load_false(self):\n        self.append(False)\n    dispatch[NEWFALSE] = load_false\n\n    def load_true(self):\n        self.append(True)\n    dispatch[NEWTRUE] = load_true\n\n    def load_int(self):\n        data = self.readline()\n        if data == FALSE[1:]:\n            val = False\n        elif data == TRUE[1:]:\n            val = True\n        else:\n            val = int(data)\n        self.append(val)\n    dispatch[INT] = load_int\n\n    def load_binint(self):\n        self.append(mloads('i' + self.read(4)))\n    dispatch[BININT] = load_binint\n\n    def load_binint1(self):\n        self.append(ord(self.read(1)))\n    dispatch[BININT1] = load_binint1\n\n    def load_binint2(self):\n        self.append(mloads('i' + self.read(2) + '\\000\\000'))\n    dispatch[BININT2] = load_binint2\n\n    def load_long(self):\n        self.append(long(self.readline()[:-1], 0))\n    dispatch[LONG] = load_long\n\n    def load_long1(self):\n        n = ord(self.read(1))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG1] = load_long1\n\n    def load_long4(self):\n        n = mloads('i' + self.read(4))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG4] = load_long4\n\n    def load_float(self):\n        self.append(float(self.readline()[:-1]))\n    dispatch[FLOAT] = load_float\n\n    def load_binfloat(self, unpack=struct.unpack):\n        self.append(unpack('>d', self.read(8))[0])\n    dispatch[BINFLOAT] = load_binfloat\n\n    def load_string(self):\n        rep = self.readline()\n        if len(rep) < 3:\n            raise ValueError, \"insecure string pickle\"\n        if rep[0] == \"'\" == rep[-2]:\n            rep = rep[1:-2]\n        elif rep[0] == '\"' == rep[-2]:\n            rep = rep[1:-2]\n        else:\n            raise ValueError, \"insecure string pickle\"\n        self.append(rep.decode(\"string-escape\"))\n    dispatch[STRING] = load_string\n\n    def load_binstring(self):\n        L = mloads('i' + self.read(4))\n        self.append(self.read(L))\n    dispatch[BINSTRING] = load_binstring\n\n    def load_unicode(self):\n        self.append(unicode(self.readline()[:-1],'raw-unicode-escape'))\n    dispatch[UNICODE] = load_unicode\n\n    def load_binunicode(self):\n        L = mloads('i' + self.read(4))\n        self.append(unicode(self.read(L),'utf-8'))\n    dispatch[BINUNICODE] = load_binunicode\n\n    def load_short_binstring(self):\n        L = ord(self.read(1))\n        self.append(self.read(L))\n    dispatch[SHORT_BINSTRING] = load_short_binstring\n\n    def load_tuple(self):\n        k = self.marker()\n        self.stack[k:] = [tuple(self.stack[k+1:])]\n    dispatch[TUPLE] = load_tuple\n\n    def load_empty_tuple(self):\n        self.stack.append(())\n    dispatch[EMPTY_TUPLE] = load_empty_tuple\n\n    def load_tuple1(self):\n        self.stack[-1] = (self.stack[-1],)\n    dispatch[TUPLE1] = load_tuple1\n\n    def load_tuple2(self):\n        self.stack[-2:] = [(self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE2] = load_tuple2\n\n    def load_tuple3(self):\n        self.stack[-3:] = [(self.stack[-3], self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE3] = load_tuple3\n\n    def load_empty_list(self):\n        self.stack.append([])\n    dispatch[EMPTY_LIST] = load_empty_list\n\n    def load_empty_dictionary(self):\n        self.stack.append({})\n    dispatch[EMPTY_DICT] = load_empty_dictionary\n\n    def load_list(self):\n        k = self.marker()\n        self.stack[k:] = [self.stack[k+1:]]\n    dispatch[LIST] = load_list\n\n    def load_dict(self):\n        k = self.marker()\n        d = {}\n        items = self.stack[k+1:]\n        for i in range(0, len(items), 2):\n            key = items[i]\n            value = items[i+1]\n            d[key] = value\n        self.stack[k:] = [d]\n    dispatch[DICT] = load_dict\n\n    # INST and OBJ differ only in how they get a class object.  It's not\n    # only sensible to do the rest in a common routine, the two routines\n    # previously diverged and grew different bugs.\n    # klass is the class to instantiate, and k points to the topmost mark\n    # object, following which are the arguments for klass.__init__.\n    def _instantiate(self, klass, k):\n        args = tuple(self.stack[k+1:])\n        del self.stack[k:]\n        instantiated = 0\n        if (not args and\n                type(klass) is ClassType and\n                not hasattr(klass, \"__getinitargs__\")):\n            try:\n                value = _EmptyClass()\n                value.__class__ = klass\n                instantiated = 1\n            except RuntimeError:\n                # In restricted execution, assignment to inst.__class__ is\n                # prohibited\n                pass\n        if not instantiated:\n            try:\n                value = klass(*args)\n            except TypeError, err:\n                raise TypeError, \"in constructor for %s: %s\" % (\n                    klass.__name__, str(err)), sys.exc_info()[2]\n        self.append(value)\n\n    def load_inst(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self._instantiate(klass, self.marker())\n    dispatch[INST] = load_inst\n\n    def load_obj(self):\n        # Stack is ... markobject classobject arg1 arg2 ...\n        k = self.marker()\n        klass = self.stack.pop(k+1)\n        self._instantiate(klass, k)\n    dispatch[OBJ] = load_obj\n\n    def load_newobj(self):\n        args = self.stack.pop()\n        cls = self.stack[-1]\n        obj = cls.__new__(cls, *args)\n        self.stack[-1] = obj\n    dispatch[NEWOBJ] = load_newobj\n\n    def load_global(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self.append(klass)\n    dispatch[GLOBAL] = load_global\n\n    def load_ext1(self):\n        code = ord(self.read(1))\n        self.get_extension(code)\n    dispatch[EXT1] = load_ext1\n\n    def load_ext2(self):\n        code = mloads('i' + self.read(2) + '\\000\\000')\n        self.get_extension(code)\n    dispatch[EXT2] = load_ext2\n\n    def load_ext4(self):\n        code = mloads('i' + self.read(4))\n        self.get_extension(code)\n    dispatch[EXT4] = load_ext4\n\n    def get_extension(self, code):\n        nil = []\n        obj = _extension_cache.get(code, nil)\n        if obj is not nil:\n            self.append(obj)\n            return\n        key = _inverted_registry.get(code)\n        if not key:\n            raise ValueError(\"unregistered extension code %d\" % code)\n        obj = self.find_class(*key)\n        _extension_cache[code] = obj\n        self.append(obj)\n\n    def find_class(self, module, name):\n        # Subclasses may override this\n        __import__(module)\n        mod = sys.modules[module]\n        klass = getattr(mod, name)\n        return klass\n\n    def load_reduce(self):\n        args = self.stack.pop()\n        func = self.stack[-1]\n        value = self.stack[-1](*args)\n        self.stack[-1] = value\n    dispatch[REDUCE] = load_reduce\n\n    def load_pop(self):\n        del self.stack[-1]\n    dispatch[POP] = load_pop\n\n    def load_pop_mark(self):\n        k = self.marker()\n        del self.stack[k:]\n    dispatch[POP_MARK] = load_pop_mark\n\n    def load_dup(self):\n        self.append(self.stack[-1])\n    dispatch[DUP] = load_dup\n\n    def load_get(self):\n        self.append(self.memo[self.readline()[:-1]])\n    dispatch[GET] = load_get\n\n    def load_binget(self):\n        i = ord(self.read(1))\n        self.append(self.memo[repr(i)])\n    dispatch[BINGET] = load_binget\n\n    def load_long_binget(self):\n        i = mloads('i' + self.read(4))\n        self.append(self.memo[repr(i)])\n    dispatch[LONG_BINGET] = load_long_binget\n\n    def load_put(self):\n        self.memo[self.readline()[:-1]] = self.stack[-1]\n    dispatch[PUT] = load_put\n\n    def load_binput(self):\n        i = ord(self.read(1))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[BINPUT] = load_binput\n\n    def load_long_binput(self):\n        i = mloads('i' + self.read(4))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[LONG_BINPUT] = load_long_binput\n\n    def load_append(self):\n        value = self.stack.pop()\n        self.stack[-1].append(value)\n    dispatch[APPEND] = load_append\n\n    def load_appends(self):\n        stack = self.stack\n        mark = self.marker()\n        lst = stack[mark - 1]\n        lst.extend(stack[mark + 1:])\n        del stack[mark:]\n    dispatch[APPENDS] = load_appends\n\n    def load_setitem(self):\n        stack = self.stack\n        value = stack.pop()\n        key = stack.pop()\n        dict = stack[-1]\n        dict[key] = value\n    dispatch[SETITEM] = load_setitem\n\n    def load_setitems(self):\n        stack = self.stack\n        mark = self.marker()\n        dict = stack[mark - 1]\n        for i in range(mark + 1, len(stack), 2):\n            dict[stack[i]] = stack[i + 1]\n\n        del stack[mark:]\n    dispatch[SETITEMS] = load_setitems\n\n    def load_build(self):\n        stack = self.stack\n        state = stack.pop()\n        inst = stack[-1]\n        setstate = getattr(inst, \"__setstate__\", None)\n        if setstate:\n            setstate(state)\n            return\n        slotstate = None\n        if isinstance(state, tuple) and len(state) == 2:\n            state, slotstate = state\n        if state:\n            try:\n                d = inst.__dict__\n                try:\n                    for k, v in state.iteritems():\n                        d[intern(k)] = v\n                # keys in state don't have to be strings\n                # don't blow up, but don't go out of our way\n                except TypeError:\n                    d.update(state)\n\n            except RuntimeError:\n                # XXX In restricted execution, the instance's __dict__\n                # is not accessible.  Use the old way of unpickling\n                # the instance variables.  This is a semantic\n                # difference when unpickling in restricted\n                # vs. unrestricted modes.\n                # Note, however, that cPickle has never tried to do the\n                # .update() business, and always uses\n                #     PyObject_SetItem(inst.__dict__, key, value) in a\n                # loop over state.items().\n                for k, v in state.items():\n                    setattr(inst, k, v)\n        if slotstate:\n            for k, v in slotstate.items():\n                setattr(inst, k, v)\n    dispatch[BUILD] = load_build\n\n    def load_mark(self):\n        self.append(self.mark)\n    dispatch[MARK] = load_mark\n\n#from pickle import decode_long\n\ndef decode_long(data):\n    r\"\"\"Decode a long from a two's complement little-endian binary string.\n\n    >>> decode_long('')\n    0L\n    >>> decode_long(\"\\xff\\x00\")\n    255L\n    >>> decode_long(\"\\xff\\x7f\")\n    32767L\n    >>> decode_long(\"\\x00\\xff\")\n    -256L\n    >>> decode_long(\"\\x00\\x80\")\n    -32768L\n    >>> decode_long(\"\\x80\")\n    -128L\n    >>> decode_long(\"\\x7f\")\n    127L\n    \"\"\"\n\n    nbytes = len(data)\n    if nbytes == 0:\n        return 0L\n    ind = nbytes - 1\n    while ind and ord(data[ind]) == 0:\n        ind -= 1\n    n = ord(data[ind])\n    while ind:\n        n <<= 8\n        ind -= 1\n        if ord(data[ind]):\n            n += ord(data[ind])\n    if ord(data[nbytes - 1]) >= 128:\n        n -= 1L << (nbytes << 3)\n    return n\n\ndef load(f):\n    return Unpickler(f).load()\n\ndef loads(str):\n    f = StringIO(str)\n    return Unpickler(f).load()\n",
		"file_name": "cPickle.py"
	},
	{
		"content": "# Note that PyPy contains also a built-in module 'marshal' which will\n# hide this one if compiled in.\n\nfrom _marshal import __doc__\nfrom _marshal import *\n",
		"file_name": "marshal.py"
	},
	{
		"content": "\"\"\"Internal Python object serialization\n\nThis module contains functions that can read and write Python values in a binary format. The format is specific to Python, but independent of machine architecture issues (e.g., you can write a Python value to a file on a PC, transport the file to a Sun, and read it back there). Details of the format may change between Python versions.\n\"\"\"\n\n# NOTE: This module is used in the Python3 interpreter, but also by\n# the \"sandboxed\" process.  It must work for Python2 as well.\n\nimport types\n\ntry:\n    intern\nexcept NameError:\n    from sys import intern\n\ntry: from __pypy__ import builtinify\nexcept ImportError: builtinify = lambda f: f\n\n\nTYPE_NULL     = '0'\nTYPE_NONE     = 'N'\nTYPE_FALSE    = 'F'\nTYPE_TRUE     = 'T'\nTYPE_STOPITER = 'S'\nTYPE_ELLIPSIS = '.'\nTYPE_INT      = 'i'\nTYPE_INT64    = 'I'\nTYPE_FLOAT    = 'f'\nTYPE_COMPLEX  = 'x'\nTYPE_LONG     = 'l'\nTYPE_STRING   = 's'\nTYPE_INTERNED = 't'\nTYPE_STRINGREF= 'R'\nTYPE_TUPLE    = '('\nTYPE_LIST     = '['\nTYPE_DICT     = '{'\nTYPE_CODE     = 'c'\nTYPE_UNICODE  = 'u'\nTYPE_UNKNOWN  = '?'\nTYPE_SET      = '<'\nTYPE_FROZENSET= '>'\n\nclass _Marshaller:\n\n    dispatch = {}\n\n    def __init__(self, writefunc):\n        self._write = writefunc\n\n    def dump(self, x):\n        try:\n            self.dispatch[type(x)](self, x)\n        except KeyError:\n            for tp in type(x).mro():\n                func = self.dispatch.get(tp)\n                if func:\n                    break\n            else:\n                raise ValueError(\"unmarshallable object\")\n            func(self, x)\n\n    def w_long64(self, x):\n        self.w_long(x)\n        self.w_long(x>>32)\n\n    def w_long(self, x):\n        a = chr(x & 0xff)\n        x >>= 8\n        b = chr(x & 0xff)\n        x >>= 8\n        c = chr(x & 0xff)\n        x >>= 8\n        d = chr(x & 0xff)\n        self._write(a + b + c + d)\n\n    def w_short(self, x):\n        self._write(chr((x)     & 0xff))\n        self._write(chr((x>> 8) & 0xff))\n\n    def dump_none(self, x):\n        self._write(TYPE_NONE)\n    dispatch[type(None)] = dump_none\n\n    def dump_bool(self, x):\n        if x:\n            self._write(TYPE_TRUE)\n        else:\n            self._write(TYPE_FALSE)\n    dispatch[bool] = dump_bool\n\n    def dump_stopiter(self, x):\n        if x is not StopIteration:\n            raise ValueError(\"unmarshallable object\")\n        self._write(TYPE_STOPITER)\n    dispatch[type(StopIteration)] = dump_stopiter\n\n    def dump_ellipsis(self, x):\n        self._write(TYPE_ELLIPSIS)\n    \n    try:\n        dispatch[type(Ellipsis)] = dump_ellipsis\n    except NameError:\n        pass\n\n    # In Python3, this function is not used; see dump_long() below.\n    def dump_int(self, x):\n        y = x>>31\n        if y and y != -1:\n            self._write(TYPE_INT64)\n            self.w_long64(x)\n        else:\n            self._write(TYPE_INT)\n            self.w_long(x)\n    dispatch[int] = dump_int\n\n    def dump_long(self, x):\n        self._write(TYPE_LONG)\n        sign = 1\n        if x < 0:\n            sign = -1\n            x = -x\n        digits = []\n        while x:\n            digits.append(x & 0x7FFF)\n            x = x>>15\n        self.w_long(len(digits) * sign)\n        for d in digits:\n            self.w_short(d)\n    try:\n        long\n    except NameError:\n        dispatch[int] = dump_long\n    else:\n        dispatch[long] = dump_long\n\n    def dump_float(self, x):\n        write = self._write\n        write(TYPE_FLOAT)\n        s = repr(x)\n        write(chr(len(s)))\n        write(s)\n    dispatch[float] = dump_float\n\n    def dump_complex(self, x):\n        write = self._write\n        write(TYPE_COMPLEX)\n        s = repr(x.real)\n        write(chr(len(s)))\n        write(s)\n        s = repr(x.imag)\n        write(chr(len(s)))\n        write(s)\n    try:\n        dispatch[complex] = dump_complex\n    except NameError:\n        pass\n\n    def dump_string(self, x):\n        # XXX we can't check for interned strings, yet,\n        # so we (for now) never create TYPE_INTERNED or TYPE_STRINGREF\n        self._write(TYPE_STRING)\n        self.w_long(len(x))\n        self._write(x)\n    dispatch[bytes] = dump_string\n\n    def dump_unicode(self, x):\n        self._write(TYPE_UNICODE)\n        s = x.encode('utf8')\n        self.w_long(len(s))\n        self._write(s)\n    try:\n        unicode\n    except NameError:\n        dispatch[str] = dump_unicode\n    else:\n        dispatch[unicode] = dump_unicode\n\n    def dump_tuple(self, x):\n        self._write(TYPE_TUPLE)\n        self.w_long(len(x))\n        for item in x:\n            self.dump(item)\n    dispatch[tuple] = dump_tuple\n\n    def dump_list(self, x):\n        self._write(TYPE_LIST)\n        self.w_long(len(x))\n        for item in x:\n            self.dump(item)\n    dispatch[list] = dump_list\n\n    def dump_dict(self, x):\n        self._write(TYPE_DICT)\n        for key, value in x.items():\n            self.dump(key)\n            self.dump(value)\n        self._write(TYPE_NULL)\n    dispatch[dict] = dump_dict\n\n    def dump_code(self, x):\n        self._write(TYPE_CODE)\n        self.w_long(x.co_argcount)\n        self.w_long(x.co_nlocals)\n        self.w_long(x.co_stacksize)\n        self.w_long(x.co_flags)\n        self.dump(x.co_code)\n        self.dump(x.co_consts)\n        self.dump(x.co_names)\n        self.dump(x.co_varnames)\n        self.dump(x.co_freevars)\n        self.dump(x.co_cellvars)\n        self.dump(x.co_filename)\n        self.dump(x.co_name)\n        self.w_long(x.co_firstlineno)\n        self.dump(x.co_lnotab)\n    try:\n        dispatch[types.CodeType] = dump_code\n    except NameError:\n        pass\n\n    def dump_set(self, x):\n        self._write(TYPE_SET)\n        self.w_long(len(x))\n        for each in x:\n            self.dump(each)\n    try:\n        dispatch[set] = dump_set\n    except NameError:\n        pass\n\n    def dump_frozenset(self, x):\n        self._write(TYPE_FROZENSET)\n        self.w_long(len(x))\n        for each in x:\n            self.dump(each)\n    try:\n        dispatch[frozenset] = dump_frozenset\n    except NameError:\n        pass\n\nclass _NULL:\n    pass\n\nclass _StringBuffer:\n    def __init__(self, value):\n        self.bufstr = value\n        self.bufpos = 0\n\n    def read(self, n):\n        pos = self.bufpos\n        newpos = pos + n\n        ret = self.bufstr[pos : newpos]\n        self.bufpos = newpos\n        return ret\n\n\nclass _Unmarshaller:\n\n    dispatch = {}\n\n    def __init__(self, readfunc):\n        self._read = readfunc\n        self._stringtable = []\n\n    def load(self):\n        c = self._read(1)\n        if not c:\n            raise EOFError\n        try:\n            return self.dispatch[c](self)\n        except KeyError:\n            raise ValueError(\"bad marshal code: %c (%d)\" % (c, ord(c)))\n\n    def r_short(self):\n        lo = ord(self._read(1))\n        hi = ord(self._read(1))\n        x = lo | (hi<<8)\n        if x & 0x8000:\n            x = x - 0x10000\n        return x\n\n    def r_long(self):\n        s = self._read(4)\n        a = ord(s[0])\n        b = ord(s[1])\n        c = ord(s[2])\n        d = ord(s[3])\n        x = a | (b<<8) | (c<<16) | (d<<24)\n        if d & 0x80 and x > 0:\n            x = -((1<<32) - x)\n            return int(x)\n        else:\n            return x\n\n    def r_long64(self):\n        a = ord(self._read(1))\n        b = ord(self._read(1))\n        c = ord(self._read(1))\n        d = ord(self._read(1))\n        e = ord(self._read(1))\n        f = ord(self._read(1))\n        g = ord(self._read(1))\n        h = ord(self._read(1))\n        x = a | (b<<8) | (c<<16) | (d<<24)\n        x = x | (e<<32) | (f<<40) | (g<<48) | (h<<56)\n        if h & 0x80 and x > 0:\n            x = -((1<<64) - x)\n        return x\n\n    def load_null(self):\n        return _NULL\n    dispatch[TYPE_NULL] = load_null\n\n    def load_none(self):\n        return None\n    dispatch[TYPE_NONE] = load_none\n\n    def load_true(self):\n        return True\n    dispatch[TYPE_TRUE] = load_true\n\n    def load_false(self):\n        return False\n    dispatch[TYPE_FALSE] = load_false\n\n    def load_stopiter(self):\n        return StopIteration\n    dispatch[TYPE_STOPITER] = load_stopiter\n\n    def load_ellipsis(self):\n        return Ellipsis\n    dispatch[TYPE_ELLIPSIS] = load_ellipsis\n\n    dispatch[TYPE_INT] = r_long\n\n    dispatch[TYPE_INT64] = r_long64\n\n    def load_long(self):\n        size = self.r_long()\n        sign = 1\n        if size < 0:\n            sign = -1\n            size = -size\n        x = 0\n        for i in range(size):\n            d = self.r_short()\n            x = x | (d<<(i*15))\n        return x * sign\n    dispatch[TYPE_LONG] = load_long\n\n    def load_float(self):\n        n = ord(self._read(1))\n        s = self._read(n)\n        return float(s)\n    dispatch[TYPE_FLOAT] = load_float\n\n    def load_complex(self):\n        n = ord(self._read(1))\n        s = self._read(n)\n        real = float(s)\n        n = ord(self._read(1))\n        s = self._read(n)\n        imag = float(s)\n        return complex(real, imag)\n    dispatch[TYPE_COMPLEX] = load_complex\n\n    def load_string(self):\n        n = self.r_long()\n        return self._read(n)\n    dispatch[TYPE_STRING] = load_string\n\n    def load_interned(self):\n        n = self.r_long()\n        ret = intern(self._read(n))\n        self._stringtable.append(ret)\n        return ret\n    dispatch[TYPE_INTERNED] = load_interned\n\n    def load_stringref(self):\n        n = self.r_long()\n        return self._stringtable[n]\n    dispatch[TYPE_STRINGREF] = load_stringref\n\n    def load_unicode(self):\n        n = self.r_long()\n        s = self._read(n)\n        ret = s.decode('utf8')\n        return ret\n    dispatch[TYPE_UNICODE] = load_unicode\n\n    def load_tuple(self):\n        return tuple(self.load_list())\n    dispatch[TYPE_TUPLE] = load_tuple\n\n    def load_list(self):\n        n = self.r_long()\n        list = [self.load() for i in range(n)]\n        return list\n    dispatch[TYPE_LIST] = load_list\n\n    def load_dict(self):\n        d = {}\n        while 1:\n            key = self.load()\n            if key is _NULL:\n                break\n            value = self.load()\n            d[key] = value\n        return d\n    dispatch[TYPE_DICT] = load_dict\n\n    def load_code(self):\n        argcount = self.r_long()\n        nlocals = self.r_long()\n        stacksize = self.r_long()\n        flags = self.r_long()\n        code = self.load()\n        consts = self.load()\n        names = self.load()\n        varnames = self.load()\n        freevars = self.load()\n        cellvars = self.load()\n        filename = self.load()\n        name = self.load()\n        firstlineno = self.r_long()\n        lnotab = self.load()\n        return types.CodeType(argcount, nlocals, stacksize, flags, code, consts,\n                              names, varnames, filename, name, firstlineno,\n                              lnotab, freevars, cellvars)\n    dispatch[TYPE_CODE] = load_code\n\n    def load_set(self):\n        n = self.r_long()\n        args = [self.load() for i in range(n)]\n        return set(args)\n    dispatch[TYPE_SET] = load_set\n\n    def load_frozenset(self):\n        n = self.r_long()\n        args = [self.load() for i in range(n)]\n        return frozenset(args)\n    dispatch[TYPE_FROZENSET] = load_frozenset\n\n# ________________________________________________________________\n\ndef _read(self, n):\n    pos = self.bufpos\n    newpos = pos + n\n    if newpos > len(self.bufstr): raise EOFError\n    ret = self.bufstr[pos : newpos]\n    self.bufpos = newpos\n    return ret\n\ndef _read1(self):\n    ret = self.bufstr[self.bufpos]\n    self.bufpos += 1\n    return ret\n\ndef _r_short(self):\n    lo = ord(_read1(self))\n    hi = ord(_read1(self))\n    x = lo | (hi<<8)\n    if x & 0x8000:\n        x = x - 0x10000\n    return x\n\ndef _r_long(self):\n    # inlined this most common case\n    p = self.bufpos\n    s = self.bufstr\n    a = ord(s[p])\n    b = ord(s[p+1])\n    c = ord(s[p+2])\n    d = ord(s[p+3])\n    self.bufpos += 4\n    x = a | (b<<8) | (c<<16) | (d<<24)\n    if d & 0x80 and x > 0:\n        x = -((1<<32) - x)\n        return int(x)\n    else:\n        return x\n\ndef _r_long64(self):\n    a = ord(_read1(self))\n    b = ord(_read1(self))\n    c = ord(_read1(self))\n    d = ord(_read1(self))\n    e = ord(_read1(self))\n    f = ord(_read1(self))\n    g = ord(_read1(self))\n    h = ord(_read1(self))\n    x = a | (b<<8) | (c<<16) | (d<<24)\n    x = x | (e<<32) | (f<<40) | (g<<48) | (h<<56)\n    if h & 0x80 and x > 0:\n        x = -((1<<64) - x)\n    return x\n\n_load_dispatch = {}\n\nclass _FastUnmarshaller:\n\n    dispatch = {}\n\n    def __init__(self, buffer):\n        self.bufstr = buffer\n        self.bufpos = 0\n        self._stringtable = []\n\n    def load(self):\n        # make flow space happy\n        c = '?'\n        try:\n            c = self.bufstr[self.bufpos]\n            self.bufpos += 1\n            return _load_dispatch[c](self)\n        except KeyError:\n            raise ValueError(\"bad marshal code: %c (%d)\" % (c, ord(c)))\n        except IndexError:\n            raise EOFError\n\n    def load_null(self):\n        return _NULL\n    dispatch[TYPE_NULL] = load_null\n\n    def load_none(self):\n        return None\n    dispatch[TYPE_NONE] = load_none\n\n    def load_true(self):\n        return True\n    dispatch[TYPE_TRUE] = load_true\n\n    def load_false(self):\n        return False\n    dispatch[TYPE_FALSE] = load_false\n\n    def load_stopiter(self):\n        return StopIteration\n    dispatch[TYPE_STOPITER] = load_stopiter\n\n    def load_ellipsis(self):\n        return Ellipsis\n    dispatch[TYPE_ELLIPSIS] = load_ellipsis\n\n    def load_int(self):\n        return _r_long(self)\n    dispatch[TYPE_INT] = load_int\n\n    def load_int64(self):\n        return _r_long64(self)\n    dispatch[TYPE_INT64] = load_int64\n\n    def load_long(self):\n        size = _r_long(self)\n        sign = 1\n        if size < 0:\n            sign = -1\n            size = -size\n        x = 0\n        for i in range(size):\n            d = _r_short(self)\n            x = x | (d<<(i*15))\n        return x * sign\n    dispatch[TYPE_LONG] = load_long\n\n    def load_float(self):\n        n = ord(_read1(self))\n        s = _read(self, n)\n        return float(s)\n    dispatch[TYPE_FLOAT] = load_float\n\n    def load_complex(self):\n        n = ord(_read1(self))\n        s = _read(self, n)\n        real = float(s)\n        n = ord(_read1(self))\n        s = _read(self, n)\n        imag = float(s)\n        return complex(real, imag)\n    dispatch[TYPE_COMPLEX] = load_complex\n\n    def load_string(self):\n        n = _r_long(self)\n        return _read(self, n)\n    dispatch[TYPE_STRING] = load_string\n\n    def load_interned(self):\n        n = _r_long(self)\n        ret = intern(_read(self, n))\n        self._stringtable.append(ret)\n        return ret\n    dispatch[TYPE_INTERNED] = load_interned\n\n    def load_stringref(self):\n        n = _r_long(self)\n        return self._stringtable[n]\n    dispatch[TYPE_STRINGREF] = load_stringref\n\n    def load_unicode(self):\n        n = _r_long(self)\n        s = _read(self, n)\n        ret = s.decode('utf8')\n        return ret\n    dispatch[TYPE_UNICODE] = load_unicode\n\n    def load_tuple(self):\n        return tuple(self.load_list())\n    dispatch[TYPE_TUPLE] = load_tuple\n\n    def load_list(self):\n        n = _r_long(self)\n        list = []\n        for i in range(n):\n            list.append(self.load())\n        return list\n    dispatch[TYPE_LIST] = load_list\n\n    def load_dict(self):\n        d = {}\n        while 1:\n            key = self.load()\n            if key is _NULL:\n                break\n            value = self.load()\n            d[key] = value\n        return d\n    dispatch[TYPE_DICT] = load_dict\n\n    def load_code(self):\n        argcount = _r_long(self)\n        nlocals = _r_long(self)\n        stacksize = _r_long(self)\n        flags = _r_long(self)\n        code = self.load()\n        consts = self.load()\n        names = self.load()\n        varnames = self.load()\n        freevars = self.load()\n        cellvars = self.load()\n        filename = self.load()\n        name = self.load()\n        firstlineno = _r_long(self)\n        lnotab = self.load()\n        return types.CodeType(argcount, nlocals, stacksize, flags, code, consts,\n                              names, varnames, filename, name, firstlineno,\n                              lnotab, freevars, cellvars)\n    dispatch[TYPE_CODE] = load_code\n\n    def load_set(self):\n        n = _r_long(self)\n        args = [self.load() for i in range(n)]\n        return set(args)\n    dispatch[TYPE_SET] = load_set\n\n    def load_frozenset(self):\n        n = _r_long(self)\n        args = [self.load() for i in range(n)]\n        return frozenset(args)\n    dispatch[TYPE_FROZENSET] = load_frozenset\n\n_load_dispatch = _FastUnmarshaller.dispatch\n\n# _________________________________________________________________\n#\n# user interface\n\nversion = 1\n\n@builtinify\ndef dump(x, f, version=version):\n    # XXX 'version' is ignored, we always dump in a version-0-compatible format\n    m = _Marshaller(f.write)\n    m.dump(x)\n\n@builtinify\ndef load(f):\n    um = _Unmarshaller(f.read)\n    return um.load()\n\n@builtinify\ndef dumps(x, version=version):\n    # XXX 'version' is ignored, we always dump in a version-0-compatible format\n    buffer = []\n    m = _Marshaller(buffer.append)\n    m.dump(x)\n    return ''.join(buffer)\n\n@builtinify\ndef loads(s):\n    um = _FastUnmarshaller(s)\n    return um.load()\n",
		"file_name": "_marshal.py"
	},
	{
		"content": "\"\"\"Create portable serialized representations of Python objects.\n\nSee module cPickle for a (much) faster implementation.\nSee module copy_reg for a mechanism for registering custom picklers.\nSee module pickletools source for extensive comments.\n\nClasses:\n\n    Pickler\n    Unpickler\n\nFunctions:\n\n    dump(object, file)\n    dumps(object) -> string\n    load(file) -> object\n    loads(string) -> object\n\nMisc variables:\n\n    __version__\n    format_version\n    compatible_formats\n\n\"\"\"\n\n__version__ = \"$Revision: 72223 $\"       # Code version\n\nfrom types import *\nfrom copy_reg import dispatch_table\nfrom copy_reg import _extension_registry, _inverted_registry, _extension_cache\nimport marshal\nimport sys\nimport struct\nimport re\n\n__all__ = [\"PickleError\", \"PicklingError\", \"UnpicklingError\", \"Pickler\",\n           \"Unpickler\", \"dump\", \"dumps\", \"load\", \"loads\"]\n\n# These are purely informational; no code uses these.\nformat_version = \"2.0\"                  # File format version we write\ncompatible_formats = [\"1.0\",            # Original protocol 0\n                      \"1.1\",            # Protocol 0 with INST added\n                      \"1.2\",            # Original protocol 1\n                      \"1.3\",            # Protocol 1 with BINFLOAT added\n                      \"2.0\",            # Protocol 2\n                      ]                 # Old format versions we can read\n\n# Keep in synch with cPickle.  This is the highest protocol number we\n# know how to read.\nHIGHEST_PROTOCOL = 2\n\n# Why use struct.pack() for pickling but marshal.loads() for\n# unpickling?  struct.pack() is 40% faster than marshal.dumps(), but\n# marshal.loads() is twice as fast as struct.unpack()!\nmloads = marshal.loads\n\nclass PickleError(Exception):\n    \"\"\"A common base class for the other pickling exceptions.\"\"\"\n    pass\n\nclass PicklingError(PickleError):\n    \"\"\"This exception is raised when an unpicklable object is passed to the\n    dump() method.\n\n    \"\"\"\n    pass\n\nclass UnpicklingError(PickleError):\n    \"\"\"This exception is raised when there is a problem unpickling an object,\n    such as a security violation.\n\n    Note that other exceptions may also be raised during unpickling, including\n    (but not necessarily limited to) AttributeError, EOFError, ImportError,\n    and IndexError.\n\n    \"\"\"\n    pass\n\n# An instance of _Stop is raised by Unpickler.load_stop() in response to\n# the STOP opcode, passing the object that is the result of unpickling.\nclass _Stop(Exception):\n    def __init__(self, value):\n        self.value = value\n\n# Jython has PyStringMap; it's a dict subclass with string keys\ntry:\n    from org.python.core import PyStringMap\nexcept ImportError:\n    PyStringMap = None\n\n# UnicodeType may or may not be exported (normally imported from types)\ntry:\n    UnicodeType\nexcept NameError:\n    UnicodeType = None\n\n# Pickle opcodes.  See pickletools.py for extensive docs.  The listing\n# here is in kind-of alphabetical order of 1-character pickle code.\n# pickletools groups them by purpose.\n\nMARK            = '('   # push special markobject on stack\nSTOP            = '.'   # every pickle ends with STOP\nPOP             = '0'   # discard topmost stack item\nPOP_MARK        = '1'   # discard stack top through topmost markobject\nDUP             = '2'   # duplicate top stack item\nFLOAT           = 'F'   # push float object; decimal string argument\nINT             = 'I'   # push integer or bool; decimal string argument\nBININT          = 'J'   # push four-byte signed int\nBININT1         = 'K'   # push 1-byte unsigned int\nLONG            = 'L'   # push long; decimal string argument\nBININT2         = 'M'   # push 2-byte unsigned int\nNONE            = 'N'   # push None\nPERSID          = 'P'   # push persistent object; id is taken from string arg\nBINPERSID       = 'Q'   #  \"       \"         \"  ;  \"  \"   \"     \"  stack\nREDUCE          = 'R'   # apply callable to argtuple, both on stack\nSTRING          = 'S'   # push string; NL-terminated string argument\nBINSTRING       = 'T'   # push string; counted binary string argument\nSHORT_BINSTRING = 'U'   #  \"     \"   ;    \"      \"       \"      \" < 256 bytes\nUNICODE         = 'V'   # push Unicode string; raw-unicode-escaped'd argument\nBINUNICODE      = 'X'   #   \"     \"       \"  ; counted UTF-8 string argument\nAPPEND          = 'a'   # append stack top to list below it\nBUILD           = 'b'   # call __setstate__ or __dict__.update()\nGLOBAL          = 'c'   # push self.find_class(modname, name); 2 string args\nDICT            = 'd'   # build a dict from stack items\nEMPTY_DICT      = '}'   # push empty dict\nAPPENDS         = 'e'   # extend list on stack by topmost stack slice\nGET             = 'g'   # push item from memo on stack; index is string arg\nBINGET          = 'h'   #   \"    \"    \"    \"   \"   \"  ;   \"    \" 1-byte arg\nINST            = 'i'   # build & push class instance\nLONG_BINGET     = 'j'   # push item from memo on stack; index is 4-byte arg\nLIST            = 'l'   # build list from topmost stack items\nEMPTY_LIST      = ']'   # push empty list\nOBJ             = 'o'   # build & push class instance\nPUT             = 'p'   # store stack top in memo; index is string arg\nBINPUT          = 'q'   #   \"     \"    \"   \"   \" ;   \"    \" 1-byte arg\nLONG_BINPUT     = 'r'   #   \"     \"    \"   \"   \" ;   \"    \" 4-byte arg\nSETITEM         = 's'   # add key+value pair to dict\nTUPLE           = 't'   # build tuple from topmost stack items\nEMPTY_TUPLE     = ')'   # push empty tuple\nSETITEMS        = 'u'   # modify dict by adding topmost key+value pairs\nBINFLOAT        = 'G'   # push float; arg is 8-byte float encoding\n\nTRUE            = 'I01\\n'  # not an opcode; see INT docs in pickletools.py\nFALSE           = 'I00\\n'  # not an opcode; see INT docs in pickletools.py\n\n# Protocol 2\n\nPROTO           = '\\x80'  # identify pickle protocol\nNEWOBJ          = '\\x81'  # build object by applying cls.__new__ to argtuple\nEXT1            = '\\x82'  # push object from extension registry; 1-byte index\nEXT2            = '\\x83'  # ditto, but 2-byte index\nEXT4            = '\\x84'  # ditto, but 4-byte index\nTUPLE1          = '\\x85'  # build 1-tuple from stack top\nTUPLE2          = '\\x86'  # build 2-tuple from two topmost stack items\nTUPLE3          = '\\x87'  # build 3-tuple from three topmost stack items\nNEWTRUE         = '\\x88'  # push True\nNEWFALSE        = '\\x89'  # push False\nLONG1           = '\\x8a'  # push long from < 256 bytes\nLONG4           = '\\x8b'  # push really big long\n\n_tuplesize2code = [EMPTY_TUPLE, TUPLE1, TUPLE2, TUPLE3]\n\n\n__all__.extend([x for x in dir() if re.match(\"[A-Z][A-Z0-9_]+$\",x)])\ndel x\n\n\n# Pickling machinery\n\nclass Pickler(object):\n\n    def __init__(self, file, protocol=None):\n        \"\"\"This takes a file-like object for writing a pickle data stream.\n\n        The optional protocol argument tells the pickler to use the\n        given protocol; supported protocols are 0, 1, 2.  The default\n        protocol is 0, to be backwards compatible.  (Protocol 0 is the\n        only protocol that can be written to a file opened in text\n        mode and read back successfully.  When using a protocol higher\n        than 0, make sure the file is opened in binary mode, both when\n        pickling and unpickling.)\n\n        Protocol 1 is more efficient than protocol 0; protocol 2 is\n        more efficient than protocol 1.\n\n        Specifying a negative protocol version selects the highest\n        protocol version supported.  The higher the protocol used, the\n        more recent the version of Python needed to read the pickle\n        produced.\n\n        The file parameter must have a write() method that accepts a single\n        string argument.  It can thus be an open file object, a StringIO\n        object, or any other custom object that meets this interface.\n\n        \"\"\"\n        if protocol is None:\n            protocol = 0\n        if protocol < 0:\n            protocol = HIGHEST_PROTOCOL\n        elif not 0 <= protocol <= HIGHEST_PROTOCOL:\n            raise ValueError(\"pickle protocol must be <= %d\" % HIGHEST_PROTOCOL)\n        self.write = file.write\n        self.memo = {}\n        self.proto = int(protocol)\n        self.bin = protocol >= 1\n        self.fast = 0\n\n    def clear_memo(self):\n        \"\"\"Clears the pickler's \"memo\".\n\n        The memo is the data structure that remembers which objects the\n        pickler has already seen, so that shared or recursive objects are\n        pickled by reference and not by value.  This method is useful when\n        re-using picklers.\n\n        \"\"\"\n        self.memo.clear()\n\n    def dump(self, obj):\n        \"\"\"Write a pickled representation of obj to the open file.\"\"\"\n        if self.proto >= 2:\n            self.write(PROTO + chr(self.proto))\n        self.save(obj)\n        self.write(STOP)\n\n    def memoize(self, obj):\n        \"\"\"Store an object in the memo.\"\"\"\n\n        # The Pickler memo is a dictionary mapping object ids to 2-tuples\n        # that contain the Unpickler memo key and the object being memoized.\n        # The memo key is written to the pickle and will become\n        # the key in the Unpickler's memo.  The object is stored in the\n        # Pickler memo so that transient objects are kept alive during\n        # pickling.\n\n        # The use of the Unpickler memo length as the memo key is just a\n        # convention.  The only requirement is that the memo values be unique.\n        # But there appears no advantage to any other scheme, and this\n        # scheme allows the Unpickler memo to be implemented as a plain (but\n        # growable) array, indexed by memo key.\n        if self.fast:\n            return\n        assert id(obj) not in self.memo\n        memo_len = len(self.memo)\n        self.write(self.put(memo_len))\n        self.memo[id(obj)] = memo_len, obj\n\n    # Return a PUT (BINPUT, LONG_BINPUT) opcode string, with argument i.\n    def put(self, i, pack=struct.pack):\n        if self.bin:\n            if i < 256:\n                return BINPUT + chr(i)\n            else:\n                return LONG_BINPUT + pack(\"<i\", i)\n\n        return PUT + repr(i) + '\\n'\n\n    # Return a GET (BINGET, LONG_BINGET) opcode string, with argument i.\n    def get(self, i, pack=struct.pack):\n        if self.bin:\n            if i < 256:\n                return BINGET + chr(i)\n            else:\n                return LONG_BINGET + pack(\"<i\", i)\n\n        return GET + repr(i) + '\\n'\n\n    def save(self, obj):\n        # Check for persistent id (defined by a subclass)\n        pid = self.persistent_id(obj)\n        if pid is not None:\n            self.save_pers(pid)\n            return\n\n        # Check the memo\n        x = self.memo.get(id(obj))\n        if x:\n            self.write(self.get(x[0]))\n            return\n\n        # Check the type dispatch table\n        t = type(obj)\n        f = self.dispatch.get(t)\n        if f:\n            f(self, obj) # Call unbound method with explicit self\n            return\n\n        # Check copy_reg.dispatch_table\n        reduce = dispatch_table.get(t)\n        if reduce:\n            rv = reduce(obj)\n        else:\n            # Check for a class with a custom metaclass; treat as regular class\n            try:\n                issc = issubclass(t, TypeType)\n            except TypeError: # t is not a class (old Boost; see SF #502085)\n                issc = 0\n            if issc:\n                self.save_global(obj)\n                return\n\n            # Check for a __reduce_ex__ method, fall back to __reduce__\n            reduce = getattr(obj, \"__reduce_ex__\", None)\n            if reduce:\n                rv = reduce(self.proto)\n            else:\n                reduce = getattr(obj, \"__reduce__\", None)\n                if reduce:\n                    rv = reduce()\n                else:\n                    raise PicklingError(\"Can't pickle %r object: %r\" %\n                                        (t.__name__, obj))\n\n        # Check for string returned by reduce(), meaning \"save as global\"\n        if type(rv) is StringType:\n            self.save_global(obj, rv)\n            return\n\n        # Assert that reduce() returned a tuple\n        if type(rv) is not TupleType:\n            raise PicklingError(\"%s must return string or tuple\" % reduce)\n\n        # Assert that it returned an appropriately sized tuple\n        l = len(rv)\n        if not (2 <= l <= 5):\n            raise PicklingError(\"Tuple returned by %s must have \"\n                                \"two to five elements\" % reduce)\n\n        # Save the reduce() output and finally memoize the object\n        self.save_reduce(obj=obj, *rv)\n\n    def persistent_id(self, obj):\n        # This exists so a subclass can override it\n        return None\n\n    def save_pers(self, pid):\n        # Save a persistent id reference\n        if self.bin:\n            self.save(pid)\n            self.write(BINPERSID)\n        else:\n            self.write(PERSID + str(pid) + '\\n')\n\n    def save_reduce(self, func, args, state=None,\n                    listitems=None, dictitems=None, obj=None):\n        # This API is called by some subclasses\n\n        # Assert that args is a tuple or None\n        if not isinstance(args, TupleType):\n            raise PicklingError(\"args from reduce() should be a tuple\")\n\n        # Assert that func is callable\n        if not hasattr(func, '__call__'):\n            raise PicklingError(\"func from reduce should be callable\")\n\n        save = self.save\n        write = self.write\n\n        # Protocol 2 special case: if func's name is __newobj__, use NEWOBJ\n        if self.proto >= 2 and getattr(func, \"__name__\", \"\") == \"__newobj__\":\n            # A __reduce__ implementation can direct protocol 2 to\n            # use the more efficient NEWOBJ opcode, while still\n            # allowing protocol 0 and 1 to work normally.  For this to\n            # work, the function returned by __reduce__ should be\n            # called __newobj__, and its first argument should be a\n            # new-style class.  The implementation for __newobj__\n            # should be as follows, although pickle has no way to\n            # verify this:\n            #\n            # def __newobj__(cls, *args):\n            #     return cls.__new__(cls, *args)\n            #\n            # Protocols 0 and 1 will pickle a reference to __newobj__,\n            # while protocol 2 (and above) will pickle a reference to\n            # cls, the remaining args tuple, and the NEWOBJ code,\n            # which calls cls.__new__(cls, *args) at unpickling time\n            # (see load_newobj below).  If __reduce__ returns a\n            # three-tuple, the state from the third tuple item will be\n            # pickled regardless of the protocol, calling __setstate__\n            # at unpickling time (see load_build below).\n            #\n            # Note that no standard __newobj__ implementation exists;\n            # you have to provide your own.  This is to enforce\n            # compatibility with Python 2.2 (pickles written using\n            # protocol 0 or 1 in Python 2.3 should be unpicklable by\n            # Python 2.2).\n            cls = args[0]\n            if not hasattr(cls, \"__new__\"):\n                raise PicklingError(\n                    \"args[0] from __newobj__ args has no __new__\")\n            if obj is not None and cls is not obj.__class__:\n                raise PicklingError(\n                    \"args[0] from __newobj__ args has the wrong class\")\n            args = args[1:]\n            save(cls)\n            save(args)\n            write(NEWOBJ)\n        else:\n            save(func)\n            save(args)\n            write(REDUCE)\n\n        if obj is not None:\n            self.memoize(obj)\n\n        # More new special cases (that work with older protocols as\n        # well): when __reduce__ returns a tuple with 4 or 5 items,\n        # the 4th and 5th item should be iterators that provide list\n        # items and dict items (as (key, value) tuples), or None.\n\n        if listitems is not None:\n            self._batch_appends(listitems)\n\n        if dictitems is not None:\n            self._batch_setitems(dictitems)\n\n        if state is not None:\n            save(state)\n            write(BUILD)\n\n    # Methods below this point are dispatched through the dispatch table\n\n    dispatch = {}\n\n    def save_none(self, obj):\n        self.write(NONE)\n    dispatch[NoneType] = save_none\n\n    def save_bool(self, obj):\n        if self.proto >= 2:\n            self.write(obj and NEWTRUE or NEWFALSE)\n        else:\n            self.write(obj and TRUE or FALSE)\n    dispatch[bool] = save_bool\n\n    def save_int(self, obj, pack=struct.pack):\n        if self.bin:\n            # If the int is small enough to fit in a signed 4-byte 2's-comp\n            # format, we can store it more efficiently than the general\n            # case.\n            # First one- and two-byte unsigned ints:\n            if obj >= 0:\n                if obj <= 0xff:\n                    self.write(BININT1 + chr(obj))\n                    return\n                if obj <= 0xffff:\n                    self.write(\"%c%c%c\" % (BININT2, obj&0xff, obj>>8))\n                    return\n            # Next check for 4-byte signed ints:\n            high_bits = obj >> 31  # note that Python shift sign-extends\n            if high_bits == 0 or high_bits == -1:\n                # All high bits are copies of bit 2**31, so the value\n                # fits in a 4-byte signed int.\n                self.write(BININT + pack(\"<i\", obj))\n                return\n        # Text pickle, or int too big to fit in signed 4-byte format.\n        self.write(INT + repr(obj) + '\\n')\n    dispatch[IntType] = save_int\n\n    def save_long(self, obj, pack=struct.pack):\n        if self.proto >= 2:\n            bytes = encode_long(obj)\n            n = len(bytes)\n            if n < 256:\n                self.write(LONG1 + chr(n) + bytes)\n            else:\n                self.write(LONG4 + pack(\"<i\", n) + bytes)\n            return\n        self.write(LONG + repr(obj) + '\\n')\n    dispatch[LongType] = save_long\n\n    def save_float(self, obj, pack=struct.pack):\n        if self.bin:\n            self.write(BINFLOAT + pack('>d', obj))\n        else:\n            self.write(FLOAT + repr(obj) + '\\n')\n    dispatch[FloatType] = save_float\n\n    def save_string(self, obj, pack=struct.pack):\n        if self.bin:\n            n = len(obj)\n            if n < 256:\n                self.write(SHORT_BINSTRING + chr(n) + obj)\n            else:\n                self.write(BINSTRING + pack(\"<i\", n) + obj)\n        else:\n            self.write(STRING + repr(obj) + '\\n')\n        self.memoize(obj)\n    dispatch[StringType] = save_string\n\n    def save_unicode(self, obj, pack=struct.pack):\n        if self.bin:\n            encoding = obj.encode('utf-8')\n            n = len(encoding)\n            self.write(BINUNICODE + pack(\"<i\", n) + encoding)\n        else:\n            obj = obj.replace(\"\\\\\", \"\\\\u005c\")\n            obj = obj.replace(\"\\n\", \"\\\\u000a\")\n            self.write(UNICODE + obj.encode('raw-unicode-escape') + '\\n')\n        self.memoize(obj)\n    dispatch[UnicodeType] = save_unicode\n\n    if StringType is UnicodeType:\n        # This is true for Jython\n        def save_string(self, obj, pack=struct.pack):\n            unicode = obj.isunicode()\n\n            if self.bin:\n                if unicode:\n                    obj = obj.encode(\"utf-8\")\n                l = len(obj)\n                if l < 256 and not unicode:\n                    self.write(SHORT_BINSTRING + chr(l) + obj)\n                else:\n                    s = pack(\"<i\", l)\n                    if unicode:\n                        self.write(BINUNICODE + s + obj)\n                    else:\n                        self.write(BINSTRING + s + obj)\n            else:\n                if unicode:\n                    obj = obj.replace(\"\\\\\", \"\\\\u005c\")\n                    obj = obj.replace(\"\\n\", \"\\\\u000a\")\n                    obj = obj.encode('raw-unicode-escape')\n                    self.write(UNICODE + obj + '\\n')\n                else:\n                    self.write(STRING + repr(obj) + '\\n')\n            self.memoize(obj)\n        dispatch[StringType] = save_string\n\n    def save_tuple(self, obj):\n        write = self.write\n        proto = self.proto\n\n        n = len(obj)\n        if n == 0:\n            if proto:\n                write(EMPTY_TUPLE)\n            else:\n                write(MARK + TUPLE)\n            return\n\n        save = self.save\n        memo = self.memo\n        if n <= 3 and proto >= 2:\n            for element in obj:\n                save(element)\n            # Subtle.  Same as in the big comment below.\n            if id(obj) in memo:\n                get = self.get(memo[id(obj)][0])\n                write(POP * n + get)\n            else:\n                write(_tuplesize2code[n])\n                self.memoize(obj)\n            return\n\n        # proto 0 or proto 1 and tuple isn't empty, or proto > 1 and tuple\n        # has more than 3 elements.\n        write(MARK)\n        for element in obj:\n            save(element)\n\n        if id(obj) in memo:\n            # Subtle.  d was not in memo when we entered save_tuple(), so\n            # the process of saving the tuple's elements must have saved\n            # the tuple itself:  the tuple is recursive.  The proper action\n            # now is to throw away everything we put on the stack, and\n            # simply GET the tuple (it's already constructed).  This check\n            # could have been done in the \"for element\" loop instead, but\n            # recursive tuples are a rare thing.\n            get = self.get(memo[id(obj)][0])\n            if proto:\n                write(POP_MARK + get)\n            else:   # proto 0 -- POP_MARK not available\n                write(POP * (n+1) + get)\n            return\n\n        # No recursion.\n        self.write(TUPLE)\n        self.memoize(obj)\n\n    dispatch[TupleType] = save_tuple\n\n    # save_empty_tuple() isn't used by anything in Python 2.3.  However, I\n    # found a Pickler subclass in Zope3 that calls it, so it's not harmless\n    # to remove it.\n    def save_empty_tuple(self, obj):\n        self.write(EMPTY_TUPLE)\n\n    def save_list(self, obj):\n        write = self.write\n\n        if self.bin:\n            write(EMPTY_LIST)\n        else:   # proto 0 -- can't use EMPTY_LIST\n            write(MARK + LIST)\n\n        self.memoize(obj)\n        self._batch_appends(iter(obj))\n\n    dispatch[ListType] = save_list\n\n    # Keep in synch with cPickle's BATCHSIZE.  Nothing will break if it gets\n    # out of synch, though.\n    _BATCHSIZE = 1000\n\n    def _batch_appends(self, items):\n        # Helper to batch up APPENDS sequences\n        save = self.save\n        write = self.write\n\n        if not self.bin:\n            for x in items:\n                save(x)\n                write(APPEND)\n            return\n\n        r = xrange(self._BATCHSIZE)\n        while items is not None:\n            tmp = []\n            for i in r:\n                try:\n                    x = items.next()\n                    tmp.append(x)\n                except StopIteration:\n                    items = None\n                    break\n            n = len(tmp)\n            if n > 1:\n                write(MARK)\n                for x in tmp:\n                    save(x)\n                write(APPENDS)\n            elif n:\n                save(tmp[0])\n                write(APPEND)\n            # else tmp is empty, and we're done\n\n    def save_dict(self, obj):\n        modict_saver = self._pickle_maybe_moduledict(obj)\n        if modict_saver is not None:\n            return self.save_reduce(*modict_saver)\n\n        write = self.write\n\n        if self.bin:\n            write(EMPTY_DICT)\n        else:   # proto 0 -- can't use EMPTY_DICT\n            write(MARK + DICT)\n\n        self.memoize(obj)\n        self._batch_setitems(obj.iteritems())\n\n    dispatch[DictionaryType] = save_dict\n    if not PyStringMap is None:\n        dispatch[PyStringMap] = save_dict\n\n    def _batch_setitems(self, items):\n        # Helper to batch up SETITEMS sequences; proto >= 1 only\n        save = self.save\n        write = self.write\n\n        if not self.bin:\n            for k, v in items:\n                save(k)\n                save(v)\n                write(SETITEM)\n            return\n\n        r = xrange(self._BATCHSIZE)\n        while items is not None:\n            tmp = []\n            for i in r:\n                try:\n                    tmp.append(items.next())\n                except StopIteration:\n                    items = None\n                    break\n            n = len(tmp)\n            if n > 1:\n                write(MARK)\n                for k, v in tmp:\n                    save(k)\n                    save(v)\n                write(SETITEMS)\n            elif n:\n                k, v = tmp[0]\n                save(k)\n                save(v)\n                write(SETITEM)\n            # else tmp is empty, and we're done\n\n    def _pickle_maybe_moduledict(self, obj):\n        # save module dictionary as \"getattr(module, '__dict__')\"\n        try:\n            name = obj['__name__']\n            if type(name) is not str:\n                return None\n            themodule = sys.modules[name]\n            if type(themodule) is not ModuleType:\n                return None\n            if themodule.__dict__ is not obj:\n                return None\n        except (AttributeError, KeyError, TypeError):\n            return None\n\n        return getattr, (themodule, '__dict__')\n\n\n    def save_inst(self, obj):\n        cls = obj.__class__\n\n        memo  = self.memo\n        write = self.write\n        save  = self.save\n\n        if hasattr(obj, '__getinitargs__'):\n            args = obj.__getinitargs__()\n            len(args) # XXX Assert it's a sequence\n            _keep_alive(args, memo)\n        else:\n            args = ()\n\n        write(MARK)\n\n        if self.bin:\n            save(cls)\n            for arg in args:\n                save(arg)\n            write(OBJ)\n        else:\n            for arg in args:\n                save(arg)\n            write(INST + cls.__module__ + '\\n' + cls.__name__ + '\\n')\n\n        self.memoize(obj)\n\n        try:\n            getstate = obj.__getstate__\n        except AttributeError:\n            stuff = obj.__dict__\n        else:\n            stuff = getstate()\n            _keep_alive(stuff, memo)\n        save(stuff)\n        write(BUILD)\n\n    dispatch[InstanceType] = save_inst\n\n    def save_function(self, obj):\n        try:\n            return self.save_global(obj)\n        except PicklingError, e:\n            pass\n        # Check copy_reg.dispatch_table\n        reduce = dispatch_table.get(type(obj))\n        if reduce:\n            rv = reduce(obj)\n        else:\n            # Check for a __reduce_ex__ method, fall back to __reduce__\n            reduce = getattr(obj, \"__reduce_ex__\", None)\n            if reduce:\n                rv = reduce(self.proto)\n            else:\n                reduce = getattr(obj, \"__reduce__\", None)\n                if reduce:\n                    rv = reduce()\n                else:\n                    raise e\n        return self.save_reduce(obj=obj, *rv)\n    dispatch[FunctionType] = save_function\n\n    def save_global(self, obj, name=None, pack=struct.pack):\n        write = self.write\n        memo = self.memo\n\n        if name is None:\n            name = obj.__name__\n\n        module = getattr(obj, \"__module__\", None)\n        if module is None:\n            module = whichmodule(obj, name)\n\n        try:\n            __import__(module)\n            mod = sys.modules[module]\n            klass = getattr(mod, name)\n        except (ImportError, KeyError, AttributeError):\n            raise PicklingError(\n                \"Can't pickle %r: it's not found as %s.%s\" %\n                (obj, module, name))\n        else:\n            if klass is not obj:\n                raise PicklingError(\n                    \"Can't pickle %r: it's not the same object as %s.%s\" %\n                    (obj, module, name))\n\n        if self.proto >= 2:\n            code = _extension_registry.get((module, name))\n            if code:\n                assert code > 0\n                if code <= 0xff:\n                    write(EXT1 + chr(code))\n                elif code <= 0xffff:\n                    write(\"%c%c%c\" % (EXT2, code&0xff, code>>8))\n                else:\n                    write(EXT4 + pack(\"<i\", code))\n                return\n\n        write(GLOBAL + module + '\\n' + name + '\\n')\n        self.memoize(obj)\n\n    dispatch[ClassType] = save_global\n    dispatch[BuiltinFunctionType] = save_global\n    dispatch[TypeType] = save_global\n\n# Pickling helpers\n\ndef _keep_alive(x, memo):\n    \"\"\"Keeps a reference to the object x in the memo.\n\n    Because we remember objects by their id, we have\n    to assure that possibly temporary objects are kept\n    alive by referencing them.\n    We store a reference at the id of the memo, which should\n    normally not be used unless someone tries to deepcopy\n    the memo itself...\n    \"\"\"\n    try:\n        memo[id(memo)].append(x)\n    except KeyError:\n        # aha, this is the first one :-)\n        memo[id(memo)]=[x]\n\n\n# A cache for whichmodule(), mapping a function object to the name of\n# the module in which the function was found.\n\nclassmap = {} # called classmap for backwards compatibility\n\ndef whichmodule(func, funcname):\n    \"\"\"Figure out the module in which a function occurs.\n\n    Search sys.modules for the module.\n    Cache in classmap.\n    Return a module name.\n    If the function cannot be found, return \"__main__\".\n    \"\"\"\n    # Python functions should always get an __module__ from their globals.\n    mod = getattr(func, \"__module__\", None)\n    if mod is not None:\n        return mod\n    if func in classmap:\n        return classmap[func]\n\n    for name, module in sys.modules.items():\n        if module is None:\n            continue # skip dummy package entries\n        if name != '__main__' and getattr(module, funcname, None) is func:\n            break\n    else:\n        name = '__main__'\n    classmap[func] = name\n    return name\n\n\n# Unpickling machinery\n\nclass Unpickler(object):\n\n    def __init__(self, file):\n        \"\"\"This takes a file-like object for reading a pickle data stream.\n\n        The protocol version of the pickle is detected automatically, so no\n        proto argument is needed.\n\n        The file-like object must have two methods, a read() method that\n        takes an integer argument, and a readline() method that requires no\n        arguments.  Both methods should return a string.  Thus file-like\n        object can be a file object opened for reading, a StringIO object,\n        or any other custom object that meets this interface.\n        \"\"\"\n        self.readline = file.readline\n        self.read = file.read\n        self.memo = {}\n\n    def load(self):\n        \"\"\"Read a pickled object representation from the open file.\n\n        Return the reconstituted object hierarchy specified in the file.\n        \"\"\"\n        self.mark = object() # any new unique object\n        self.stack = []\n        self.append = self.stack.append\n        read = self.read\n        dispatch = self.dispatch\n        try:\n            while 1:\n                key = read(1)\n                dispatch[key](self)\n        except _Stop, stopinst:\n            return stopinst.value\n\n    # Return largest index k such that self.stack[k] is self.mark.\n    # If the stack doesn't contain a mark, eventually raises IndexError.\n    # This could be sped by maintaining another stack, of indices at which\n    # the mark appears.  For that matter, the latter stack would suffice,\n    # and we wouldn't need to push mark objects on self.stack at all.\n    # Doing so is probably a good thing, though, since if the pickle is\n    # corrupt (or hostile) we may get a clue from finding self.mark embedded\n    # in unpickled objects.\n    def marker(self):\n        stack = self.stack\n        mark = self.mark\n        k = len(stack)-1\n        while stack[k] is not mark: k = k-1\n        return k\n\n    dispatch = {}\n\n    def load_eof(self):\n        raise EOFError\n    dispatch[''] = load_eof\n\n    def load_proto(self):\n        proto = ord(self.read(1))\n        if not 0 <= proto <= 2:\n            raise ValueError, \"unsupported pickle protocol: %d\" % proto\n    dispatch[PROTO] = load_proto\n\n    def load_persid(self):\n        pid = self.readline()[:-1]\n        self.append(self.persistent_load(pid))\n    dispatch[PERSID] = load_persid\n\n    def load_binpersid(self):\n        pid = self.stack.pop()\n        self.append(self.persistent_load(pid))\n    dispatch[BINPERSID] = load_binpersid\n\n    def load_none(self):\n        self.append(None)\n    dispatch[NONE] = load_none\n\n    def load_false(self):\n        self.append(False)\n    dispatch[NEWFALSE] = load_false\n\n    def load_true(self):\n        self.append(True)\n    dispatch[NEWTRUE] = load_true\n\n    def load_int(self):\n        data = self.readline()\n        if data == FALSE[1:]:\n            val = False\n        elif data == TRUE[1:]:\n            val = True\n        else:\n            try:\n                val = int(data)\n            except ValueError:\n                val = long(data)\n        self.append(val)\n    dispatch[INT] = load_int\n\n    def load_binint(self):\n        self.append(mloads('i' + self.read(4)))\n    dispatch[BININT] = load_binint\n\n    def load_binint1(self):\n        self.append(ord(self.read(1)))\n    dispatch[BININT1] = load_binint1\n\n    def load_binint2(self):\n        self.append(mloads('i' + self.read(2) + '\\000\\000'))\n    dispatch[BININT2] = load_binint2\n\n    def load_long(self):\n        self.append(long(self.readline()[:-1], 0))\n    dispatch[LONG] = load_long\n\n    def load_long1(self):\n        n = ord(self.read(1))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG1] = load_long1\n\n    def load_long4(self):\n        n = mloads('i' + self.read(4))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG4] = load_long4\n\n    def load_float(self):\n        self.append(float(self.readline()[:-1]))\n    dispatch[FLOAT] = load_float\n\n    def load_binfloat(self, unpack=struct.unpack):\n        self.append(unpack('>d', self.read(8))[0])\n    dispatch[BINFLOAT] = load_binfloat\n\n    def load_string(self):\n        rep = self.readline()[:-1]\n        for q in \"\\\"'\": # double or single quote\n            if rep.startswith(q):\n                if len(rep) < 2 or not rep.endswith(q):\n                    raise ValueError, \"insecure string pickle\"\n                rep = rep[len(q):-len(q)]\n                break\n        else:\n            raise ValueError, \"insecure string pickle\"\n        self.append(rep.decode(\"string-escape\"))\n    dispatch[STRING] = load_string\n\n    def load_binstring(self):\n        len = mloads('i' + self.read(4))\n        self.append(self.read(len))\n    dispatch[BINSTRING] = load_binstring\n\n    def load_unicode(self):\n        self.append(unicode(self.readline()[:-1],'raw-unicode-escape'))\n    dispatch[UNICODE] = load_unicode\n\n    def load_binunicode(self):\n        len = mloads('i' + self.read(4))\n        self.append(unicode(self.read(len),'utf-8'))\n    dispatch[BINUNICODE] = load_binunicode\n\n    def load_short_binstring(self):\n        len = ord(self.read(1))\n        self.append(self.read(len))\n    dispatch[SHORT_BINSTRING] = load_short_binstring\n\n    def load_tuple(self):\n        k = self.marker()\n        self.stack[k:] = [tuple(self.stack[k+1:])]\n    dispatch[TUPLE] = load_tuple\n\n    def load_empty_tuple(self):\n        self.stack.append(())\n    dispatch[EMPTY_TUPLE] = load_empty_tuple\n\n    def load_tuple1(self):\n        self.stack[-1] = (self.stack[-1],)\n    dispatch[TUPLE1] = load_tuple1\n\n    def load_tuple2(self):\n        self.stack[-2:] = [(self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE2] = load_tuple2\n\n    def load_tuple3(self):\n        self.stack[-3:] = [(self.stack[-3], self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE3] = load_tuple3\n\n    def load_empty_list(self):\n        self.stack.append([])\n    dispatch[EMPTY_LIST] = load_empty_list\n\n    def load_empty_dictionary(self):\n        self.stack.append({})\n    dispatch[EMPTY_DICT] = load_empty_dictionary\n\n    def load_list(self):\n        k = self.marker()\n        self.stack[k:] = [self.stack[k+1:]]\n    dispatch[LIST] = load_list\n\n    def load_dict(self):\n        k = self.marker()\n        d = {}\n        items = self.stack[k+1:]\n        for i in range(0, len(items), 2):\n            key = items[i]\n            value = items[i+1]\n            d[key] = value\n        self.stack[k:] = [d]\n    dispatch[DICT] = load_dict\n\n    # INST and OBJ differ only in how they get a class object.  It's not\n    # only sensible to do the rest in a common routine, the two routines\n    # previously diverged and grew different bugs.\n    # klass is the class to instantiate, and k points to the topmost mark\n    # object, following which are the arguments for klass.__init__.\n    def _instantiate(self, klass, k):\n        args = tuple(self.stack[k+1:])\n        del self.stack[k:]\n        instantiated = 0\n        if (not args and\n                type(klass) is ClassType and\n                not hasattr(klass, \"__getinitargs__\")):\n            try:\n                value = _EmptyClass()\n                value.__class__ = klass\n                instantiated = 1\n            except RuntimeError:\n                # In restricted execution, assignment to inst.__class__ is\n                # prohibited\n                pass\n        if not instantiated:\n            try:\n                value = klass(*args)\n            except TypeError, err:\n                raise TypeError, \"in constructor for %s: %s\" % (\n                    klass.__name__, str(err)), sys.exc_info()[2]\n        self.append(value)\n\n    def load_inst(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self._instantiate(klass, self.marker())\n    dispatch[INST] = load_inst\n\n    def load_obj(self):\n        # Stack is ... markobject classobject arg1 arg2 ...\n        k = self.marker()\n        klass = self.stack.pop(k+1)\n        self._instantiate(klass, k)\n    dispatch[OBJ] = load_obj\n\n    def load_newobj(self):\n        args = self.stack.pop()\n        cls = self.stack[-1]\n        obj = cls.__new__(cls, *args)\n        self.stack[-1] = obj\n    dispatch[NEWOBJ] = load_newobj\n\n    def load_global(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self.append(klass)\n    dispatch[GLOBAL] = load_global\n\n    def load_ext1(self):\n        code = ord(self.read(1))\n        self.get_extension(code)\n    dispatch[EXT1] = load_ext1\n\n    def load_ext2(self):\n        code = mloads('i' + self.read(2) + '\\000\\000')\n        self.get_extension(code)\n    dispatch[EXT2] = load_ext2\n\n    def load_ext4(self):\n        code = mloads('i' + self.read(4))\n        self.get_extension(code)\n    dispatch[EXT4] = load_ext4\n\n    def get_extension(self, code):\n        nil = []\n        obj = _extension_cache.get(code, nil)\n        if obj is not nil:\n            self.append(obj)\n            return\n        key = _inverted_registry.get(code)\n        if not key:\n            raise ValueError(\"unregistered extension code %d\" % code)\n        obj = self.find_class(*key)\n        _extension_cache[code] = obj\n        self.append(obj)\n\n    def find_class(self, module, name):\n        # Subclasses may override this\n        __import__(module)\n        mod = sys.modules[module]\n        klass = getattr(mod, name)\n        return klass\n\n    def load_reduce(self):\n        stack = self.stack\n        args = stack.pop()\n        func = stack[-1]\n        value = func(*args)\n        stack[-1] = value\n    dispatch[REDUCE] = load_reduce\n\n    def load_pop(self):\n        del self.stack[-1]\n    dispatch[POP] = load_pop\n\n    def load_pop_mark(self):\n        k = self.marker()\n        del self.stack[k:]\n    dispatch[POP_MARK] = load_pop_mark\n\n    def load_dup(self):\n        self.append(self.stack[-1])\n    dispatch[DUP] = load_dup\n\n    def load_get(self):\n        self.append(self.memo[self.readline()[:-1]])\n    dispatch[GET] = load_get\n\n    def load_binget(self):\n        i = ord(self.read(1))\n        self.append(self.memo[repr(i)])\n    dispatch[BINGET] = load_binget\n\n    def load_long_binget(self):\n        i = mloads('i' + self.read(4))\n        self.append(self.memo[repr(i)])\n    dispatch[LONG_BINGET] = load_long_binget\n\n    def load_put(self):\n        self.memo[self.readline()[:-1]] = self.stack[-1]\n    dispatch[PUT] = load_put\n\n    def load_binput(self):\n        i = ord(self.read(1))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[BINPUT] = load_binput\n\n    def load_long_binput(self):\n        i = mloads('i' + self.read(4))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[LONG_BINPUT] = load_long_binput\n\n    def load_append(self):\n        stack = self.stack\n        value = stack.pop()\n        list = stack[-1]\n        list.append(value)\n    dispatch[APPEND] = load_append\n\n    def load_appends(self):\n        stack = self.stack\n        mark = self.marker()\n        list = stack[mark - 1]\n        list.extend(stack[mark + 1:])\n        del stack[mark:]\n    dispatch[APPENDS] = load_appends\n\n    def load_setitem(self):\n        stack = self.stack\n        value = stack.pop()\n        key = stack.pop()\n        dict = stack[-1]\n        dict[key] = value\n    dispatch[SETITEM] = load_setitem\n\n    def load_setitems(self):\n        stack = self.stack\n        mark = self.marker()\n        dict = stack[mark - 1]\n        for i in range(mark + 1, len(stack), 2):\n            dict[stack[i]] = stack[i + 1]\n\n        del stack[mark:]\n    dispatch[SETITEMS] = load_setitems\n\n    def load_build(self):\n        stack = self.stack\n        state = stack.pop()\n        inst = stack[-1]\n        setstate = getattr(inst, \"__setstate__\", None)\n        if setstate:\n            setstate(state)\n            return\n        slotstate = None\n        if isinstance(state, tuple) and len(state) == 2:\n            state, slotstate = state\n        if state:\n            try:\n                d = inst.__dict__\n                try:\n                    for k, v in state.iteritems():\n                        d[intern(k)] = v\n                # keys in state don't have to be strings\n                # don't blow up, but don't go out of our way\n                except TypeError:\n                    d.update(state)\n\n            except RuntimeError:\n                # XXX In restricted execution, the instance's __dict__\n                # is not accessible.  Use the old way of unpickling\n                # the instance variables.  This is a semantic\n                # difference when unpickling in restricted\n                # vs. unrestricted modes.\n                # Note, however, that cPickle has never tried to do the\n                # .update() business, and always uses\n                #     PyObject_SetItem(inst.__dict__, key, value) in a\n                # loop over state.items().\n                for k, v in state.items():\n                    setattr(inst, k, v)\n        if slotstate:\n            for k, v in slotstate.items():\n                setattr(inst, k, v)\n    dispatch[BUILD] = load_build\n\n    def load_mark(self):\n        self.append(self.mark)\n    dispatch[MARK] = load_mark\n\n    def load_stop(self):\n        value = self.stack.pop()\n        raise _Stop(value)\n    dispatch[STOP] = load_stop\n\n# Helper class for load_inst/load_obj\n\nclass _EmptyClass:\n    pass\n\n# Encode/decode longs in linear time.\n\nimport binascii as _binascii\n\ndef encode_long(x):\n    r\"\"\"Encode a long to a two's complement little-endian binary string.\n    Note that 0L is a special case, returning an empty string, to save a\n    byte in the LONG1 pickling context.\n\n    >>> encode_long(0L)\n    ''\n    >>> encode_long(255L)\n    '\\xff\\x00'\n    >>> encode_long(32767L)\n    '\\xff\\x7f'\n    >>> encode_long(-256L)\n    '\\x00\\xff'\n    >>> encode_long(-32768L)\n    '\\x00\\x80'\n    >>> encode_long(-128L)\n    '\\x80'\n    >>> encode_long(127L)\n    '\\x7f'\n    >>>\n    \"\"\"\n\n    if x == 0:\n        return ''\n    if x > 0:\n        ashex = hex(x)\n        assert ashex.startswith(\"0x\")\n        njunkchars = 2 + ashex.endswith('L')\n        nibbles = len(ashex) - njunkchars\n        if nibbles & 1:\n            # need an even # of nibbles for unhexlify\n            ashex = \"0x0\" + ashex[2:]\n        elif int(ashex[2], 16) >= 8:\n            # \"looks negative\", so need a byte of sign bits\n            ashex = \"0x00\" + ashex[2:]\n    else:\n        # Build the 256's-complement:  (1L << nbytes) + x.  The trick is\n        # to find the number of bytes in linear time (although that should\n        # really be a constant-time task).\n        ashex = hex(-x)\n        assert ashex.startswith(\"0x\")\n        njunkchars = 2 + ashex.endswith('L')\n        nibbles = len(ashex) - njunkchars\n        if nibbles & 1:\n            # Extend to a full byte.\n            nibbles += 1\n        nbits = nibbles * 4\n        x += 1L << nbits\n        assert x > 0\n        ashex = hex(x)\n        njunkchars = 2 + ashex.endswith('L')\n        newnibbles = len(ashex) - njunkchars\n        if newnibbles < nibbles:\n            ashex = \"0x\" + \"0\" * (nibbles - newnibbles) + ashex[2:]\n        if int(ashex[2], 16) < 8:\n            # \"looks positive\", so need a byte of sign bits\n            ashex = \"0xff\" + ashex[2:]\n\n    if ashex.endswith('L'):\n        ashex = ashex[2:-1]\n    else:\n        ashex = ashex[2:]\n    assert len(ashex) & 1 == 0, (x, ashex)\n    binary = _binascii.unhexlify(ashex)\n    return binary[::-1]\n\ndef decode_long(data):\n    r\"\"\"Decode a long from a two's complement little-endian binary string.\n\n    >>> decode_long('')\n    0L\n    >>> decode_long(\"\\xff\\x00\")\n    255L\n    >>> decode_long(\"\\xff\\x7f\")\n    32767L\n    >>> decode_long(\"\\x00\\xff\")\n    -256L\n    >>> decode_long(\"\\x00\\x80\")\n    -32768L\n    >>> decode_long(\"\\x80\")\n    -128L\n    >>> decode_long(\"\\x7f\")\n    127L\n    \"\"\"\n\n    nbytes = len(data)\n    if nbytes == 0:\n        return 0L\n    ashex = _binascii.hexlify(data[::-1])\n    n = long(ashex, 16) # quadratic time before Python 2.3; linear now\n    if data[-1] >= '\\x80':\n        n -= 1L << (nbytes * 8)\n    return n\n\n# Shorthands\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\ndef dump(obj, file, protocol=None):\n    Pickler(file, protocol).dump(obj)\n\ndef dumps(obj, protocol=None):\n    file = StringIO()\n    Pickler(file, protocol).dump(obj)\n    return file.getvalue()\n\ndef load(file):\n    return Unpickler(file).load()\n\ndef loads(str):\n    file = StringIO(str)\n    return Unpickler(file).load()\n\n# Doctest\n\ndef _test():\n    import doctest\n    return doctest.testmod()\n\nif __name__ == \"__main__\":\n    _test()\n",
		"file_name": "pickle.py"
	},
	{
		"content": "import struct\n\nSHA_BLOCKSIZE = 64\nSHA_DIGESTSIZE = 32\n\n\ndef new_shaobject():\n    return {\n        'digest': [0]*8,\n        'count_lo': 0,\n        'count_hi': 0,\n        'data': [0]* SHA_BLOCKSIZE,\n        'local': 0,\n        'digestsize': 0\n    }\n\nROR = lambda x, y: (((x & 0xffffffff) >> (y & 31)) | (x << (32 - (y & 31)))) & 0xffffffff\nCh = lambda x, y, z: (z ^ (x & (y ^ z)))\nMaj = lambda x, y, z: (((x | y) & z) | (x & y))\nS = lambda x, n: ROR(x, n)\nR = lambda x, n: (x & 0xffffffff) >> n\nSigma0 = lambda x: (S(x, 2) ^ S(x, 13) ^ S(x, 22))\nSigma1 = lambda x: (S(x, 6) ^ S(x, 11) ^ S(x, 25))\nGamma0 = lambda x: (S(x, 7) ^ S(x, 18) ^ R(x, 3))\nGamma1 = lambda x: (S(x, 17) ^ S(x, 19) ^ R(x, 10))\n\ndef sha_transform(sha_info):\n    W = []\n    \n    d = sha_info['data']\n    for i in xrange(0,16):\n        W.append( (d[4*i]<<24) + (d[4*i+1]<<16) + (d[4*i+2]<<8) + d[4*i+3])\n    \n    for i in xrange(16,64):\n        W.append( (Gamma1(W[i - 2]) + W[i - 7] + Gamma0(W[i - 15]) + W[i - 16]) & 0xffffffff )\n    \n    ss = sha_info['digest'][:]\n    \n    def RND(a,b,c,d,e,f,g,h,i,ki):\n        t0 = h + Sigma1(e) + Ch(e, f, g) + ki + W[i];\n        t1 = Sigma0(a) + Maj(a, b, c);\n        d += t0;\n        h  = t0 + t1;\n        return d & 0xffffffff, h & 0xffffffff\n    \n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],0,0x428a2f98);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],1,0x71374491);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],2,0xb5c0fbcf);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],3,0xe9b5dba5);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],4,0x3956c25b);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],5,0x59f111f1);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],6,0x923f82a4);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],7,0xab1c5ed5);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],8,0xd807aa98);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],9,0x12835b01);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],10,0x243185be);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],11,0x550c7dc3);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],12,0x72be5d74);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],13,0x80deb1fe);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],14,0x9bdc06a7);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],15,0xc19bf174);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],16,0xe49b69c1);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],17,0xefbe4786);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],18,0x0fc19dc6);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],19,0x240ca1cc);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],20,0x2de92c6f);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],21,0x4a7484aa);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],22,0x5cb0a9dc);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],23,0x76f988da);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],24,0x983e5152);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],25,0xa831c66d);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],26,0xb00327c8);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],27,0xbf597fc7);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],28,0xc6e00bf3);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],29,0xd5a79147);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],30,0x06ca6351);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],31,0x14292967);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],32,0x27b70a85);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],33,0x2e1b2138);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],34,0x4d2c6dfc);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],35,0x53380d13);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],36,0x650a7354);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],37,0x766a0abb);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],38,0x81c2c92e);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],39,0x92722c85);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],40,0xa2bfe8a1);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],41,0xa81a664b);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],42,0xc24b8b70);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],43,0xc76c51a3);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],44,0xd192e819);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],45,0xd6990624);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],46,0xf40e3585);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],47,0x106aa070);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],48,0x19a4c116);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],49,0x1e376c08);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],50,0x2748774c);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],51,0x34b0bcb5);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],52,0x391c0cb3);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],53,0x4ed8aa4a);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],54,0x5b9cca4f);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],55,0x682e6ff3);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],56,0x748f82ee);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],57,0x78a5636f);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],58,0x84c87814);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],59,0x8cc70208);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],60,0x90befffa);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],61,0xa4506ceb);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],62,0xbef9a3f7);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],63,0xc67178f2);\n    \n    dig = []\n    for i, x in enumerate(sha_info['digest']):\n        dig.append( (x + ss[i]) & 0xffffffff )\n    sha_info['digest'] = dig\n\ndef sha_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [0x6A09E667, 0xBB67AE85, 0x3C6EF372, 0xA54FF53A, 0x510E527F, 0x9B05688C, 0x1F83D9AB, 0x5BE0CD19]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 32\n    return sha_info\n\ndef sha224_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [0xc1059ed8, 0x367cd507, 0x3070dd17, 0xf70e5939, 0xffc00b31, 0x68581511, 0x64f98fa7, 0xbefa4fa4]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 28\n    return sha_info\n\ndef getbuf(s):\n    if isinstance(s, str):\n        return s\n    elif isinstance(s, unicode):\n        return str(s)\n    else:\n        return buffer(s)\n\ndef sha_update(sha_info, buffer):\n    count = len(buffer)\n    buffer_idx = 0\n    clo = (sha_info['count_lo'] + (count << 3)) & 0xffffffff\n    if clo < sha_info['count_lo']:\n        sha_info['count_hi'] += 1\n    sha_info['count_lo'] = clo\n    \n    sha_info['count_hi'] += (count >> 29)\n    \n    if sha_info['local']:\n        i = SHA_BLOCKSIZE - sha_info['local']\n        if i > count:\n            i = count\n        \n        # copy buffer\n        for x in enumerate(buffer[buffer_idx:buffer_idx+i]):\n            sha_info['data'][sha_info['local']+x[0]] = struct.unpack('B', x[1])[0]\n        \n        count -= i\n        buffer_idx += i\n        \n        sha_info['local'] += i\n        if sha_info['local'] == SHA_BLOCKSIZE:\n            sha_transform(sha_info)\n            sha_info['local'] = 0\n        else:\n            return\n    \n    while count >= SHA_BLOCKSIZE:\n        # copy buffer\n        sha_info['data'] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + SHA_BLOCKSIZE]]\n        count -= SHA_BLOCKSIZE\n        buffer_idx += SHA_BLOCKSIZE\n        sha_transform(sha_info)\n        \n    \n    # copy buffer\n    pos = sha_info['local']\n    sha_info['data'][pos:pos+count] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + count]]\n    sha_info['local'] = count\n\ndef sha_final(sha_info):\n    lo_bit_count = sha_info['count_lo']\n    hi_bit_count = sha_info['count_hi']\n    count = (lo_bit_count >> 3) & 0x3f\n    sha_info['data'][count] = 0x80;\n    count += 1\n    if count > SHA_BLOCKSIZE - 8:\n        # zero the bytes in data after the count\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n        sha_transform(sha_info)\n        # zero bytes in data\n        sha_info['data'] = [0] * SHA_BLOCKSIZE\n    else:\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n    \n    sha_info['data'][56] = (hi_bit_count >> 24) & 0xff\n    sha_info['data'][57] = (hi_bit_count >> 16) & 0xff\n    sha_info['data'][58] = (hi_bit_count >>  8) & 0xff\n    sha_info['data'][59] = (hi_bit_count >>  0) & 0xff\n    sha_info['data'][60] = (lo_bit_count >> 24) & 0xff\n    sha_info['data'][61] = (lo_bit_count >> 16) & 0xff\n    sha_info['data'][62] = (lo_bit_count >>  8) & 0xff\n    sha_info['data'][63] = (lo_bit_count >>  0) & 0xff\n    \n    sha_transform(sha_info)\n    \n    dig = []\n    for i in sha_info['digest']:\n        dig.extend([ ((i>>24) & 0xff), ((i>>16) & 0xff), ((i>>8) & 0xff), (i & 0xff) ])\n    return ''.join([chr(i) for i in dig])\n\nclass sha256(object):\n    digest_size = digestsize = SHA_DIGESTSIZE\n    block_size = SHA_BLOCKSIZE\n\n    def __init__(self, s=None):\n        self._sha = sha_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n    \n    def update(self, s):\n        sha_update(self._sha, getbuf(s))\n    \n    def digest(self):\n        return sha_final(self._sha.copy())[:self._sha['digestsize']]\n    \n    def hexdigest(self):\n        return ''.join(['%.2x' % ord(i) for i in self.digest()])\n\n    def copy(self):\n        new = sha256.__new__(sha256)\n        new._sha = self._sha.copy()\n        return new\n\nclass sha224(sha256):\n    digest_size = digestsize = 28\n\n    def __init__(self, s=None):\n        self._sha = sha224_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n\n    def copy(self):\n        new = sha224.__new__(sha224)\n        new._sha = self._sha.copy()\n        return new\n\ndef test():\n    a_str = \"just a test string\"\n    \n    assert 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855' == sha256().hexdigest()\n    assert 'd7b553c6f09ac85d142415f857c5310f3bbbe7cdd787cce4b985acedd585266f' == sha256(a_str).hexdigest()\n    assert '8113ebf33c97daa9998762aacafe750c7cefc2b2f173c90c59663a57fe626f21' == sha256(a_str*7).hexdigest()\n    \n    s = sha256(a_str)\n    s.update(a_str)\n    assert '03d9963e05a094593190b6fc794cb1a3e1ac7d7883f0b5855268afeccc70d461' == s.hexdigest()\n\nif __name__ == \"__main__\":\n    test()\n\n\n",
		"file_name": "_sha256.py"
	},
	{
		"content": "\"\"\"\nThis code was Ported from CPython's sha512module.c\n\"\"\"\n\nimport struct\n\nSHA_BLOCKSIZE = 128\nSHA_DIGESTSIZE = 64\n\n\ndef new_shaobject():\n    return {\n        'digest': [0]*8,\n        'count_lo': 0,\n        'count_hi': 0,\n        'data': [0]* SHA_BLOCKSIZE,\n        'local': 0,\n        'digestsize': 0\n    }\n\nROR64 = lambda x, y: (((x & 0xffffffffffffffff) >> (y & 63)) | (x << (64 - (y & 63)))) & 0xffffffffffffffff\nCh = lambda x, y, z: (z ^ (x & (y ^ z)))\nMaj = lambda x, y, z: (((x | y) & z) | (x & y))\nS = lambda x, n: ROR64(x, n)\nR = lambda x, n: (x & 0xffffffffffffffff) >> n\nSigma0 = lambda x: (S(x, 28) ^ S(x, 34) ^ S(x, 39))\nSigma1 = lambda x: (S(x, 14) ^ S(x, 18) ^ S(x, 41))\nGamma0 = lambda x: (S(x, 1) ^ S(x, 8) ^ R(x, 7))\nGamma1 = lambda x: (S(x, 19) ^ S(x, 61) ^ R(x, 6))\n\ndef sha_transform(sha_info):\n    W = []\n\n    d = sha_info['data']\n    for i in xrange(0,16):\n        W.append( (d[8*i]<<56) + (d[8*i+1]<<48) + (d[8*i+2]<<40) + (d[8*i+3]<<32) + (d[8*i+4]<<24) + (d[8*i+5]<<16) + (d[8*i+6]<<8) + d[8*i+7])\n\n    for i in xrange(16,80):\n        W.append( (Gamma1(W[i - 2]) + W[i - 7] + Gamma0(W[i - 15]) + W[i - 16]) & 0xffffffffffffffff )\n\n    ss = sha_info['digest'][:]\n\n    def RND(a,b,c,d,e,f,g,h,i,ki):\n        t0 = (h + Sigma1(e) + Ch(e, f, g) + ki + W[i]) & 0xffffffffffffffff\n        t1 = (Sigma0(a) + Maj(a, b, c)) & 0xffffffffffffffff\n        d = (d + t0) & 0xffffffffffffffff\n        h = (t0 + t1) & 0xffffffffffffffff\n        return d & 0xffffffffffffffff, h & 0xffffffffffffffff\n\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],0,0x428a2f98d728ae22)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],1,0x7137449123ef65cd)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],2,0xb5c0fbcfec4d3b2f)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],3,0xe9b5dba58189dbbc)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],4,0x3956c25bf348b538)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],5,0x59f111f1b605d019)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],6,0x923f82a4af194f9b)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],7,0xab1c5ed5da6d8118)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],8,0xd807aa98a3030242)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],9,0x12835b0145706fbe)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],10,0x243185be4ee4b28c)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],11,0x550c7dc3d5ffb4e2)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],12,0x72be5d74f27b896f)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],13,0x80deb1fe3b1696b1)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],14,0x9bdc06a725c71235)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],15,0xc19bf174cf692694)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],16,0xe49b69c19ef14ad2)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],17,0xefbe4786384f25e3)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],18,0x0fc19dc68b8cd5b5)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],19,0x240ca1cc77ac9c65)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],20,0x2de92c6f592b0275)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],21,0x4a7484aa6ea6e483)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],22,0x5cb0a9dcbd41fbd4)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],23,0x76f988da831153b5)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],24,0x983e5152ee66dfab)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],25,0xa831c66d2db43210)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],26,0xb00327c898fb213f)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],27,0xbf597fc7beef0ee4)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],28,0xc6e00bf33da88fc2)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],29,0xd5a79147930aa725)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],30,0x06ca6351e003826f)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],31,0x142929670a0e6e70)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],32,0x27b70a8546d22ffc)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],33,0x2e1b21385c26c926)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],34,0x4d2c6dfc5ac42aed)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],35,0x53380d139d95b3df)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],36,0x650a73548baf63de)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],37,0x766a0abb3c77b2a8)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],38,0x81c2c92e47edaee6)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],39,0x92722c851482353b)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],40,0xa2bfe8a14cf10364)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],41,0xa81a664bbc423001)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],42,0xc24b8b70d0f89791)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],43,0xc76c51a30654be30)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],44,0xd192e819d6ef5218)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],45,0xd69906245565a910)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],46,0xf40e35855771202a)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],47,0x106aa07032bbd1b8)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],48,0x19a4c116b8d2d0c8)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],49,0x1e376c085141ab53)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],50,0x2748774cdf8eeb99)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],51,0x34b0bcb5e19b48a8)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],52,0x391c0cb3c5c95a63)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],53,0x4ed8aa4ae3418acb)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],54,0x5b9cca4f7763e373)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],55,0x682e6ff3d6b2b8a3)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],56,0x748f82ee5defb2fc)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],57,0x78a5636f43172f60)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],58,0x84c87814a1f0ab72)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],59,0x8cc702081a6439ec)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],60,0x90befffa23631e28)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],61,0xa4506cebde82bde9)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],62,0xbef9a3f7b2c67915)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],63,0xc67178f2e372532b)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],64,0xca273eceea26619c)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],65,0xd186b8c721c0c207)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],66,0xeada7dd6cde0eb1e)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],67,0xf57d4f7fee6ed178)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],68,0x06f067aa72176fba)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],69,0x0a637dc5a2c898a6)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],70,0x113f9804bef90dae)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],71,0x1b710b35131c471b)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],72,0x28db77f523047d84)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],73,0x32caab7b40c72493)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],74,0x3c9ebe0a15c9bebc)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],75,0x431d67c49c100d4c)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],76,0x4cc5d4becb3e42b6)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],77,0x597f299cfc657e2a)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],78,0x5fcb6fab3ad6faec)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],79,0x6c44198c4a475817)\n\n    dig = []\n    for i, x in enumerate(sha_info['digest']):\n        dig.append( (x + ss[i]) & 0xffffffffffffffff )\n    sha_info['digest'] = dig\n\ndef sha_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [ 0x6a09e667f3bcc908, 0xbb67ae8584caa73b, 0x3c6ef372fe94f82b, 0xa54ff53a5f1d36f1, 0x510e527fade682d1, 0x9b05688c2b3e6c1f, 0x1f83d9abfb41bd6b, 0x5be0cd19137e2179]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 64\n    return sha_info\n\ndef sha384_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [ 0xcbbb9d5dc1059ed8, 0x629a292a367cd507, 0x9159015a3070dd17, 0x152fecd8f70e5939, 0x67332667ffc00b31, 0x8eb44a8768581511, 0xdb0c2e0d64f98fa7, 0x47b5481dbefa4fa4]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 48\n    return sha_info\n\ndef getbuf(s):\n    if isinstance(s, str):\n        return s\n    elif isinstance(s, unicode):\n        return str(s)\n    else:\n        return buffer(s)\n\ndef sha_update(sha_info, buffer):\n    count = len(buffer)\n    buffer_idx = 0\n    clo = (sha_info['count_lo'] + (count << 3)) & 0xffffffff\n    if clo < sha_info['count_lo']:\n        sha_info['count_hi'] += 1\n    sha_info['count_lo'] = clo\n\n    sha_info['count_hi'] += (count >> 29)\n\n    if sha_info['local']:\n        i = SHA_BLOCKSIZE - sha_info['local']\n        if i > count:\n            i = count\n\n        # copy buffer\n        for x in enumerate(buffer[buffer_idx:buffer_idx+i]):\n            sha_info['data'][sha_info['local']+x[0]] = struct.unpack('B', x[1])[0]\n\n        count -= i\n        buffer_idx += i\n\n        sha_info['local'] += i\n        if sha_info['local'] == SHA_BLOCKSIZE:\n            sha_transform(sha_info)\n            sha_info['local'] = 0\n        else:\n            return\n\n    while count >= SHA_BLOCKSIZE:\n        # copy buffer\n        sha_info['data'] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + SHA_BLOCKSIZE]]\n        count -= SHA_BLOCKSIZE\n        buffer_idx += SHA_BLOCKSIZE\n        sha_transform(sha_info)\n\n    # copy buffer\n    pos = sha_info['local']\n    sha_info['data'][pos:pos+count] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + count]]\n    sha_info['local'] = count\n\ndef sha_final(sha_info):\n    lo_bit_count = sha_info['count_lo']\n    hi_bit_count = sha_info['count_hi']\n    count = (lo_bit_count >> 3) & 0x7f\n    sha_info['data'][count] = 0x80;\n    count += 1\n    if count > SHA_BLOCKSIZE - 16:\n        # zero the bytes in data after the count\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n        sha_transform(sha_info)\n        # zero bytes in data\n        sha_info['data'] = [0] * SHA_BLOCKSIZE\n    else:\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n\n    sha_info['data'][112] = 0;\n    sha_info['data'][113] = 0;\n    sha_info['data'][114] = 0;\n    sha_info['data'][115] = 0;\n    sha_info['data'][116] = 0;\n    sha_info['data'][117] = 0;\n    sha_info['data'][118] = 0;\n    sha_info['data'][119] = 0;\n\n    sha_info['data'][120] = (hi_bit_count >> 24) & 0xff\n    sha_info['data'][121] = (hi_bit_count >> 16) & 0xff\n    sha_info['data'][122] = (hi_bit_count >>  8) & 0xff\n    sha_info['data'][123] = (hi_bit_count >>  0) & 0xff\n    sha_info['data'][124] = (lo_bit_count >> 24) & 0xff\n    sha_info['data'][125] = (lo_bit_count >> 16) & 0xff\n    sha_info['data'][126] = (lo_bit_count >>  8) & 0xff\n    sha_info['data'][127] = (lo_bit_count >>  0) & 0xff\n\n    sha_transform(sha_info)\n\n    dig = []\n    for i in sha_info['digest']:\n        dig.extend([ ((i>>56) & 0xff), ((i>>48) & 0xff), ((i>>40) & 0xff), ((i>>32) & 0xff), ((i>>24) & 0xff), ((i>>16) & 0xff), ((i>>8) & 0xff), (i & 0xff) ])\n    return ''.join([chr(i) for i in dig])\n\nclass sha512(object):\n    digest_size = digestsize = SHA_DIGESTSIZE\n    block_size = SHA_BLOCKSIZE\n\n    def __init__(self, s=None):\n        self._sha = sha_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n\n    def update(self, s):\n        sha_update(self._sha, getbuf(s))\n\n    def digest(self):\n        return sha_final(self._sha.copy())[:self._sha['digestsize']]\n\n    def hexdigest(self):\n        return ''.join(['%.2x' % ord(i) for i in self.digest()])\n\n    def copy(self):\n        new = sha512.__new__(sha512)\n        new._sha = self._sha.copy()\n        return new\n\nclass sha384(sha512):\n    digest_size = digestsize = 48\n\n    def __init__(self, s=None):\n        self._sha = sha384_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n\n    def copy(self):\n        new = sha384.__new__(sha384)\n        new._sha = self._sha.copy()\n        return new\n\ndef test():\n    import _sha512\n\n    a_str = \"just a test string\"\n\n    assert _sha512.sha512().hexdigest() == sha512().hexdigest()\n    assert _sha512.sha512(a_str).hexdigest() == sha512(a_str).hexdigest()\n    assert _sha512.sha512(a_str*7).hexdigest() == sha512(a_str*7).hexdigest()\n\n    s = sha512(a_str)\n    s.update(a_str)\n    assert _sha512.sha512(a_str+a_str).hexdigest() == s.hexdigest()\n\nif __name__ == \"__main__\":\n    test()\n",
		"file_name": "_sha512.py"
	}
]